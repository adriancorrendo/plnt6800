{
  "hash": "1439cd416de34a2c94f318e560c1851b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Models with R: I - Essentials\"\nauthor: \"Dr. Adrian Correndo\"\ndate: \"2025-02-26\"\ncategories: [statistical modelling, R, agriculture, agridat, agricolae]\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 4\n    number-sections: true\n    table-class: \"table table-striped table-hover\"\neditor: source\nexecute:\n  echo: true\n  warning: false\n  message: false\nsmooth-scroll: true\n---\n\n\n\n## Introduction\n\nStatistical modeling is a process of developing and analyzing mathematical models to represent real-world phenomena. In agricultural research, statistical modeling plays a crucial role in understanding the relationships between environmental variables, management practices, and crop responses. By leveraging statistical models, researchers can make informed decisions about optimizing yield, improving resource efficiency, and enhancing sustainability in agriculture.\n\nIn this lesson, we focus on the essentials of statistical modeling using R, with examples relevant to ag-data science. We will explore the use of statistical models to analyze field experiments, evaluate treatment effects, and understand interactions between genotype, environment, and management practices. The examples will utilize datasets from the `agridat` package, particularly the `lasrosas.corn` dataset, and introduce key functions from `stats`, `nlme`, `lme4`, `car`, `multcomp`, and `agricolae`.\n\n### Statistical Modeling Process\n\n1. **Data Collection and Exploratory Data Analysis (EDA)**\n   - Statistical modeling starts with data collection and EDA. In agricultural experiments, this involves gathering data on yield, soil properties, weather conditions, and management practices. EDA helps identify patterns, trends, and relationships between variables while detecting outliers or anomalies.\n\n2. **Types of Statistical Models**\n   - In agricultural research, common models include:\n     - **Regression Models**: For predicting continuous outcomes like crop yield based on variables such as soil nutrients or precipitation.\n     - **Time Series Models**: To analyze temporal data like seasonal growth patterns or yield trends over years.\n     - **Mixed-Effects Models**: Ideal for experimental designs with hierarchical structures, such as split-plot designs or repeated measures.\n\n3. **Model Selection and Assumptions**\n   - The choice of model depends on the data type, research question, and assumptions about the data. For example:\n     - **Linear Regression** assumes a linear relationship between predictors and outcome, suitable for continuous variables like yield or biomass. We call it linear because we are basically comparing \"lines\", where the lines represent the \"means\".\n     - **Generalized Linear Models (GLM)** are used for non-normal distributions, such as count data (e.g., pest counts) or binary outcomes (e.g., disease presence).\n\n4. **Model Evaluation**\n   - Evaluating model performance is crucial to ensure accurate predictions and inferences. In agricultural modeling, common metrics include:\n     - **R-squared (R²)**: Measures the proportion of variation explained by the model. But it is NOT always recommended as a criterion the \"select models\".\n     - **Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)**: Assess prediction accuracy.\n     - **AIC and BIC**: For model comparison and selection. These two are recommended when selecting a model.\n\n5. **Application in Agricultural Research**\n   - Statistical models provide insights into complex agricultural systems, enabling researchers to:\n     - **Identify Key Drivers**: Determine which factors most influence crop performance, such as genotype-environment interactions.\n     - **Predict Future Trends**: Forecast yield potential under different climate scenarios or management practices.\n     - **Optimize Inputs**: Inform decision-making for fertilizer application, irrigation scheduling, or pest management.\n\n## Essential R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(agridat, agricolae)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(nlme, lme4, car, multcomp)\n```\n:::\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corn <- agridat::lasrosas.corn\n# Check data structure and variables\nglimpse(data_corn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,443\nColumns: 9\n$ year  <int> 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   <dbl> -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  <dbl> -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield <dbl> 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro <dbl> 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  <fct> W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    <dbl> 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   <fct> R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    <fct> N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …\n```\n\n\n:::\n:::\n\n\n\n\n\n## Key Statistical Models\n\n### Linear Models (LM)\n\nLinear regression models are fundamental for analyzing relationships between variables. The term \"regression\" could be confusing because it means we are working with a \"continous response variable\", but it could also mean we using a \"continuous covariate\" (or independent / or explanatory variable) (e.g. a \"regressor\").\n\n#### Categorical covariate/s as independent variable/s\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Complete Randomized\nlm_fit_01 <- lm(yield ~ nf, data = data_corn)\n# See summary\nsummary(lm_fit_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.9729     0.8218  79.060  < 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,\tAdjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n```\n\n\n:::\n\n```{.r .cell-code}\n# Alternative models\n# Blocks (as fixed)\nlm_fit_02 <- lm(yield ~ nf + rep , data = data_corn)\n# Add year (as fixed)\nlm_fit_03 <- lm(yield ~ nf + rep + year, data = data_corn)\n# Add topography (as fixed)\nlm_fit_04 <- lm(yield ~ nf + rep + year + topo, data = data_corn)\n# Different order \nlm_fit_05 <- lm(yield ~ nf + year + topo + rep, data = data_corn)\n```\n:::\n\n\n#### Continuous covariate/s as independent variable/s\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Nitrogen (independent variable) as continuous predictor\nlm_reg_01 <- lm(yield ~ nitro, data = data_corn)\n# See summary\nsummary(lm_reg_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 65.843213   0.608573 108.193  < 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,\tAdjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare to N as a categorical variable\nsummary(lm_fit_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.9729     0.8218  79.060  < 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,\tAdjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n```\n\n\n:::\n:::\n\n\n\n### Generalized Linear Models (GLM)\n\nGLMs extend linear models to handle non-normal response distributions. In agricultural research, they are useful for modeling yield data with non-constant variance or non-normal residuals.\n\n#### Example using GLM as an LM\n\nlm() is just a special case of glm where the distribution of error is assumed to be Gaussian (i.e. normal)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_fit_01 <- glm(yield ~ nf + rep, data = data_corn, family = gaussian)\n# See summary\nsummary(glm_fit_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian, data = data_corn)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.7216     0.9486  68.225  < 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8527)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328839  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare to lm\nsummary(lm_fit_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.7216     0.9486  68.225  < 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,\tAdjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n```\n\n\n:::\n:::\n\n\n\n#### Example using the Gaussian family with log link:\n\nThese approaches are particularly useful when yield data exhibit heteroscedasticity or skewness.\n\n- For this first approach, the model assumes a multiplicative relationship between predictors and yield, modeling the expected value as an exponential function. If you want to stabilize variance or normalize the residuals, use the first approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using log link without manually transforming yield\nglm_fit_02 <- glm(yield ~ nf + rep, data = data_corn, \n                  family = gaussian(link = \"log\"))\nsummary(glm_fit_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian(link = \"log\"), \n    data = data_corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.170242   0.014369 290.223  < 2e-16 ***\nnfN1         0.054586   0.017387   3.140  0.00171 ** \nnfN2         0.069457   0.017308   4.013 6.12e-05 ***\nnfN3         0.079305   0.017202   4.610 4.17e-06 ***\nnfN4         0.110495   0.016981   6.507 8.79e-11 ***\nnfN5         0.114107   0.016934   6.738 1.87e-11 ***\nrepR2       -0.004492   0.011824  -0.380  0.70404    \nrepR3        0.015601   0.011702   1.333  0.18254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8745)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328857  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\nAlternatively, you may want to manually log-transform the response:\n\n- For this second approach, the log-transformed yield is modeled as a linear function of the predictors, stabilizing variance or normalizing residuals. If you believe the relationship between predictors and the expected value of yield is multiplicative, use the second approach.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manually log-transforming yield\nglm_fit_03 <- glm(log(yield) ~ nf + rep, data = lasrosas.corn, \n                  family = gaussian(link = \"identity\"))\nsummary(glm_fit_03)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = log(yield) ~ nf + rep, family = gaussian(link = \"identity\"), \n    data = lasrosas.corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.114234   0.013869 296.650  < 2e-16 ***\nnfN1         0.070491   0.016959   4.157 3.31e-05 ***\nnfN2         0.086082   0.017003   5.063 4.35e-07 ***\nnfN3         0.096942   0.016974   5.711 1.22e-08 ***\nnfN4         0.130277   0.016996   7.665 2.31e-14 ***\nnfN5         0.129767   0.016974   7.645 2.69e-14 ***\nrepR2       -0.004546   0.012007  -0.379    0.705    \nrepR3        0.019599   0.012002   1.633    0.103    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.08268533)\n\n    Null deviance: 291.07  on 3442  degrees of freedom\nResidual deviance: 284.02  on 3435  degrees of freedom\nAIC: 1198.4\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n\n** Key Difference: **\n- In the first case, the model is:  $\\log(Y) = X\\beta + \\epsilon $, which is a linear model on the log-transformed outcome.\n\n- In the second case, the model is:  $E(Y) = \\exp(X\\beta)$ , where the expected value of yield is modeled as an exponential function of the predictors.\n\n\n### Mixed-Effects Models\n\nMixed-effects models account for both fixed and random effects, often used in agricultural experiments.\n\nUsing `nlme`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_fit <- lme(yield ~ nf, random = ~1 | rep, data = data_corn)\nsummary(lme_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed-effects model fit by REML\n  Data: data_corn \n       AIC      BIC    logLik\n  30286.69 30335.83 -15135.34\n\nRandom effects:\n Formula: ~1 | rep\n        (Intercept) Residual\nStdDev:   0.4656023 19.66857\n\nFixed effects:  yield ~ nf \n               Value Std.Error   DF  t-value p-value\n(Intercept) 64.97387 0.8645219 3435 75.15584  0.0000\nnfN1         3.64192 1.1599967 3435  3.13960  0.0017\nnfN2         4.67371 1.1630343 3435  4.01855  0.0001\nnfN3         5.36182 1.1610011 3435  4.61827  0.0000\nnfN4         7.59070 1.1625194 3435  6.52953  0.0000\nnfN5         7.85772 1.1610011 3435  6.76805  0.0000\n Correlation: \n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.6547157 -0.7871554 -0.1563133  0.6960595  2.2882930 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n```\n\n\n:::\n:::\n\n\n\nUsing `lme4`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer_fit <- lmer(yield ~ nf + (1 | rep), data = data_corn)\nsummary(lmer_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ nf + (1 | rep)\n   Data: data_corn\n\nREML criterion at convergence: 30270.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6547 -0.7872 -0.1563  0.6961  2.2883 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n rep      (Intercept)   0.2168  0.4656 \n Residual             386.8526 19.6686 \nNumber of obs: 3443, groups:  rep, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  64.9739     0.8645  75.156\nnfN1          3.6419     1.1600   3.140\nnfN2          4.6737     1.1630   4.019\nnfN3          5.3618     1.1610   4.618\nnfN4          7.5907     1.1625   6.530\nnfN5          7.8577     1.1610   6.768\n\nCorrelation of Fixed Effects:\n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n```\n\n\n:::\n:::\n\n\n\n### Choosing Between `nlme` and `lme4`\n- **nlme**: Suitable for models that are linear and nonlinear mixed-effects models. It provides robust tools for analyzing data with nested random effects and handling different types of correlation structures within the data. It can handle heterogeneous variance models.\n\n- **lme4**: Best for fitting large linear mixed-effects models. It does not handle nonlinear mixed-effects models or autoregressive correlation structures but is highly efficient with large datasets and complex random effects structures. It cannot handle heterogeneous variance models.\n\n### Analysis of Variance (ANOVA)\n\nAnalysis of Variance (ANOVA) is widely used in agricultural research to compare the means of multiple groups and to understand the influence of categorical factors on continuous outcomes, such as yield or biomass. In R, there are multiple ways to perform ANOVA:\n\n- `anova()`: Sequential (Type I) ANOVA\n- `aov()`: Similar for balanced designs\n- `car::Anova()`: Flexible ANOVA with options for Type II and Type III Sum of Squares\n\n#### Using `anova()`\n\n`anova()` performs Type I Sum of Squares (sequential). It tests each term sequentially, considering the order of the terms in the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm(yield ~ rep + nf + year + topo, data = data_corn)\nanova(lm_fit)  # Type I Sum of Squares\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(>F)    \nrep          2   1271     635    3.7264 0.02418 *  \nnf           5  23987    4797   28.1334 < 2e-16 ***\nyear         1  97313   97313  570.6692 < 2e-16 ***\ntopo         3 646456  215485 1263.6625 < 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n#### Using `aov()`\n\n`aov()` is similar to `lm()` but is designed for balanced experimental designs. It also uses Type I Sum of Squares.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov_fit <- aov(yield ~ nf + year + topo + rep, data = data_corn)\nsummary(aov_fit)  # Type I Sum of Squares\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Df Sum Sq Mean Sq F value   Pr(>F)    \nnf             5  23987    4797   28.13  < 2e-16 ***\nyear           1  97321   97321  570.71  < 2e-16 ***\ntopo           3 643667  214556 1258.21  < 2e-16 ***\nrep            2   4053    2027   11.88 7.18e-06 ***\nResiduals   3431 585070     171                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n#### Using `car::Anova()`\n\nThe `Anova()` function from the `car` package allows for Type II and Type III Sum of Squares:\n\n- **Type II**: Assumes no interaction between factors and tests each main effect after the other main effects.\n- **Type III**: Tests each main effect and interaction after all other terms, typically used with dummy coding.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Anova(lm_fit, type = 2)  # Type II Sum of Squares\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type II tests)\n\nResponse: yield\n          Sum Sq   Df  F value    Pr(>F)    \nrep         4053    2   11.885 7.183e-06 ***\nnf         21727    5   25.483 < 2.2e-16 ***\nyear      120660    1  707.584 < 2.2e-16 ***\ntopo      646456    3 1263.662 < 2.2e-16 ***\nResiduals 585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(lm_fit, type = 3)  # Type III Sum of Squares\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(>F)    \n(Intercept) 119183    1  698.920 < 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nnf           21727    5   25.483 < 2.2e-16 ***\nyear        120660    1  707.584 < 2.2e-16 ***\ntopo        646456    3 1263.662 < 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n#### Comparison of anova() vs. Anova()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For the anova(), the order of factors matter\nanova(lm_fit_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nnf           5   23987  4797.4  12.396 6.075e-12 ***\nResiduals 3437 1330110   387.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lm_fit_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value   Pr(>F)    \nnf           5   23987  4797.4 12.4011 6.01e-12 ***\nrep          2    1271   635.6  1.6429   0.1936    \nResiduals 3435 1328839   386.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lm_fit_03)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq  F value    Pr(>F)    \nnf           5   23987    4797  13.3771 6.105e-13 ***\nrep          2    1271     636   1.7722    0.1701    \nyear         1   97313   97313 271.3489 < 2.2e-16 ***\nResiduals 3434 1231526     359                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lm_fit_04)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(>F)    \nnf           5  23987    4797   28.1331 < 2e-16 ***\nrep          2   1271     636    3.7272 0.02416 *  \nyear         1  97313   97313  570.6692 < 2e-16 ***\ntopo         3 646456  215485 1263.6625 < 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lm_fit_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq  F value    Pr(>F)    \nnf           5  23987    4797   28.133 < 2.2e-16 ***\nyear         1  97321   97321  570.714 < 2.2e-16 ***\ntopo         3 643667  214556 1258.209 < 2.2e-16 ***\nrep          2   4053    2027   11.885 7.183e-06 ***\nResiduals 3431 585070     171                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# For the Anova(type=3), the order of factors doesn't matter\ncar::Anova(lm_fit_01, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(>F)    \n(Intercept) 2418907    1 6250.447 < 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(lm_fit_02, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(>F)    \n(Intercept) 1800690    1 4654.7170 < 2.2e-16 ***\nnf            23987    5   12.4012 6.009e-12 ***\nrep            1271    2    1.6429    0.1936    \nResiduals   1328839 3435                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(lm_fit_03, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(>F)    \n(Intercept)   96132    1 268.0552 < 2.2e-16 ***\nnf            23836    5  13.2930 7.436e-13 ***\nrep            1264    2   1.7616    0.1719    \nyear          97313    1 271.3489 < 2.2e-16 ***\nResiduals   1231526 3434                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(lm_fit_04, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(>F)    \n(Intercept) 119183    1  698.920 < 2.2e-16 ***\nnf           21727    5   25.483 < 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nyear        120660    1  707.584 < 2.2e-16 ***\ntopo        646456    3 1263.662 < 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(lm_fit_05, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(>F)    \n(Intercept) 119183    1  698.920 < 2.2e-16 ***\nnf           21727    5   25.483 < 2.2e-16 ***\nyear        120660    1  707.584 < 2.2e-16 ***\ntopo        646456    3 1263.662 < 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nIn agricultural research, Type III Sum of Squares is particularly useful for unbalanced designs, such as field trials with missing data or unequal replications.\n\n### Post-hoc Tests\n\nAfter detecting significant differences with ANOVA, post-hoc tests can be conducted to identify specific group differences.\n\nUsing `multcomp` for multiple comparisons:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using glht() function\ncomp <- glht(aov_fit, linfct = mcp(nf = \"Tukey\"))\nsummary(comp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = yield ~ nf + year + topo + rep, data = data_corn)\n\nLinear Hypotheses:\n             Estimate Std. Error t value Pr(>|t|)    \nN1 - N0 == 0   3.8048     0.7702   4.940  < 0.001 ***\nN2 - N0 == 0   4.8232     0.7722   6.246  < 0.001 ***\nN3 - N0 == 0   5.0724     0.7709   6.580  < 0.001 ***\nN4 - N0 == 0   7.4876     0.7718   9.701  < 0.001 ***\nN5 - N0 == 0   7.3672     0.7709   9.556  < 0.001 ***\nN2 - N1 == 0   1.0184     0.7708   1.321  0.77334    \nN3 - N1 == 0   1.2676     0.7696   1.647  0.56709    \nN4 - N1 == 0   3.6828     0.7705   4.779  < 0.001 ***\nN5 - N1 == 0   3.5624     0.7697   4.628  < 0.001 ***\nN3 - N2 == 0   0.2492     0.7716   0.323  0.99954    \nN4 - N2 == 0   2.6644     0.7726   3.449  0.00759 ** \nN5 - N2 == 0   2.5440     0.7717   3.297  0.01271 *  \nN4 - N3 == 0   2.4152     0.7712   3.132  0.02173 *  \nN5 - N3 == 0   2.2948     0.7702   2.980  0.03452 *  \nN5 - N4 == 0  -0.1204     0.7712  -0.156  0.99999    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n```\n\n\n:::\n:::\n\n\n\n### Nonlinear Models\n\nNonlinear models are useful when the relationship between the predictor and response variables is not linear. In agricultural research, these models are commonly used to model yield response to inputs, such as nitrogen fertilizer.\n\nFor nonlinear relationships, we could use `nls()`. Let's see an example using a power function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnls_fit <- nls(yield ~ a * nitro^b, data = data_corn, start = list(a = 1, b = 1))\nsummary(nls_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: yield ~ a * nitro^b\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na 55.71689    4.33102  12.865  < 2e-16 ***\nb  0.05641    0.01811   3.116  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.99 on 3441 degrees of freedom\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.822e-07\n```\n\n\n:::\n\n```{.r .cell-code}\n# Alternative exponential\n# nls_mitscherlich <- nls(yield ~ a * (1 - exp(-b * nitro)), data = data_corn, start = list(a = 55, b = 0.05))\n```\n:::\n\n\nVisualizing the model's predictions can help in understanding the fitted curve and the data's behavior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating a data frame with predictions\ndata_corn <- data_corn %>% \n  mutate(pred = predict(nls_fit))\n\n# Plotting observed vs. predicted yield\ndata_corn %>% \nggplot(aes(x = nitro, y = yield)) +\n  geom_point(color = \"blue\", size = 2) +  # Observed data\n  geom_line(aes(y = pred), color = \"red\", size = 1) +  # Fitted curve\n  geom_smooth()+\n  labs(title = \"Yield Response to Nitrogen\",\n       x = \"Nitrogen (kg/ha)\",\n       y = \"Yield (qq/ha)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](models_01_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Conclusion\n\nThis lesson introduced essential statistical models in R for agricultural research, providing practical code examples. In the next session, we will delve deeper into model diagnostics and interpretation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "models_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}