{
  "hash": "aad70ffc2e32b7f0721887469da57359",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Explanatory vs. Predictive Models in Agriculture with R\"\nauthor: \"Dr. Adrian Correndo\"\ndate: \"2025-02-28\"\ncategories: [statistical modelling, R, agriculture, predictive modeling]\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 4\n    number-sections: true\n    table-class: \"table table-striped table-hover\"\neditor: source\nexecute:\n  echo: true\n  warning: false\n  message: false\nsmooth-scroll: true\n---\n\n\n\n\n## Introduction\n\nStatistical models in agriculture serve two primary purposes: **explanatory** and **predictive** modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.\n\nThis article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.\n\n**Required packages:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(emmeans, multcomp, multcompView)\np_load(randomForest, caret, metrica)\n```\n:::\n\n\n\n\n\n## Explanatory Models\n\nExplanatory models are designed to **understand** how different factors influence a response variable. These models help answer questions such as: *What are the main drivers of yield variation? How do nitrogen application and rainfall affect crop performance?*\n\n### Example: Linear Regression for Explanation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated agricultural data\ndata_ag <- data.frame(\n  nitrogen = c(50, 100, 150, 200, 250, 300),\n  rainfall = c(800, 850, 900, 950, 1000, 1050),\n  yield = c(2.5, 3.1, 3.8, 4.2, 4.5, 4.6)\n)\n\n# Fit a linear model\nlm_fit <- lm(yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\nsummary(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\n\nResiduals:\nALL 6 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n                       Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 2.5        NaN     NaN      NaN\nas.factor(nitrogen)100      0.6        NaN     NaN      NaN\nas.factor(nitrogen)150      1.3        NaN     NaN      NaN\nas.factor(nitrogen)200      1.7        NaN     NaN      NaN\nas.factor(nitrogen)250      2.0        NaN     NaN      NaN\nas.factor(nitrogen)300      2.1        NaN     NaN      NaN\nrainfall                     NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:    NaN \nF-statistic:   NaN on 5 and 0 DF,  p-value: NA\n```\n\n\n:::\n:::\n\n\n\n\n### Interpretation\n\n- The **coefficients** indicate the effect of each predictor on yield.\n- The **p-values** help determine statistical significance.\n- The **R-squared** value explains how much variance is accounted for by the model.\n\n### Means Comparisons\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform multiple comparisons\nemmeans_fit <- emmeans(lm_fit, ~ nitrogen)\ncomp <- cld(emmeans_fit, Letters = letters)\ncomp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n nitrogen emmean  SE df lower.CL upper.CL .group\n      100    3.1 NaN  0      NaN      NaN       \n      150    3.8 NaN  0      NaN      NaN       \n      200    4.2 NaN  0      NaN      NaN       \n      250    4.5 NaN  0      NaN      NaN       \n       50 nonEst  NA NA       NA       NA       \n      300 nonEst  NA NA       NA       NA       \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 2 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n```\n\n\n:::\n:::\n\n\n\n\n### Visualization\nWhat are we missing here?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ggplot of estimated means\ncomp_plot <- ggplot(comp, aes(x = as.factor(nitrogen), y = emmean, fill = nitrogen)) +\n  geom_col(color = \"black\") +\n  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.2) +\n  geom_text(aes(label = .group), vjust = -0.5, size = 5) +\n  labs(title = \"Means Comparison for Nitrogen Levels\",\n       x = \"Nitrogen Levels (kg/ha)\",\n       y = \"Estimated Yield (t/ha)\") +\n  theme_minimal()\n\ncomp_plot\n```\n\n::: {.cell-output-display}\n![](models_02_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Predictive Models\n\nPredictive models aim to **forecast** future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments. **Machine learning** models dominate here. A key challenge in predictive modeling is ensuring that the model **generalizes well** to unseen data, which is why we use techniques like **cross-validation**. An advantage is that we don't need repetitions of the data to use these models, but we need to have a good size so the algorithms can \"learn\" (machine learning).\n\n### Cross-Validation and Generalization Performance\n\nCross-validation is a resampling technique used to evaluate a modelâ€™s ability to generalize to new data. It helps avoid overfitting, where a model performs well on training data but poorly on unseen data. One common method is **k-fold cross-validation**, where the dataset is split into k subsets, and the model is trained and tested multiple times.\n\n- **Training Error**: The error the model makes on the data it was trained on.\n- **Generalization Performance**: The model's ability to make accurate predictions on unseen data.\n- **Validation Set Approach**: One practical method in agriculture is to leave out data from the latest year as a test set, ensuring the model is evaluated on future-like conditions.\n\n### Updated Agricultural Dataset with Multiple Years\n\nTo better illustrate predictive modeling, we expand our dataset to include multiple years, allowing us to simulate a real-world scenario where we leave the latest year out for validation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated multi-year agricultural data\ndata_ag <- data.frame(\n  year = rep(2011:2020, each = 50),  # 20 observations per year\n  nitrogen = runif(500, 90, 300),\n  rainfall = runif(500, 700, 1050),\n  psnt = runif(500, 5, 60), # pre-sidedress N test (ppm)\n  yield = 2 + 0.01 * runif(500, 90, 300) + 0.005 * runif(500, 700, 1050) + rnorm(500, 0, 0.2) - 0.002 * runif(500, 5, 60) \n)\n\n# Splitting into training (excluding latest year) and test set (latest year only)\ntrain_data <- data_ag %>% filter(year < 2020)\ntest_data <- data_ag %>% filter(year == 2020)\n```\n:::\n\n\n\n\n### Example: Random Forest with Cross-Validation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Train model with cross-validation using \"caret\" package\ntrain_control <- caret::trainControl(method = \"cv\", number = 5)\nrf_fit <- caret::train(yield ~ nitrogen + rainfall + psnt, data = train_data, method = \"rf\", trControl = train_control)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model Performance\nprint(rf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n450 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 359, 360, 360, 361, 360 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared     MAE      \n  2     0.9101655  0.005663882  0.7370689\n  3     0.9151354  0.005187781  0.7380953\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predict on the test set\npredictions <- predict(rf_fit, test_data)\n\n# Evaluate Generalization Performance\nsqrt(mean((predictions - test_data$yield)^2))  # Root Mean Squared Error on test data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7080847\n```\n\n\n:::\n\n```{.r .cell-code}\n# Using the metrica package\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$RMSE\n[1] 0.7080847\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot predicted vs observed scatter\nmetrica::scatter_plot(pred = predictions, obs = test_data$yield)\n```\n\n::: {.cell-output-display}\n![](models_02_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# With tidyverse syntax will be...\ntest_preds <- test_data %>% mutate(predictions = predict(rf_fit, test_data))\n\nmetrica::scatter_plot(data = test_preds, \n                      pred = predictions, obs = yield)\n```\n\n::: {.cell-output-display}\n![](models_02_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Root mean square error\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$RMSE\n[1] 0.7080847\n```\n\n\n:::\n\n```{.r .cell-code}\n# Relative mean square error (as a proportion)\nmetrica::RRMSE(data = test_preds, pred = predictions, obs = yield)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$RRMSE\n[1] 0.08590629\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimate more prediction error metrics\nmetrica::metrics_summary(data = test_preds, pred = predictions, obs = yield,\n                         type = \"regression\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Metric         Score\n1      B0  5.0350052760\n2      B1  0.3876722744\n3       r  0.2274194841\n4      R2  0.0517196217\n5      Xa  0.6738755631\n6     CCC  0.1532524329\n7     MAE  0.5788009582\n8    RMAE  0.0702213256\n9    MAPE  7.1616881311\n10  SMAPE  7.0596644078\n11    RAE  1.0032320662\n12    RSE  0.9742467992\n13    MBE  0.0121206657\n14    PBE  0.1470504149\n15    PAB  0.0293010081\n16    PPB 38.4856531036\n17    MSE  0.5013838960\n18   RMSE  0.7080846672\n19  RRMSE  0.0859062918\n20    RSR  1.3758902632\n21 iqRMSE  0.8338883760\n22    MLA  0.1931077775\n23    MLP  0.3082761185\n24   RMLA  0.1931077775\n25   RMLP  0.3082761185\n26     SB  0.0001469105\n27   SDSD  0.1929608669\n28    LCS  0.3082761185\n29    PLA 38.5149541118\n30    PLP 61.4850458882\n31     Ue 61.4850458882\n32     Uc 38.4856531036\n33     Ub  0.0293010081\n34    NSE  0.0257532008\n35     E1 -0.0032320662\n36   Erel -0.0689801883\n37    KGE  0.0145411153\n38      d  0.3918632952\n39     d1  0.2676482520\n40    d1r  0.4983839669\n41    RAC  0.5765736892\n42     AC -2.9375050057\n43 lambda  0.1532524329\n44  dcorr  0.3276562374\n45    MIC  0.3437581054\n```\n\n\n:::\n:::\n\n\n\n\n### Interpretation\n\n- **Cross-validation** ensures the model is not just memorizing the training data but generalizing well (e.g. predicting well on unseen observations).\n\n- **Training vs. Test/Validation Performance**: Comparing error metrics between the training set and the unseen test set gives an estimate of real-world predictive ability. If the difference training between training error and testing error is too much, it's very likely our model is \"over-fitted\" (e.g. reading really well the training data but too much).\n\n- **Leaving the latest year out** allows us to test predictions on future-like data, a common technique in agricultural forecasting.\n\nUsing these techniques ensures that predictive models in agriculture provide **reliable and actionable insights** rather than overfitted results that fail in practice.\n\nPredictive models aim to **forecast** future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments.\n\n### Example: Final Random Forest for Forecasting\nNow we have our final model, we train one more time with all the available data, then predict new observations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a random forest model\nset.seed(123)\n\nrf_fit <- randomForest(yield ~ nitrogen + rainfall + psnt, data = data_ag, ntree = 500)\nprint(rf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = yield ~ nitrogen + rainfall + psnt, data = data_ag,      ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.7684589\n                    % Var explained: -6.18\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predict yield for new nitrogen and rainfall levels\nnew_data <- data.frame(field = c(\"Elora\", \"Waterloo\", \"Ridgetown\", \"Winchester\"),\n                       nitrogen = c(125, 175, 225, 220), \n                       rainfall = c(870, 920, 980, 1000),\n                       psnt = c(35, 30, 45, 60))\n\n# Adding predictions\nnew_preds <- new_data %>% mutate(predictions = predict(rf_fit, new_data))\n```\n:::\n\n\n\n\n### Interpretation\n\n- This model is **non-parametric** and learns patterns from data.\n- It is robust against outliers and complex interactions.\n- Performance is evaluated using **Mean Squared Error (MSE)** or **R-squared**.\n\n## Key Differences Between Explanatory and Predictive Models\n\n| Feature | Explanatory Models | Predictive Models |\n|---------|-------------------|-------------------|\n| Purpose | Understanding relationships | Making accurate forecasts |\n| Example | Linear regression, ANOVA | Machine learning (random forests, neural networks) |\n| Assumptions | Requires assumptions about data distribution | Often non-parametric, flexible |\n| Output | Coefficients, p-values | Predictions, accuracy metrics |\n\n## Conclusion\n\nUnderstanding the distinction between explanatory and predictive models is essential for agricultural research. While explanatory models help us understand **why** certain patterns exist, predictive models allow us to make **data-driven decisions** for future planning. A combination of both approaches can maximize insights and improve decision-making in precision agriculture.\n\nThis article brings simple examples in R using linear regression for explanatory analysis and random forests for prediction. Depending on the research question, both modeling strategies play a crucial role in agricultural data science.\n\n",
    "supporting": [
      "models_02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}