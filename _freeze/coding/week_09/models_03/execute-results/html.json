{
  "hash": "9fec451dbae2b3a8389be79c96df5380",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models III: Linear Models in Ag-Data Science\"\nauthor: \"Dr. Adrian Correndo\"\ndate: \"2025-03-05\"\ncategories: [linear models, R, statistics, agriculture]\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 4\n    number-sections: true\n    table-class: \"table table-striped table-hover\"\neditor: source\nexecute:\n  echo: true\n  warning: false\n  message: false\nsmooth-scroll: true\n---\n\n\n\n\n## Introduction\n\nLinear models are foundational in statistical analysis, particularly in agricultural data science. These models allow researchers to evaluate relationships between variables and assess treatment effects in experiments. This document covers the essentials of linear modeling in R using `stats`, `car`, `broom`, `emmeans`, `multcomp`, and `cld` for statistical inference and means comparisons.\n\n## What is a Linear Model?\n\nA **linear model** is a mathematical equation describing the relationship between a response variable (dependent) and one or more explanatory variables (independent). The simplest form is:\n\n$$ Y = \\beta_0 + \\beta_1 X + ... + \\beta_2 X ... + \\epsilon $$\n\nwhere:\n\n$$Y$$ is the dependent variable (response variable), <br/>\n$$X$$ is the independent variable (matrix of experimental design), <br/>\n$$ \\beta_0 $$ is the intercept, <br/>\n$$ \\beta_1 $$ is the effect of factor #1 on Y (i.e. slope (regression) or mean effect (anova)), <br/>\n$$ \\beta_2 $$ is the effect of factor #2 on Y (i.e. slope (regression) or mean effect (anova)), <br/>\n$$ \\epsilon $$ represents error (unexplained variation). <br/>\n\nNote: A polynomial term–a quadratic (squared or $2^{nd}$ order) or cubic ($3^{rd}$ order) term turns a linear regression model into a curve. But because it is X that is squared or cubed, not the Beta coefficient ($ \\beta $), it still qualifies as a linear model.\n\n### Regression vs. ANOVA\n\n-   **Regression Analysis**: Used when the explanatory variable(s) are continuous (e.g., predicting yield from nitrogen levels).\n\n-   **ANOVA (Analysis of Variance)**: Used when at least one explanatory variable is categorical (e.g., comparing mean yields across different treatments).\n\n-   Technically, ANOVA is a type of linear model, but its focus is on comparing means across groups (made from categorical predictors), while regression aims to quantify the relationship between continuous variables.\n\nWe will use the `data_corn` dataset from the `agridat` package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(agridat)\np_load(broom)\np_load(emmeans)\np_load(multcomp, multcompView)\np_load(car) # for assumption checks\np_load(performance) # for assumption checks\n```\n:::\n\n\n\n\n**Load dataset:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corn <- agridat::lasrosas.corn\n\n# Inspect dataset\nglimpse(data_corn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,443\nColumns: 9\n$ year  <int> 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   <dbl> -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  <dbl> -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield <dbl> 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro <dbl> 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  <fct> W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    <dbl> 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   <fct> R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    <fct> N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …\n```\n\n\n:::\n:::\n\n\n\n\n## Model Fitting: Linear Regression & ANOVA\n\n### Simple Linear Regression (Continuous Predictor)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_fit <- lm(yield ~ 1 + nitro, data = data_corn)\nsummary(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 65.843213   0.608573 108.193  < 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,\tAdjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparing analysis of variance options\nanova(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nnitro        1   23790 23790.3  61.537 5.754e-15 ***\nResiduals 3441 1330307   386.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(reg_fit, type = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type II tests)\n\nResponse: yield\n           Sum Sq   Df F value    Pr(>F)    \nnitro       23790    1  61.537 5.754e-15 ***\nResiduals 1330307 3441                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(reg_fit, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(>F)    \n(Intercept) 4525464    1 11705.665 < 2.2e-16 ***\nnitro         23790    1    61.537 5.754e-15 ***\nResiduals   1330307 3441                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Linear Regression, no-intercept\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_noint <- lm(yield ~ -1 + nitro, data = data_corn)\nsummary(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 65.843213   0.608573 108.193  < 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,\tAdjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(reg_noint)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ -1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.435  -7.134  20.741  45.962 108.840 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nnitro 0.772271   0.009088   84.98   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.25 on 3442 degrees of freedom\nMultiple R-squared:  0.6772,\tAdjusted R-squared:  0.6771 \nF-statistic:  7222 on 1 and 3442 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparing analysis of variance options\nanova(reg_noint)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: yield\n            Df   Sum Sq  Mean Sq F value    Pr(>F)    \nnitro        1 12286371 12286371  7221.9 < 2.2e-16 ***\nResiduals 3442  5855771     1701                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(reg_noint, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(>F)    \nnitro     12286371    1  7221.9 < 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(reg_noint, type = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type II tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(>F)    \nnitro     12286371    1  7221.9 < 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### ANOVA (Categorical Predictors)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_crd <- lm(yield ~ nf, data = data_corn) # assuming CRD\nanova_fit <- lm(yield ~ nf + rep, data = data_corn) # assuming RCBD\nanova_03 <- lm(yield ~ nf*rep, data = data_corn) # assuming RCBD\n\nAnova(anova_crd, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(>F)    \n(Intercept) 2418907    1 6250.447 < 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n#Anova(anova_fit, type = 3)\nAnova(anova_03, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(>F)    \n(Intercept)  799899    1 2063.2074 < 2.2e-16 ***\nnf             7035    5    3.6290  0.002818 ** \nrep              88    2    0.1139  0.892378    \nnf:rep          977   10    0.2520  0.990548    \nResiduals   1327862 3425                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### ANOVA, no-intercept\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_noint <- lm(yield ~ -1 + nf + rep, data = data_corn)\nAnova(anova_noint, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n           Sum Sq   Df   F value Pr(>F)    \nnf        5575937    6 2402.2655 <2e-16 ***\nrep          1271    2    1.6429 0.1936    \nResiduals 1328839 3435                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(anova_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.7216     0.9486  68.225  < 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,\tAdjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(anova_noint)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ -1 + nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nnfN0   64.7216     0.9486  68.225   <2e-16 ***\nnfN1   68.3611     0.9464  72.235   <2e-16 ***\nnfN2   69.3895     0.9495  73.082   <2e-16 ***\nnfN3   70.0816     0.9478  73.940   <2e-16 ***\nnfN4   72.3132     0.9491  76.195   <2e-16 ***\nnfN5   72.5775     0.9478  76.573   <2e-16 ***\nrepR2  -0.3301     0.8213  -0.402    0.688    \nrepR3   1.0915     0.8210   1.329    0.184    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.9268,\tAdjusted R-squared:  0.9266 \nF-statistic:  5433 on 8 and 3435 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Model Assumptions\n\nLinear models assume:\n\n1.  **Linearity**: Relationship between predictors and response is linear (continuous).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual diagnostics\npar(mfrow=c(2,2))\nplot(anova_fit)\n```\n\n::: {.cell-output-display}\n![](models_03_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n2.  **Normality of Residuals**: Residuals should be normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normality test\nshapiro.test(resid(anova_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid(anova_fit)\nW = 0.94724, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n3.  **Homoscedasticity**: Equal variance across all levels of predictors.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Homoscedasticity check\nleveneTest(anova_crd) #1-way anova, only nf and CRD\nleveneTest(anova_fit) #2-way anova including blocks\n```\n:::\n\n\n\n\n4.  **Independence**: Observations are independent (i.e. the “error” of replications is independent).\n\nThere is no test for independence. You have to make sure you specify the error-structure correctly for potential autocorrelation (e.g. blocks, split-plots, repeated measures, etc.).\n\n5.  **Performance package**\nWith the `performance` package, we could check all at once.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(anova_fit)\n```\n\n::: {.cell-output-display}\n![](models_03_files/figure-html/unnamed-chunk-10-1.png){width=768}\n:::\n:::\n\n\n\n\n## Model Selection: AIC/BIC Criteria\n\n### Candidate models\nThese are all fixed-effect models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# null model\nlm_00 <- lm(yield ~ 1, data = data_corn)\n# Simplest model\nlm_01 <- lm(yield ~ nf + rep, data = data_corn)\n# Add year\nlm_02 <- lm(yield ~ nf + year + rep, data = data_corn)\n# Add topo\nlm_03 <- lm(yield ~ nf + topo + rep, data = data_corn)\n# Add year and topo\nlm_04 <- lm(yield ~ nf + year + topo + rep, data = data_corn)\n\n# Main effects and interactions\nlm_05 <- lm(yield ~ nf*year*topo + rep, data = data_corn)\n#lm_05 <- lm(yield ~ nf + year + topo + nf:year + nf:topo + year:topo + nf:year:topo + rep, data = data_corn)\n```\n:::\n\n\n\n\n### Selection criteria\n#### F-Test\n- **F-tests** evaluate if added predictors significantly improve model fit via sum of squares and degrees of freedom.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lm_01, lm_02, lm_03, lm_04, lm_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: yield ~ nf + rep\nModel 2: yield ~ nf + year + rep\nModel 3: yield ~ nf + topo + rep\nModel 4: yield ~ nf + year + topo + rep\nModel 5: yield ~ nf * year * topo + rep\n  Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n1   3435 1328839                                    \n2   3434 1231526  1     97313  914.137 < 2.2e-16 ***\n3   3432  705730  2    525796 2469.604 < 2.2e-16 ***\n4   3431  585070  1    120660 1133.457 < 2.2e-16 ***\n5   3393  361197 38    223873   55.342 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### AIC (Akaike Information Criterion)\n- **AIC** balances goodness of fit with model complexity.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      df      AIC\nlm_01  9 30294.35\nlm_02 10 30034.50\nlm_03 12 28121.52\nlm_04 13 27477.95\nlm_05 51 25893.36\n```\n\n\n:::\n:::\n\n\n\n\n#### BIC (Bayesian Information criterion)\n- **BIC** applies a stricter penalty for complexity, favoring simpler models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      df      BIC\nlm_01  9 30349.64\nlm_02 10 30095.94\nlm_03 12 28195.25\nlm_04 13 27557.82\nlm_05 51 26206.71\n```\n\n\n:::\n:::\n\n\n\n\n## Significance of effects\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare Anova sum of squares\nAnova(lm_05, type = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type II tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(>F)    \nnf            24028   20   11.2855 < 2.2e-16 ***\nyear         120553    1 1132.4508 < 2.2e-16 ***\ntopo         648289   18  338.3269 < 2.2e-16 ***\nrep            3995    2   18.7623 7.877e-09 ***\nnf:year        1486    5    2.7919   0.01601 *  \nnf:topo        2126   15    1.3315   0.17376    \nyear:topo    218811    3  685.1532 < 2.2e-16 ***\nnf:year:topo   1869   15    1.1704   0.28739    \nResiduals    361197 3393                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(lm_05, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(>F)    \n(Intercept)   26405    1 248.0418 < 2.2e-16 ***\nnf              322    5   0.6050    0.6961    \nyear          26539    1 249.2978 < 2.2e-16 ***\ntopo          38269    3 119.8315 < 2.2e-16 ***\nrep            3995    2  18.7623 7.877e-09 ***\nnf:year         321    5   0.6039    0.6970    \nnf:topo        1868   15   1.1701    0.2876    \nyear:topo     38337    3 120.0429 < 2.2e-16 ***\nnf:year:topo   1869   15   1.1704    0.2874    \nResiduals    361197 3393                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Means Comparisons with `emmeans` and `cld`\n### Interaction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pairwise comparisons among treatment means\nemmeans(lm_05, pairwise ~ year:topo) %>% \n  cld(., level = 0.05, decreasing = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n year topo emmean    SE   df lower.CL upper.CL .group  \n 2001 HT     44.6 0.497 3393     44.6     44.6  1      \n 1999 HT     53.4 0.549 3393     53.4     53.5   2     \n 1999 E      64.8 0.538 3393     64.7     64.8    3    \n 1999 W      66.0 0.438 3393     65.9     66.0    34   \n 2001 W      67.7 0.467 3393     67.7     67.8     4   \n 1999 LO     71.3 0.481 3393     71.3     71.3      5  \n 2001 E      92.7 0.543 3393     92.6     92.7       6 \n 2001 LO     99.9 0.501 3393     99.9     99.9        7\n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 8 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n```\n\n\n:::\n:::\n\n\n\n### Alternative \nUsing the same model, same sum of squares we could group comparisons differently. In this case, showing comparisons of topo means, grouped by \"year\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(lm_05, pairwise ~ topo, by = \"year\") %>% \n  cld(., level = 0.05, decreasing = FALSE, Letters = letters) # add letters\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nyear = 1999:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     53.4 0.549 3393     53.4     53.5  a    \n E      64.8 0.538 3393     64.7     64.8   b   \n W      66.0 0.438 3393     65.9     66.0   b   \n LO     71.3 0.481 3393     71.3     71.3    c  \n\nyear = 2001:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     44.6 0.497 3393     44.6     44.6  a    \n W      67.7 0.467 3393     67.7     67.8   b   \n E      92.7 0.543 3393     92.6     92.7    c  \n LO     99.9 0.501 3393     99.9     99.9     d \n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n```\n\n\n:::\n\n```{.r .cell-code}\n# By default in R\nLETTERS\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n```\n\n\n:::\n\n```{.r .cell-code}\nletters\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n```\n\n\n:::\n:::\n\n\n\n\n## Interpreting Coefficients\n### Regression\n-   **Intercept ($\\beta_0$)**: Baseline value of the dependent variable. Is the value of Y, when X = 0.\n\n-   **Slope ($\\beta_1$)**: Change in response variable (Y) per unit increase in predictor (X).\n\n-   **p-value**: Significance of predictor effect.\n\nWe can extract regression coefficient estimates with the 'coef()' function, or with the 'tidy()' function of the \"broom\" package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extracting coefficients with coef()\ncoef(reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       nitro \n65.84321305  0.06171718 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Tidy summary of coefficients\nreg_coefs <- broom::tidy(reg_fit)\nreg_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  65.8      0.609      108.   0       \n2 nitro         0.0617   0.00787      7.84 5.75e-15\n```\n\n\n:::\n:::\n\n\n\n\n### ANOVA\n-   **Intercept**: the reference or benchmark level (baseline mean) for categorical predictors.\n-   **Factor Levels**: estimates of mean differences \"with respect to\" the intercept (if any, if not, from zero).\n-   **p-value**: whether a factor significantly differs from baseline.\n\nWe can also extract ANOVA coefficient estimates with the 'coef()' function, or with the 'tidy()' function of the \"broom\" package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extracting coefficients with coef()\ncoef(lm_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)             nfN1             nfN2             nfN3 \n   -2.930444e+04     7.134866e+01     4.085152e+02     3.143485e+03 \n            nfN4             nfN5             year           topoHT \n    2.469591e+03     2.724513e+03     1.468933e+01     3.840595e+04 \n          topoLO            topoW            repR2            repR3 \n   -8.387929e+02     2.518130e+04     4.372000e-01     2.476942e+00 \n       nfN1:year        nfN2:year        nfN3:year        nfN4:year \n   -3.417600e-02    -2.023608e-01    -1.569826e+00    -1.232124e+00 \n       nfN5:year      nfN1:topoHT      nfN2:topoHT      nfN3:topoHT \n   -1.359608e+00    -3.857242e+03    -2.925203e+03    -4.044360e+03 \n     nfN4:topoHT      nfN5:topoHT      nfN1:topoLO      nfN2:topoLO \n   -1.041123e+03     1.878725e+03     3.636709e+03     1.417504e+03 \n     nfN3:topoLO      nfN4:topoLO      nfN5:topoLO       nfN1:topoW \n   -2.679130e+03    -1.098798e+02    -1.411964e+03    -5.869613e+01 \n      nfN2:topoW       nfN3:topoW       nfN4:topoW       nfN5:topoW \n    3.338172e+03    -2.204795e+03     3.275604e+03     1.455853e+03 \n     year:topoHT      year:topoLO       year:topoW nfN1:year:topoHT \n   -1.921966e+01     4.227740e-01    -1.259712e+01     1.930477e+00 \nnfN2:year:topoHT nfN3:year:topoHT nfN4:year:topoHT nfN5:year:topoHT \n    1.464313e+00     2.024633e+00     5.232640e-01    -9.370874e-01 \nnfN1:year:topoLO nfN2:year:topoLO nfN3:year:topoLO nfN4:year:topoLO \n   -1.818932e+00    -7.089521e-01     1.339684e+00     5.517450e-02 \nnfN5:year:topoLO  nfN1:year:topoW  nfN2:year:topoW  nfN3:year:topoW \n    7.067462e-01     2.969667e-02    -1.668718e+00     1.102522e+00 \n nfN4:year:topoW  nfN5:year:topoW \n   -1.636535e+00    -7.268434e-01 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Tidy summary of coefficients\nanova_coefs <- broom::tidy(lm_05)\nanova_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 5\n   term        estimate std.error statistic  p.value\n   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept) -29304.   1861.     -15.7    5.43e-54\n 2 nfN1            71.3  2621.       0.0272 9.78e- 1\n 3 nfN2           409.   2621.       0.156  8.76e- 1\n 4 nfN3          3143.   2653.       1.18   2.36e- 1\n 5 nfN4          2470.   2637.       0.937  3.49e- 1\n 6 nfN5          2725.   2671.       1.02   3.08e- 1\n 7 year            14.7     0.930   15.8    3.01e-54\n 8 topoHT       38406.   2590.      14.8    2.95e-48\n 9 topoLO        -839.   2527.      -0.332  7.40e- 1\n10 topoW        25181.   2438.      10.3    1.23e-24\n# ℹ 40 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_00)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.000\n  adj. R2: 0.000\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.019\n  adj. R2: 0.017\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.091\n  adj. R2: 0.088\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_03)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.479\n  adj. R2: 0.477\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_04)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.568\n  adj. R2: 0.567\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::r2(lm_05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Linear Regression\n       R2: 0.733\n  adj. R2: 0.729\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Conclusion\n\nLinear models are essential for agricultural research, helping to quantify relationships and test hypotheses. This quick guide covered essentials for regression (continuous predictors), ANOVA (categorical predictors), assumption checks, model selection, and means comparisons using `emmeans`, and coefficients' extraction with `broom`.\n",
    "supporting": [
      "models_03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}