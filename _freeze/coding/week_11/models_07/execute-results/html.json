{
  "hash": "3ae9ac46308d7aeb3a2d99b094efa351",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification models in R\"\nauthor: \"Dr. Adrian Correndo\"\ndate: \"2025-03-19\" \ncategories: [classification, logistic regression, cart, random forest, machine learning, R, agriculture] \nabstract-title: 'Summary' \nabstract: 'This tutorial provides a short review about classification models using categorical variables as the response variable. For this, we will cover logistic regression (binomial and multinomial), as well as classification trees and random forest (bootstrapped trees) all using the \"tidymodels\" approach.'\nformat: \n  html:\n    toc: true\n    toc-location: left \n    toc-depth: 4 \n    number-sections: true \n    table-class: \"table table-striped table-hover\" \n    editor: source \nexecute: \n  echo: true \n  warning: false \n  message: false \nsmooth-scroll: true \nbibliography: references.bib \nlink-citations: TRUE\n---\n\n\n\n\n# Introduction\n\nThis document demonstrates how to apply **Logistic Regression** @james2013 and **tree-based models** [@therneau1997, @breiman2001] using the **tidyverse** approach in R. We use the Dry Bean Dataset from the UCI Machine Learning Repository.\n\n**Packages required for today**\n\n\n\n\n\n\n\n\n\n# Simple Binary Classification Example (Iris Dataset)\n\n## Logistic regression\n\nWe will start with a simple example using the **Iris dataset**, where we will create a binary classification problem.\n\n### `logistic_reg()` from tidymodels\n\nWe will filter the dataset to only include **two species** (`setosa` and `versicolor`) and train a logistic regression model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load iris dataset\niris_binary <- iris %>%\n  filter(Species != \"versicolor\") %>%  # Keep only two classes\n  mutate(Species = as.factor(Species))\n\n# Split the data\nset.seed(42)\niris_split <- initial_split(iris_binary, prop = 0.8, strata = Species)\niris_train <- training(iris_split)\niris_test <- testing(iris_split)\n\n# Train logistic regression using tidymodels\niris_log_model <- \n  logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\niris_log_wf <- workflow() %>%\n  add_model(iris_log_model) %>%\n  add_formula(Species ~ .)\n\niris_log_fit <- iris_log_wf %>% fit(data = iris_train)\n\n# Evaluate model\niris_predictions <- predict(iris_log_fit, iris_test, type = \"class\") %>%\n  bind_cols(iris_test)\n\niris_metrics <- iris_predictions %>%\n  metrics(truth = Species, estimate = .pred_class)\n\niris_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.5  \n2 kap      multiclass     0.333\n```\n\n\n:::\n:::\n\n\n\n\n### `glm()` from base R\n\nAlternatively, we can use the more **classic** approach with `glm()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train logistic regression using glm\niris_glm <- glm(Species ~ ., data = iris_train, family = binomial)\n\n# Predict on test data\niris_glm_probs <- predict(iris_glm, iris_test, type = \"response\")\n\n# Ensure the response variable is a factor with two levels\niris_test <- iris_test %>% \n  mutate(Species = factor(Species, levels = c(\"setosa\", \"virginica\")))\n\n# Plot ROC Curve\nroc_curve_glm <- roc(response = iris_test$Species, predictor = iris_glm_probs, levels = rev(levels(iris_test$Species)))\nplot(roc_curve_glm, col = \"blue\", main = \"ROC Curve for GLM Logistic Regression\")\n```\n\n::: {.cell-output-display}\n![](models_07_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nauc(roc_curve_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 1\n```\n\n\n:::\n:::\n\n\n\n\n# Model Evaluation\n\nWe evaluate the models using accuracy, Kappa statistic, ROC curves, and other metrics.\n\n## Confusion Matrix\n\nIn machine learning and statistics, a **confusion matrix** is used to evaluate the performance of a classification model. It compares predicted labels against actual ground truth labels.\n\nBelow is a conceptual confusion matrix explaining **True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN):**\n\n| Actual / Predicted  | Positive Prediction | Negative Prediction |\n|----------------------|--------------------|--------------------|\n| **Actual Positive** | **True Positive (TP)**: Correctly predicted positive | **False Negative (FN)**: Incorrectly predicted as negative |\n| **Actual Negative** | **False Positive (FP)**: Incorrectly predicted as positive | **True Negative (TN)**: Correctly predicted negative |\n\n## Explanation of Terms:\n- **True Positive (TP):** Model correctly predicts a positive instance.\n- **False Positive (FP):** Model incorrectly predicts a negative instance as positive (Type I error).\n- **True Negative (TN):** Model correctly predicts a negative instance.\n- **False Negative (FN):** Model incorrectly predicts a positive instance as negative (Type II error).\n\nThis matrix helps in evaluating performance metrics such as **accuracy, precision, recall, and F1-score**.\n\n## Classification Metrics\n\n### Accuracy\n\nAccuracy is one of the most popular metrics that measures the proportion of correctly classified cases:\n\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$ \n- A value of 0.50 suggests the model is predicting correctly only half the time, which is just slightly better than random guessing. \n- Higher accuracy (closer to 1.00) is desirable, but accuracy alone doesn’t always reflect the model’s performance, especially in imbalanced datasets. \n\n### Kappa Statistic\n\nKappa (Cohen’s Kappa) measures how well the model performs relative to random chance:\n\n$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n\nWhere: $p_o$ is the observed accuracy, $p_e$ is the expected accuracy due to chance.\n\nInterpretation of Kappa Values: • Kappa = 1.00 → Perfect agreement. • Kappa \\> 0.80 → Strong agreement. • Kappa between 0.40 and 0.60 → Moderate agreement. • Kappa \\< 0.40 → Poor agreement. • Kappa = 0 → The model is no better than random chance.\n\nIn our Iris example, a Kappa of 0.333 suggests that while the model performs better than random, it still struggles to distinguish between the classes.\n\n## ROC Curve & AUC (Area Under the Curve)\n\nThe Receiver Operating Characteristic (ROC) curve shows the trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity).\n\nInterpretation of AUC Values: \n- AUC = 1.00 → Perfect classification. \n- AUC > 0.90 → Excellent discrimination. \n- AUC between 0.80 - 0.90 → Good classification. \n- AUC between 0.70 - 0.80 → Fair classification. \n- AUC = 0.50 → Random guessing.\n\nIn our Iris example, an AUC of 1.00 means that the model separates the two species perfectly. However, this should be interpreted alongside accuracy and kappa to ensure it’s not due to overfitting.\n\nIf accuracy is low (e.g., 50%) but AUC is high (1.00), this could indicate a problem in threshold selection or that the dataset is too simple for the model.\n\nThis example shows both the **modern tidymodels approach** and the **classic glm() approach** for binary classification.\n\n# Multinomial Classification\n\n## Dry beans dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip\"\n#download.file(url, \"DryBeanDataset.zip\")\n#unzip(\"DryBeanDataset.zip\", exdir = \"./DryBeanDataset\")\n\ndf <- readxl::read_xlsx(path=\"data/Dry_Bean_Dataset.xlsx\")\n```\n:::\n\n\n\n\nLets take a look at the structure and summary of the dataset.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 13,611\nColumns: 17\n$ Area            <dbl> 28395, 28734, 29380, 30008, 30140, 30279, 30477, 30519…\n$ Perimeter       <dbl> 610.291, 638.018, 624.110, 645.884, 620.134, 634.927, …\n$ MajorAxisLength <dbl> 208.1781, 200.5248, 212.8261, 210.5580, 201.8479, 212.…\n$ MinorAxisLength <dbl> 173.8887, 182.7344, 175.9311, 182.5165, 190.2793, 181.…\n$ AspectRation    <dbl> 1.197191, 1.097356, 1.209713, 1.153638, 1.060798, 1.17…\n$ Eccentricity    <dbl> 0.5498122, 0.4117853, 0.5627273, 0.4986160, 0.3336797,…\n$ ConvexArea      <dbl> 28715, 29172, 29690, 30724, 30417, 30600, 30970, 30847…\n$ EquivDiameter   <dbl> 190.1411, 191.2728, 193.4109, 195.4671, 195.8965, 196.…\n$ Extent          <dbl> 0.7639225, 0.7839681, 0.7781132, 0.7826813, 0.7730980,…\n$ Solidity        <dbl> 0.9888560, 0.9849856, 0.9895588, 0.9766957, 0.9908933,…\n$ roundness       <dbl> 0.9580271, 0.8870336, 0.9478495, 0.9039364, 0.9848771,…\n$ Compactness     <dbl> 0.9133578, 0.9538608, 0.9087742, 0.9283288, 0.9705155,…\n$ ShapeFactor1    <dbl> 0.007331506, 0.006978659, 0.007243912, 0.007016729, 0.…\n$ ShapeFactor2    <dbl> 0.003147289, 0.003563624, 0.003047733, 0.003214562, 0.…\n$ ShapeFactor3    <dbl> 0.8342224, 0.9098505, 0.8258706, 0.8617944, 0.9419004,…\n$ ShapeFactor4    <dbl> 0.9987239, 0.9984303, 0.9990661, 0.9941988, 0.9991661,…\n$ Class           <chr> \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", …\n```\n\n\n:::\n\n```{.r .cell-code}\nskimr::skim(df)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |df    |\n|Number of rows           |13611 |\n|Number of columns        |17    |\n|_______________________  |      |\n|Column type frequency:   |      |\n|character                |1     |\n|numeric                  |16    |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Class         |         0|             1|   4|   8|     0|        7|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable   | n_missing| complete_rate|     mean|       sd|       p0|      p25|      p50|      p75|      p100|hist  |\n|:---------------|---------:|-------------:|--------:|--------:|--------:|--------:|--------:|--------:|---------:|:-----|\n|Area            |         0|             1| 53048.28| 29324.10| 20420.00| 36328.00| 44652.00| 61332.00| 254616.00|▇▂▁▁▁ |\n|Perimeter       |         0|             1|   855.28|   214.29|   524.74|   703.52|   794.94|   977.21|   1985.37|▇▆▁▁▁ |\n|MajorAxisLength |         0|             1|   320.14|    85.69|   183.60|   253.30|   296.88|   376.50|    738.86|▇▆▂▁▁ |\n|MinorAxisLength |         0|             1|   202.27|    44.97|   122.51|   175.85|   192.43|   217.03|    460.20|▇▇▁▁▁ |\n|AspectRation    |         0|             1|     1.58|     0.25|     1.02|     1.43|     1.55|     1.71|      2.43|▂▇▅▂▁ |\n|Eccentricity    |         0|             1|     0.75|     0.09|     0.22|     0.72|     0.76|     0.81|      0.91|▁▁▂▇▇ |\n|ConvexArea      |         0|             1| 53768.20| 29774.92| 20684.00| 36714.50| 45178.00| 62294.00| 263261.00|▇▂▁▁▁ |\n|EquivDiameter   |         0|             1|   253.06|    59.18|   161.24|   215.07|   238.44|   279.45|    569.37|▇▆▁▁▁ |\n|Extent          |         0|             1|     0.75|     0.05|     0.56|     0.72|     0.76|     0.79|      0.87|▁▁▅▇▂ |\n|Solidity        |         0|             1|     0.99|     0.00|     0.92|     0.99|     0.99|     0.99|      0.99|▁▁▁▁▇ |\n|roundness       |         0|             1|     0.87|     0.06|     0.49|     0.83|     0.88|     0.92|      0.99|▁▁▂▇▇ |\n|Compactness     |         0|             1|     0.80|     0.06|     0.64|     0.76|     0.80|     0.83|      0.99|▂▅▇▂▁ |\n|ShapeFactor1    |         0|             1|     0.01|     0.00|     0.00|     0.01|     0.01|     0.01|      0.01|▁▃▇▃▁ |\n|ShapeFactor2    |         0|             1|     0.00|     0.00|     0.00|     0.00|     0.00|     0.00|      0.00|▇▇▇▃▁ |\n|ShapeFactor3    |         0|             1|     0.64|     0.10|     0.41|     0.58|     0.64|     0.70|      0.97|▂▇▇▃▁ |\n|ShapeFactor4    |         0|             1|     1.00|     0.00|     0.95|     0.99|     1.00|     1.00|      1.00|▁▁▁▁▇ |\n\n\n:::\n:::\n\n\n\n\n## Data Preprocessing\n\nWe handle class imbalance using **SMOTE** and ensure the target variable is a factor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df %>% \n  mutate(Class = as.factor(Class)) %>%\n  recipe(Class ~ .) %>%\n  step_smote(Class) %>%\n  prep() %>%\n  juice()\n```\n:::\n\n\n\n\n### Training and Testing Split\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_split <- initial_split(df, prop = 0.8, strata = Class)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n```\n:::\n\n\n\n\n## Logistic Regression Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, fit a standard logistic regression model\nlog_model_simple <- logistic_reg() %>%  # Logistic regression (binary only)\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nlog_wf_simple <- workflow() %>%\n  add_model(log_model_simple) %>%\n  add_formula(Class ~ .)\n\nlog_fit_simple <- log_wf_simple %>% fit(data = train_data)\n```\n:::\n\n\n\n\n::: {.callout-warning}\n**Warning:** Be cautious because logistic regression is only suitable for binary classification. Since we have 7 classes, a multinomial regression is more appropriate.\n:::\n\n## Multinomial Logistic Regression Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_model_multinom <- multinom_reg() %>%  # Multinomial logistic regression\n  set_engine(\"nnet\") %>%\n  set_mode(\"classification\")\n\nlog_wf_multinom <- workflow() %>%\n  add_model(log_model_multinom) %>%\n  add_formula(Class ~ .)\n\nlog_fit_multinom <- log_wf_multinom %>% fit(data = train_data)\n```\n:::\n\n\n\n\n## Classification Tree Model\n\nA classification tree is a single decision tree that splits the data based on feature values to classify observations. It follows these steps:\n\n1. Splits data at each node based on a feature that minimizes impurity (e.g., Gini index, entropy).\n\n2. Forms a tree structure where each leaf represents a predicted class.\n\n3. Prone to **overfitting**, as it may capture noise in the training data.\n\n::: {.callout-tip}\n**Tip:** use CART when you need interpretability, and quick decisions and you don't care about overfitting.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\ntree_wf <- workflow() %>%\n  add_model(tree_model) %>%\n  add_formula(Class ~ .)\n\ntree_fit <- tree_wf %>% fit(data = train_data)\n```\n:::\n\n\n\n\n## Random Forest Model\n\nA random forest is an ensemble method that builds multiple decision trees and combines their outputs. It works as follows:\n\n1. Randomly samples data (with replacement) to create different training sets (bootstrap sampling).\n\n2. Constructs multiple decision trees using subsets of features at each split.\n\n3. Aggregates the predictions from all trees (majority vote for classification, average for regression).\n\n4. Reduces overfitting by averaging across trees, improving generalization.\n\n::: {.callout-tip}\n**Tip:** use random forest when you need higher accuracy and robustness to overfitting.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train a Random Forest model\nrf_model <- rand_forest(mode = \"classification\") %>%\n  set_engine(\"ranger\")\n\nrf_wf <- workflow() %>%\n  add_model(rf_model) %>%\n  add_formula(Class ~ .)\n\nrf_fit <- rf_wf %>% fit(data = train_data)\n```\n:::\n\n\n\n\n## Model Evaluation\n\nWe evaluate the models using **accuracy**, **ROC curves**, and **other metrics**.\n\n### Understanding ROC Curves\n\nThe **Receiver Operating Characteristic (ROC) curve** is a graphical representation of a classification model’s performance. It shows the trade-off between **sensitivity (True Positive Rate)** and **1 - specificity (False Positive Rate)** across different threshold values. A higher area under the ROC curve (AUC) indicates better model performance.\n\nIn this case, since we have multiple classes, we use a **One-vs-All (OvA) approach**, where we compute an ROC curve for each class, treating it as a binary classification problem.\n\nWe evaluate the models using **accuracy**, **ROC curves**, and **other metrics**.\n\nWe evaluate the models using **accuracy** and **other metrics**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression\nlog_results <- predict(log_fit_simple, test_data, type = \"class\") %>%\n  bind_cols(test_data) %>%\n  metrics(truth = Class, estimate = .pred_class)\n\n# Multinomial logistic\nmultlog_results <- predict(log_fit_multinom, test_data, type = \"class\") %>%\n  bind_cols(test_data) %>%\n  metrics(truth = Class, estimate = .pred_class)\n\ntree_results <- predict(tree_fit, test_data, type = \"class\") %>%\n  bind_cols(test_data) %>%\n  metrics(truth = Class, estimate = .pred_class)\n# Random forest\nrf_results <- predict(rf_fit, test_data, type = \"class\") %>%\n  bind_cols(test_data) %>%\n  metrics(truth = Class, estimate = .pred_class)\n```\n:::\n\n\n\n\n### Logistic Regression Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.278\n2 kap      multiclass     0.158\n```\n\n\n:::\n\n```{.r .cell-code}\n# Generate ROC Curve for multinomial regression using One-vs-All approach\n\n# Convert prediction probabilities to wide format\ndata_roc <- predict(log_fit_simple, test_data, type = \"prob\") %>%\n  bind_cols(test_data)\n\n# Compute ROC curves for each class\n# roc_data <- data_roc %>%\n#   roc_curve(truth = Class, \n#             !!!syms(names(select(data_roc, starts_with(\".pred_\")))))\n\n# Plot ROC curves\n# autoplot(roc_data)\n```\n:::\n\n\n\n\n### Logistic Regression Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultlog_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.948\n2 kap      multiclass     0.940\n```\n\n\n:::\n\n```{.r .cell-code}\n# Generate ROC Curve for multinomial regression using One-vs-All approach\n\n# Convert prediction probabilities to wide format\ndata_roc <- predict(log_fit_multinom, test_data, type = \"prob\") %>%\n  bind_cols(test_data)\n\n# Compute ROC curves for each class\nroc_data <- data_roc %>%\n  roc_curve(truth = Class, \n            !!!syms(names(select(data_roc, starts_with(\".pred_\")))))\n\n# Plot ROC curves\nautoplot(roc_data)\n```\n\n::: {.cell-output-display}\n![](models_07_files/figure-html/multlog-results-1.png){width=672}\n:::\n:::\n\n\n\n\n### Decision Tree Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.896\n2 kap      multiclass     0.878\n```\n\n\n:::\n:::\n\n\n\n\n### Random Forest Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.956\n2 kap      multiclass     0.949\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute ROC Curve for Random Forest\nrf_roc <- predict(rf_fit, test_data, type = \"prob\") %>%\n  bind_cols(test_data) %>%\n  roc_curve(truth = Class, !!!syms(names(select(., starts_with(\".pred_\")))))\n\nautoplot(rf_roc)\n```\n\n::: {.cell-output-display}\n![](models_07_files/figure-html/rf-results-1.png){width=672}\n:::\n:::\n\n\n\n\n## 7. Visualizing Decision Tree\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the trained model from the workflow\ntree_model_extracted <- extract_fit_parsnip(tree_fit)\n\n# Plot the decision tree\nrpart.plot(tree_model_extracted$fit)\n```\n\n::: {.cell-output-display}\n![](models_07_files/figure-html/visualize-tree-1.png){width=672}\n:::\n:::\n\n\n\n\n## Conclusion\n\nThis tutorial demonstrates how to apply **logistic regression** and **decision tree classification** using the **tidyverse** approach in R. We also handled **class imbalance** with **SMOTE** and evaluated models based on accuracy.\n",
    "supporting": [
      "models_07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}