{
  "hash": "5c0af39cad01fac9f19c1f1ba1841d54",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Intro to Bayesian Statistics\"\nauthor: \"Dr. Adrian Correndo\"\ndate: \"2025-04-02\" \ncategories: [statistics, frequentism, bayes theorem] \neditor: source\nabstract-title: 'Intro' \nabstract: 'This article provides some basics about Bayesian statistics and a comparison with the conventional frequentist perspective about probabilities and stats.'\nformat: \n  html:\n    toc: true\n    toc-location: left \n    toc-depth: 4 \n    number-sections: true \n    table-class: \"table table-striped table-hover\" \n    editor: source \nexecute: \n  echo: true \n  warning: false \n  message: false \nsmooth-scroll: true \n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n::: callout-important\nNeither Frequentist nor Bayesian approaches are universally superior â€” each has strengths depending on the context. ğŸ˜‰\n:::\n\nToday, we'll explore and contrast both paradigms.\n\n# Frequentism vs Bayesianism\n\n::: {align=\"center\"}\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tsuJM_bHSgA\" frameborder=\"0\" allowfullscreen></iframe>\n:::\n\nWhat are your thoughts?\n\n- Let's open the floor for discussion!\n\n---\n\n## Main Differences\n\nThe key divergence lies in their treatment of TRUTH.\n\nFrequentism assumes the existence of a fixed, true value. Parameters are fixed, and randomness comes from data variation. It's called \"frequentism\" because it relies on the frequency of events under repeated sampling.\n\n> ğŸ² **Example:** To estimate the probability of rolling a 6, Frequentists say: â€œIf we roll a die infinitely many times, the proportion of 6s will approach 1/6.â€\n\nInference is based on idealized, repeated experiments â€” even ones we've never performed.\n\nIn contrast, Bayesianism does *not* assume a fixed truth. It centers on:\n\n**PROBABILITIES:** All knowledge is probabilistic â€” not true/false, but likely/unlikely.\n\n**BELIEFS:** Prior knowledge is incorporated into analysis via prior distributions. Even when we know nothing (ğŸ‘‰ uninformative priors!), Bayesianism lets us model uncertainty.\n\n> ğŸ² **Bayesian Dice:** â€œWe are *a priori* 16.7% certain weâ€™ll get a 6.â€\n\nBayesian inference updates beliefs based on observed data:\n\n$$\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n$$\n\nAnd to compare competing models or beliefs, we use the **Bayes Factor**:\n\n$$\n\\text{Bayes Factor} = \\frac{\\text{Posterior}}{\\text{Prior}} = \\frac{0.334}{0.167} = 2\n$$\n\nThis means the updated belief is twice as likely, given the data.\n\n### Summary:\n- **Frequentism**: Models are fixed; randomness is in the data.\n- **Bayesianism**: After being observed, the data is considered fixed; uncertainty lies in the model (parameters and predictions).\n\n::: callout-tip\nFor many simple models, both approaches yield similar conclusions.\n\nğŸ’¡ Hint: Consider uninformative priors!\n:::\n\n::: callout-note\nBayesian logic mirrors human reasoning:\n\n1. We collect data.\n2. We have prior beliefs.\n3. We combine both to update our beliefs.\n:::\n\n## Bayes Theorem\n\n$$\nP(A_{\\text{true}} \\mid B) = \\frac{P(B \\mid A_{\\text{true}}) \\cdot P(A_{\\text{true}})}{P(B)}\n$$\n\n$$\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n$$\n\n### Video: Bayes' Rule\n\n::: {align=\"center\"}\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HZGCoVF3YvM\" frameborder=\"0\" allowfullscreen></iframe>\n:::\n\n## The Priors\n\nPriors formalize our beliefs as probability distributions â€” based on:\n\n1. The nature of the variable (discrete/continuous)\n2. Our prior knowledge or lack thereof\n\n::: callout-tip\nğŸ§  **Bayesian Perspective on Uncertainty**\n\nIn Bayesian analysis, uncertainty is placed on:\n\n- **Parameters** (they are random variables with prior/posterior distributions)\n- **Predictions** (future outcomes are modeled probabilistically)\n\nOnce data is observed, it's considered fixed.\n\nSo rather than saying \"data is fixed, model is random,\" it's more precise to say:\n\n> \"Bayesian analysis treats unknown parameters and future data as random variables, while the observed data is fixed. The uncertainty lies in our beliefs about parameters and predictions, not in the data weâ€™ve already seen.\"\n\n:::\n\n## ğŸ“ˆ Visualization of Bayesian Updating\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 1, length.out = 300)\nprior <- exp(-((x - 0.3)^2) / 0.05)\nlikelihood <- exp(-((x - 0.7)^2) / 0.02)\nposterior <- exp(-((x - 0.6)^2) / 0.01)\n\n# Normalize\nprior <- prior / sum(prior)\nlikelihood <- likelihood / sum(likelihood)\nposterior <- posterior / sum(posterior)\n\ndf <- data.frame(x = x, Prior = prior, Likelihood = likelihood, Posterior = posterior)\n\ndf_long <- df %>% pivot_longer(cols = -x, names_to = \"Distribution\", values_to = \"Density\")\n\nggplot(df_long, aes(x = x, y = Density, color = Distribution)) +\n  geom_line(size = 1.2) +\n  labs(title = \"Bayesian Updating: Prior Ã— Likelihood â†’ Posterior\",\n       x = \"Parameter (\\u03B8)\", y = \"Density\") +\n  theme_classic() +\n  scale_color_manual(values = c(\"tomato\", \"steelblue\", \"darkgreen\"))\n```\n\n::: {.cell-output-display}\n![Fig 1. Bayesian updating: Prior Ã— Likelihood â†’ Posterior](bayes_01_files/figure-html/bayesian-updating-ggplot-1.png){width=672}\n:::\n:::\n\n\n\n\n## Credible vs. Confidence Intervals\n\nA crucial difference lies in interpreting uncertainty:\n\n- **Confidence Interval (Frequentist)**: â€œIf we repeat the experiment infinitely, 95% of the calculated intervals will contain the true value of $\\theta$.â€\n\n    > âš ï¸ $\\theta$ is fixed â€” it's either in the interval or not.\n\n- **Credible Interval (Bayesian)**: â€œThere is a 95% probability that $\\theta$ lies within the interval.â€\n\n    > ğŸ”‘ *Conditional on the prior being correct.*\n\n## Useful Resources\n\n### ğŸ§  Introductory Theory\n- [Bayesian Models: A Statistical Primer for Ecologists â€“ Hobbs & Hooten](https://www.amazon.com/dp/0691159289)\n- [Bringing Bayesian Models to Life â€“ Hooten & Hefley](https://www.amazon.com/dp/0367198487)\n- [Statistical Rethinking â€“ McElreath (PDF)](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf)\n\n### ğŸ“ Bayesian Workflow & Philosophy\n- [Bayesian workflow â€“ Gelman et al.](https://arxiv.org/abs/2011.01808)\n- [Scientific Reasoning: The Bayesian Approach â€“ Howson & Urbach](https://www.amazon.com/dp/081269578X)\n\n### ğŸ“˜ Advanced Theory\n- [Bayesian Data Analysis (3rd Edition)](https://www.amazon.com/dp/1439840954)\n\n### ğŸŒ± Agronomy Applications\n- [Makowski et al., 2020 (EJA)](https://doi.org/10.1016/j.eja.2020.126076)\n\n### ğŸ—£ï¸ Miscellaneous\n- [Statistical Modeling Blog â€“ Gelman et al.](https://statmodeling.stat.columbia.edu/)\n- [Learning Bayesian Statistics Podcast](https://open.spotify.com/show/7HYN0pLjob4d8RiwKTvLUa)\n",
    "supporting": [
      "bayes_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}