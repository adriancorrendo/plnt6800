{
  "hash": "74f60f3075c49dd2e7b00f07604973e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Statistics in R\"\nauthor: \"Dr. Adrian Correndo & Dr. Josefina Lacasa\"\ndate: \"2025-04-02\" \ncategories: [statistics, frequentism, bayes theorem] \neditor: source\nabstract-title: 'Intro' \nabstract: 'This is a follow-up from our \"Intro to Bayesian Statistics\" article. Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.'\nformat: \n  html:\n    toc: true\n    toc-location: left \n    toc-depth: 4 \n    number-sections: true \n    table-class: \"table table-striped table-hover\" \n    editor: source \nexecute: \n  echo: true \n  warning: false \n  message: false \nsmooth-scroll: true \n---\n\n\n\n\n\n::: callout-note\n## 📌 Today's Topics\n\nWe'll learn how to compute posterior distributions, step-by-step:\n\n1. 🎯 Acceptance/Rejection Sampling (AR Sampling)\n2. 🔁 Markov Chain Monte Carlo (MCMC) — more efficient than AR!\n\nAnd introduce powerful R packages for Bayesian modeling:\n\n3. 📦 `brms` — beginner-friendly interface to Stan\n4. 🔬 `rstan` — write your own Stan models\n5. 🧪 `rjags` — Gibbs sampling with BUGS syntax\n:::\n\n## 📦 Packages we'll use today\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)\n```\n:::\n\n\n\n\n## 🎲 Computing Posterior Distributions\n\n### 1️⃣ Acceptance/Rejection Sampling — Basics\n\nHere's how AR sampling works:\n\n1. Propose values for parameters\n2. Simulate data based on those values\n3. Measure how well it fits the observed data\n4. Accept if close enough (✔️), reject otherwise (❌)\n\n#### 🌽 Simulating Corn Yield vs. Plant Density\n\nWe simulate yield using a parabolic function:\n\n$$ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2 $$\n\nThen compare simulated data to the real observed values. If the \"fit\" is good enough, we keep those parameter values.\n\n👀 We'll visualize which parameter sets are accepted and which aren't!\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n$$ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.  Generate proposal parameter values **using the prior ditributions**:\n\n$$\\beta_0 \\sim uniform(4, 6)$$\n\n$$\\beta_1 \\sim uniform(1, 3)$$\n\n$$\\beta_2 \\sim uniform(0.1, 2)$$\n\n$$\\sigma \\sim Gamma(2, 2)$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6767)\nb0_try <- runif(1, 4, 6)  # Parameter model for intercept (Uniform)\nb1_try <- runif(1, 1, 3)  # Parameter model for slope (Uniform)\nb2_try <- rgamma(1, 0.1, 2) # Parameter model for quadratic term (Gamma)\n# Mathematical equation for process model\nmu_try <- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try <- rgamma(1, 2, 2)\n```\n:::\n\n\n\n\n2.  Generate data with those parameters\\\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(567)\ny_try <- rnorm(n, mu_try, sigma_try)  # Process model\n```\n:::\n\n\n\n\n3.  Compare the simulated data with the observed data = \"difference\"\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] <- sum(abs(y - y_try))\n```\n:::\n\n\n\n\n\n\n4.  \"**Accept**\" (gold) that combination of parameters if the difference \\< predifined acceptable error. \"**Reject**\" (red) if the difference \\> predifined acceptable error.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Corn Yield (Mg/ha)\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] < error, \"gold\", \"tomato\"))\n```\n\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/demo 1e-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/demo 1f-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/demo 1g-1.png){width=672}\n:::\n:::\n\n\n\n Now, what if whe change the priors:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/new_priors-1.png){width=672}\n:::\n:::\n\n\n\n\nNow, do many tries\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (k in 1:K_tries) {\n    \n    b0_try <- runif(1, 2, 10)  # Parameter model for intercept as uniform\n    b1_try <- rnorm(1, 2.2, .5)  # Parameter model for slope as normal or gaussian\n    b2_try <- rgamma(1, .25, 2) # Parameter model for quad term as gamma\n    # Mathematical equation for process model\n    mu_try <- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try <- rgamma(1, 2, 2)\n\n    y_try <- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] <- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] <- y_try\n    \n    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)\n}\n```\n:::\n\n\n\n\nAcceptance rate\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(which(diff < error))/K_tries\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.169531\n```\n\n\n:::\n:::\n\n\n\n\nPriors versus posteriors:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/post_b0-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/post_b1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/post_b2-1.png){width=672}\n:::\n:::\n\n\n\n#### 📊 Plot predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data\nfiltered_yhat <- y_hat[which(diff < error), 50]\ndf_yhat <- data.frame(pred = filtered_yhat)\n\n# Plot\nggplot(df_yhat, aes(x = pred)) +\n  # Posterior\n  geom_histogram(aes(y = ..density..), fill = \"grey\", color = \"black\", bins = 30) +\n  geom_vline(xintercept = y[50], color = \"gold\", linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    x = expression(hat(y)[50]),\n    y = \"Density\"\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/histo-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/plot_1-1.png){width=672}\n:::\n:::\n\n\n\n\nLet's get started\n\n## 🔁 Markov Chain Monte Carlo (MCMC)\n\n![](images/Handbook.jpg){width=\"241\"}\n\nMCMC methods changed Bayesian stats forever! 🧠🔥\n\n- They let us generate samples from **complex** distributions\n- They form a **chain**, where each sample depends on the previous\n- Used in packages like `brms`, `rstan`, and `rjags`\n\n📚 More info:\n- [MCMC Handbook](https://www.mcmchandbook.net/)\n- [MCMCpack](https://cran.r-project.org/package=MCMCpack)\n- [mcmc](https://cran.r-project.org/package=mcmc)\n- [Paper: Foundations of MCMC](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243)\n\n::: {align=\"center\"}\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Qqz5AJjyugM\" frameborder=\"0\" allowfullscreen></iframe>\n:::\n\n## `brms`: Bayesian Modeling Made Easy\n\n![](images/brms.png)\n\n🔗 Docs: <https://paul-buerkner.github.io/brms/>  \n🐛 Issues: <https://github.com/paul-buerkner/brms/issues>\n\n`brms` makes it easy to run complex Bayesian models — without writing Stan code manually. It’s inspired by `lme4`, so syntax feels familiar.\n\nIt supports a wide range of models:\n- Linear, GLM, survival, zero-inflated, ordinal, count, and more\n\n✨ We’ll use `brms` as our go-to interface in this session!\n\n📚 More info:\n- [JSS Article on brms](https://www.jstatsoft.org/article/view/v080i01)\n\n![](images/paste-6C740B97.png){width=\"336\"}\n\n### Fit brms\n\nLet's fit the example using the brms package.\n\n### brms pars\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n```\n:::\n\n\n\n\n### Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 01. Run model\nbayes_model <- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include <cmath>\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\n#bayes_model <- readRDS(file = \"bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n```\n\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/model-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualize model results\nbayes_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 61) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     5.42      0.27     4.89     5.95 1.00     2664     2753\nB1_Intercept     2.04      0.10     1.82     2.24 1.00     2719     2502\nB2_Intercept     0.11      0.01     0.10     0.13 1.00     2764     2641\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.73      0.07     0.61     0.88 1.00     2997     2715\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nmpling)\nChain 4: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 3: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 3: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.393 seconds (Warm-up)\nChain 3:                1.474 seconds (Sampling)\nChain 3:                1.867 seconds (Total)\nChain 3: \nChain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.525 seconds (Warm-up)\nChain 2:                1.631 seconds (Sampling)\nChain 2:                2.156 seconds (Total)\nChain 2: \nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.393 seconds (Warm-up)\nChain 1:                1.853 seconds (Sampling)\nChain 1:                2.246 seconds (Total)\nChain 1: \nChain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.437 seconds (Warm-up)\nChain 4:                2.207 seconds (Sampling)\nChain 4:                2.644 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n\n#### Compare vs traditional linear model (lm)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_frame_q <- data_frame %>% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n      5.415        2.038       -0.114  \n```\n\n\n:::\n:::\n\n\n\n\n### Using posterior distributions\n#### Prepare summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictions\nm1 <- data_frame %>% \n  ungroup() %>% \n  dplyr::select(x) %>% \n  group_by(x) %>% filter(x == max(x)) %>% \n  ungroup() %>% unique() %>% rename(max = x) %>% \n  # Generate a sequence of x values\n  mutate(data = max %>% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %>% \n  unnest() %>% dplyr::select(-max) %>%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %>% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %>% ungroup()\n\n# Summarize\nm1_quantiles <- m1 %>% \n  group_by(x) %>% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n```\n:::\n\n\n\n\n#### Plot posterior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot\nm1_plot <- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk1\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"gold3\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T, \n              linetype = \"dashed\")+\n  scale_color_manual(values=c(\"purple4\", \"tomato\"))+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_classic()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot\n```\n\n::: {.cell-output-display}\n![](bayes_02_files/figure-html/plot_posterior-1.png){width=672}\n:::\n:::\n\n\n\n\n## `rstan`: Full Control with Stan\n\n![](images/stanlogo.png){width=\"190\"}\n\n🔗 Docs: <https://mc-stan.org/rstan/>  \n🐛 Issues: <https://github.com/stan-dev/rstan/issues>\n\nStan is a **powerful**, high-performance platform for Bayesian modeling, using:\n- **Hamiltonian Monte Carlo** (HMC)\n- **No-U-Turn Sampler** (NUTS)\n\nUnlike `brms`, Stan requires writing the full model — offering full flexibility and speed.\n\n✨ `brms` can even show the Stan code it generates under the hood!\n\nStan also supports Python, Julia, MATLAB, and more.\n\n---\n\n## `rjags`: Just Another Gibbs Sampler\n\n![](images/1601161_JAGS.png){width=\"318\"}\n\n🔗 Docs: <https://mcmc-jags.sourceforge.io/>  \n🐛 Issues: <https://sourceforge.net/projects/mcmc-jags/>\n\n`rjags` uses the classic **Gibbs Sampling** approach and the **BUGS** model syntax (used in WinBUGS, OpenBUGS).\n\n- More manual than `brms`\n- Ideal for users who want to write the full statistical model\n- Often paired with the `coda` package for diagnostics\n\n---\n\nHappy Bayesian coding! 💻✨",
    "supporting": [
      "bayes_02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}