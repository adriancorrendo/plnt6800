---
title: "Explanatory vs. Predictive Models in Agriculture with R"
author: "Dr. Adrian Correndo"
date: "2025-02-28"
categories: [statistical modelling, R, agriculture, predictive modeling]
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 4
    number-sections: true
    table-class: "table table-striped table-hover"
editor: source
execute:
  echo: true
  warning: false
  message: false
smooth-scroll: true
---

## Introduction

Statistical models in agriculture serve two primary purposes: **explanatory** and **predictive** modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.

This article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.

**Required packages:**
```{r}
library(pacman)
p_load(dplyr, tidyr)
p_load(ggplot2)
p_load(emmeans, multcomp, multcompView)
p_load(randomForest, caret, metrica)
```


## Explanatory Models

Explanatory models are designed to **understand** how different factors influence a response variable. These models help answer questions such as: *What are the main drivers of yield variation? How do nitrogen application and rainfall affect crop performance?*

### Example: Linear Regression for Explanation

```{r}

# Simulated agricultural data
data_ag <- data.frame(
  nitrogen = c(50, 100, 150, 200, 250, 300),
  rainfall = c(800, 850, 900, 950, 1000, 1050),
  yield = c(2.5, 3.1, 3.8, 4.2, 4.5, 4.6)
)

# Fit a linear model
lm_fit <- lm(yield ~ as.factor(nitrogen) + rainfall, data = data_ag)
summary(lm_fit)

```

### Interpretation

- The **coefficients** indicate the effect of each predictor on yield.
- The **p-values** help determine statistical significance.
- The **R-squared** value explains how much variance is accounted for by the model.

### Means Comparisons
```{r}
# Perform multiple comparisons
emmeans_fit <- emmeans(lm_fit, ~ nitrogen)
comp <- cld(emmeans_fit, Letters = letters)
comp
```

### Visualization
What are we missing here?
```{r}
# Create ggplot of estimated means
comp_plot <- ggplot(comp, aes(x = as.factor(nitrogen), y = emmean, fill = nitrogen)) +
  geom_col(color = "black") +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.2) +
  geom_text(aes(label = .group), vjust = -0.5, size = 5) +
  labs(title = "Means Comparison for Nitrogen Levels",
       x = "Nitrogen Levels (kg/ha)",
       y = "Estimated Yield (t/ha)") +
  theme_minimal()

comp_plot
```


## Predictive Models

Predictive models aim to **forecast** future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments. **Machine learning** models dominate here. A key challenge in predictive modeling is ensuring that the model **generalizes well** to unseen data, which is why we use techniques like **cross-validation**. An advantage is that we don't need repetitions of the data to use these models, but we need to have a good size so the algorithms can "learn" (machine learning).

### Cross-Validation and Generalization Performance

Cross-validation is a resampling technique used to evaluate a modelâ€™s ability to generalize to new data. It helps avoid overfitting, where a model performs well on training data but poorly on unseen data. One common method is **k-fold cross-validation**, where the dataset is split into k subsets, and the model is trained and tested multiple times.

- **Training Error**: The error the model makes on the data it was trained on.
- **Generalization Performance**: The model's ability to make accurate predictions on unseen data.
- **Validation Set Approach**: One practical method in agriculture is to leave out data from the latest year as a test set, ensuring the model is evaluated on future-like conditions.

### Updated Agricultural Dataset with Multiple Years

To better illustrate predictive modeling, we expand our dataset to include multiple years, allowing us to simulate a real-world scenario where we leave the latest year out for validation.

```{r}

# Simulated multi-year agricultural data
data_ag <- data.frame(
  year = rep(2011:2020, each = 50),  # 20 observations per year
  nitrogen = runif(500, 90, 300),
  rainfall = runif(500, 700, 1050),
  psnt = runif(500, 5, 60), # pre-sidedress N test (ppm)
  yield = 2 + 0.01 * runif(500, 90, 300) + 0.005 * runif(500, 700, 1050) + rnorm(500, 0, 0.2) - 0.002 * runif(500, 5, 60) 
)

# Splitting into training (excluding latest year) and test set (latest year only)
train_data <- data_ag %>% filter(year < 2020)
test_data <- data_ag %>% filter(year == 2020)
```

### Example: Random Forest with Cross-Validation

```{r}
# Set seed for reproducibility
set.seed(123)

# Train model with cross-validation using "caret" package
train_control <- caret::trainControl(method = "cv", number = 5)
rf_fit <- caret::train(yield ~ nitrogen + rainfall + psnt, data = train_data, method = "rf", trControl = train_control)

# Model Performance
print(rf_fit)

# Predict on the test set
predictions <- predict(rf_fit, test_data)

# Evaluate Generalization Performance
sqrt(mean((predictions - test_data$yield)^2))  # Root Mean Squared Error on test data

# Using the metrica package
metrica::RMSE(pred = predictions, obs = test_data$yield)

# Plot predicted vs observed scatter
metrica::scatter_plot(pred = predictions, obs = test_data$yield)

# With tidyverse syntax will be...
test_preds <- test_data %>% mutate(predictions = predict(rf_fit, test_data))

metrica::scatter_plot(data = test_preds, 
                      pred = predictions, obs = yield)

# Root mean square error
metrica::RMSE(pred = predictions, obs = test_data$yield)
# Relative mean square error (as a proportion)
metrica::RRMSE(data = test_preds, pred = predictions, obs = yield)

# Estimate more prediction error metrics
metrica::metrics_summary(data = test_preds, pred = predictions, obs = yield,
                         type = "regression")

```

### Interpretation

- **Cross-validation** ensures the model is not just memorizing the training data but generalizing well (e.g. predicting well on unseen observations).

- **Training vs. Test/Validation Performance**: Comparing error metrics between the training set and the unseen test set gives an estimate of real-world predictive ability. If the difference training between training error and testing error is too much, it's very likely our model is "over-fitted" (e.g. reading really well the training data but too much).

- **Leaving the latest year out** allows us to test predictions on future-like data, a common technique in agricultural forecasting.

Using these techniques ensures that predictive models in agriculture provide **reliable and actionable insights** rather than overfitted results that fail in practice.

Predictive models aim to **forecast** future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments.

### Example: Final Random Forest for Forecasting
Now we have our final model, we train one more time with all the available data, then predict new observations

```{r}

# Fit a random forest model
set.seed(123)

rf_fit <- randomForest(yield ~ nitrogen + rainfall + psnt, data = data_ag, ntree = 500)
print(rf_fit)

# Predict yield for new nitrogen and rainfall levels
new_data <- data.frame(field = c("Elora", "Waterloo", "Ridgetown", "Winchester"),
                       nitrogen = c(125, 175, 225, 220), 
                       rainfall = c(870, 920, 980, 1000),
                       psnt = c(35, 30, 45, 60))

# Adding predictions
new_preds <- new_data %>% mutate(predictions = predict(rf_fit, new_data))

```

### Interpretation

- This model is **non-parametric** and learns patterns from data.
- It is robust against outliers and complex interactions.
- Performance is evaluated using **Mean Squared Error (MSE)** or **R-squared**.

## Key Differences Between Explanatory and Predictive Models

| Feature | Explanatory Models | Predictive Models |
|---------|-------------------|-------------------|
| Purpose | Understanding relationships | Making accurate forecasts |
| Example | Linear regression, ANOVA | Machine learning (random forests, neural networks) |
| Assumptions | Requires assumptions about data distribution | Often non-parametric, flexible |
| Output | Coefficients, p-values | Predictions, accuracy metrics |

## Conclusion

Understanding the distinction between explanatory and predictive models is essential for agricultural research. While explanatory models help us understand **why** certain patterns exist, predictive models allow us to make **data-driven decisions** for future planning. A combination of both approaches can maximize insights and improve decision-making in precision agriculture.

This article brings simple examples in R using linear regression for explanatory analysis and random forests for prediction. Depending on the research question, both modeling strategies play a crucial role in agricultural data science.

