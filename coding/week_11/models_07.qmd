---
title: "Classification models in R"
author: "Dr. Adrian Correndo"
date: "2025-03-19" 
categories: [classification, logistic regression, cart, random forest, machine learning, R, agriculture] 
abstract-title: 'Summary' 
abstract: 'This tutorial provides a short review about classification models using categorical variables as the response variable. For this, we will cover logistic regression (binomial and multinomial), as well as classification trees and random forest (bootstrapped trees) all using the "tidymodels" approach.'
format: 
  html:
    toc: true
    toc-location: left 
    toc-depth: 4 
    number-sections: true 
    table-class: "table table-striped table-hover" 
    editor: source 
execute: 
  echo: true 
  warning: false 
  message: false 
smooth-scroll: true 
bibliography: references.bib 
link-citations: TRUE
---

# Introduction

This document demonstrates how to apply **Logistic Regression** @james2013 and **tree-based models** [@therneau1997, @breiman2001] using the **tidyverse** approach in R. We use the Dry Bean Dataset from the UCI Machine Learning Repository.

**Packages required for today**

```{r setup, include=FALSE}
library(pacman) # to install/load packages
p_load(tidyverse) # data wrangling and ggplot
p_load(tidymodels) # to fit predictive models
p_load(readr, readxl) # to open files
p_load(rpart) # to fit classification tree
p_load(rpart.plot) # to plot classification tree
p_load(themis)  # For SMOTE (handling class imbalance)
p_load(pROC) # to plot roc curve of glm
p_load(ranger) # random forest
```

# Simple Binary Classification Example (Iris Dataset)

## Logistic regression

We will start with a simple example using the **Iris dataset**, where we will create a binary classification problem.

### `logistic_reg()` from tidymodels

We will filter the dataset to only include **two species** (`setosa` and `versicolor`) and train a logistic regression model.

```{r iris-logistic-tidymodels}
# Load iris dataset
iris_binary <- iris %>%
  filter(Species != "versicolor") %>%  # Keep only two classes
  mutate(Species = as.factor(Species))

# Split the data
set.seed(42)
iris_split <- initial_split(iris_binary, prop = 0.8, strata = Species)
iris_train <- training(iris_split)
iris_test <- testing(iris_split)

# Train logistic regression using tidymodels
iris_log_model <- 
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

iris_log_wf <- workflow() %>%
  add_model(iris_log_model) %>%
  add_formula(Species ~ .)

iris_log_fit <- iris_log_wf %>% fit(data = iris_train)

# Evaluate model
iris_predictions <- predict(iris_log_fit, iris_test, type = "class") %>%
  bind_cols(iris_test)

iris_metrics <- iris_predictions %>%
  metrics(truth = Species, estimate = .pred_class)

iris_metrics
```

### `glm()` from base R

Alternatively, we can use the more **classic** approach with `glm()`.

```{r}
# Train logistic regression using glm
iris_glm <- glm(Species ~ ., data = iris_train, family = binomial)

# Predict on test data
iris_glm_probs <- predict(iris_glm, iris_test, type = "response")

# Ensure the response variable is a factor with two levels
iris_test <- iris_test %>% 
  mutate(Species = factor(Species, levels = c("setosa", "virginica")))

# Plot ROC Curve
roc_curve_glm <- roc(response = iris_test$Species, predictor = iris_glm_probs, levels = rev(levels(iris_test$Species)))
plot(roc_curve_glm, col = "blue", main = "ROC Curve for GLM Logistic Regression")
auc(roc_curve_glm)
```

# Model Evaluation

We evaluate the models using accuracy, Kappa statistic, ROC curves, and other metrics.

## Confusion Matrix

In machine learning and statistics, a **confusion matrix** is used to evaluate the performance of a classification model. It compares predicted labels against actual ground truth labels.

Below is a conceptual confusion matrix explaining **True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN):**

| Actual / Predicted  | Positive Prediction | Negative Prediction |
|----------------------|--------------------|--------------------|
| **Actual Positive** | **True Positive (TP)**: Correctly predicted positive | **False Negative (FN)**: Incorrectly predicted as negative |
| **Actual Negative** | **False Positive (FP)**: Incorrectly predicted as positive | **True Negative (TN)**: Correctly predicted negative |

## Explanation of Terms:
- **True Positive (TP):** Model correctly predicts a positive instance.
- **False Positive (FP):** Model incorrectly predicts a negative instance as positive (Type I error).
- **True Negative (TN):** Model correctly predicts a negative instance.
- **False Negative (FN):** Model incorrectly predicts a positive instance as negative (Type II error).

This matrix helps in evaluating performance metrics such as **accuracy, precision, recall, and F1-score**.

## Classification Metrics

### Accuracy

Accuracy is one of the most popular metrics that measures the proportion of correctly classified cases:

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$ 
- A value of 0.50 suggests the model is predicting correctly only half the time, which is just slightly better than random guessing. 
- Higher accuracy (closer to 1.00) is desirable, but accuracy alone doesn’t always reflect the model’s performance, especially in imbalanced datasets. 

### Kappa Statistic

Kappa (Cohen’s Kappa) measures how well the model performs relative to random chance:

$$\kappa = \frac{p_o - p_e}{1 - p_e}$$

Where: $p_o$ is the observed accuracy, $p_e$ is the expected accuracy due to chance.

Interpretation of Kappa Values: • Kappa = 1.00 → Perfect agreement. • Kappa \> 0.80 → Strong agreement. • Kappa between 0.40 and 0.60 → Moderate agreement. • Kappa \< 0.40 → Poor agreement. • Kappa = 0 → The model is no better than random chance.

In our Iris example, a Kappa of 0.333 suggests that while the model performs better than random, it still struggles to distinguish between the classes.

## ROC Curve & AUC (Area Under the Curve)

The Receiver Operating Characteristic (ROC) curve shows the trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity).

Interpretation of AUC Values: 
- AUC = 1.00 → Perfect classification. 
- AUC > 0.90 → Excellent discrimination. 
- AUC between 0.80 - 0.90 → Good classification. 
- AUC between 0.70 - 0.80 → Fair classification. 
- AUC = 0.50 → Random guessing.

In our Iris example, an AUC of 1.00 means that the model separates the two species perfectly. However, this should be interpreted alongside accuracy and kappa to ensure it’s not due to overfitting.

If accuracy is low (e.g., 50%) but AUC is high (1.00), this could indicate a problem in threshold selection or that the dataset is too simple for the model.

This example shows both the **modern tidymodels approach** and the **classic glm() approach** for binary classification.

# Multinomial Classification

## Dry beans dataset

```{r load-data}
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip"
#download.file(url, "DryBeanDataset.zip")
#unzip("DryBeanDataset.zip", exdir = "./DryBeanDataset")

df <- readxl::read_xlsx(path="data/Dry_Bean_Dataset.xlsx")

```

Lets take a look at the structure and summary of the dataset.

```{r explore-data}
glimpse(df)
skimr::skim(df)
```

## Data Preprocessing

We handle class imbalance using **SMOTE** and ensure the target variable is a factor.

```{r preprocess-data}
df <- df %>% 
  mutate(Class = as.factor(Class)) %>%
  recipe(Class ~ .) %>%
  step_smote(Class) %>%
  prep() %>%
  juice()

```

### Training and Testing Split

```{r split-data}
set.seed(42)
data_split <- initial_split(df, prop = 0.8, strata = Class)
train_data <- training(data_split)
test_data <- testing(data_split)
```

## Logistic Regression Model

```{r train-logistic-regression}
# First, fit a standard logistic regression model
log_model_simple <- logistic_reg() %>%  # Logistic regression (binary only)
  set_engine("glm") %>%
  set_mode("classification")

log_wf_simple <- workflow() %>%
  add_model(log_model_simple) %>%
  add_formula(Class ~ .)

log_fit_simple <- log_wf_simple %>% fit(data = train_data)
```

::: {.callout-warning}
**Warning:** Be cautious because logistic regression is only suitable for binary classification. Since we have 7 classes, a multinomial regression is more appropriate.
:::

## Multinomial Logistic Regression Model

```{r train-multinomial}
log_model_multinom <- multinom_reg() %>%  # Multinomial logistic regression
  set_engine("nnet") %>%
  set_mode("classification")

log_wf_multinom <- workflow() %>%
  add_model(log_model_multinom) %>%
  add_formula(Class ~ .)

log_fit_multinom <- log_wf_multinom %>% fit(data = train_data)
```

## Classification Tree Model

A classification tree is a single decision tree that splits the data based on feature values to classify observations. It follows these steps:

1. Splits data at each node based on a feature that minimizes impurity (e.g., Gini index, entropy).

2. Forms a tree structure where each leaf represents a predicted class.

3. Prone to **overfitting**, as it may capture noise in the training data.

::: {.callout-tip}
**Tip:** use CART when you need interpretability, and quick decisions and you don't care about overfitting.
:::

```{r train-decision-tree}
tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wf <- workflow() %>%
  add_model(tree_model) %>%
  add_formula(Class ~ .)

tree_fit <- tree_wf %>% fit(data = train_data)
```

## Random Forest Model

A random forest is an ensemble method that builds multiple decision trees and combines their outputs. It works as follows:

1. Randomly samples data (with replacement) to create different training sets (bootstrap sampling).

2. Constructs multiple decision trees using subsets of features at each split.

3. Aggregates the predictions from all trees (majority vote for classification, average for regression).

4. Reduces overfitting by averaging across trees, improving generalization.

::: {.callout-tip}
**Tip:** use random forest when you need higher accuracy and robustness to overfitting.
:::

```{r}
# Train a Random Forest model
rf_model <- rand_forest(mode = "classification") %>%
  set_engine("ranger")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_formula(Class ~ .)

rf_fit <- rf_wf %>% fit(data = train_data)
```

## Model Evaluation

We evaluate the models using **accuracy**, **ROC curves**, and **other metrics**.

### Understanding ROC Curves

The **Receiver Operating Characteristic (ROC) curve** is a graphical representation of a classification model’s performance. It shows the trade-off between **sensitivity (True Positive Rate)** and **1 - specificity (False Positive Rate)** across different threshold values. A higher area under the ROC curve (AUC) indicates better model performance.

In this case, since we have multiple classes, we use a **One-vs-All (OvA) approach**, where we compute an ROC curve for each class, treating it as a binary classification problem.

We evaluate the models using **accuracy**, **ROC curves**, and **other metrics**.

We evaluate the models using **accuracy** and **other metrics**.

```{r evaluate-models}
# Logistic regression
log_results <- predict(log_fit_simple, test_data, type = "class") %>%
  bind_cols(test_data) %>%
  metrics(truth = Class, estimate = .pred_class)

# Multinomial logistic
multlog_results <- predict(log_fit_multinom, test_data, type = "class") %>%
  bind_cols(test_data) %>%
  metrics(truth = Class, estimate = .pred_class)

tree_results <- predict(tree_fit, test_data, type = "class") %>%
  bind_cols(test_data) %>%
  metrics(truth = Class, estimate = .pred_class)
# Random forest
rf_results <- predict(rf_fit, test_data, type = "class") %>%
  bind_cols(test_data) %>%
  metrics(truth = Class, estimate = .pred_class)
```

### Logistic Regression Results

```{r log-results}
log_results

# Generate ROC Curve for multinomial regression using One-vs-All approach

# Convert prediction probabilities to wide format
data_roc <- predict(log_fit_simple, test_data, type = "prob") %>%
  bind_cols(test_data)

# Compute ROC curves for each class
# roc_data <- data_roc %>%
#   roc_curve(truth = Class, 
#             !!!syms(names(select(data_roc, starts_with(".pred_")))))

# Plot ROC curves
# autoplot(roc_data)
```

### Logistic Regression Results

```{r multlog-results}
multlog_results

# Generate ROC Curve for multinomial regression using One-vs-All approach

# Convert prediction probabilities to wide format
data_roc <- predict(log_fit_multinom, test_data, type = "prob") %>%
  bind_cols(test_data)

# Compute ROC curves for each class
roc_data <- data_roc %>%
  roc_curve(truth = Class, 
            !!!syms(names(select(data_roc, starts_with(".pred_")))))

# Plot ROC curves
autoplot(roc_data)
```

### Decision Tree Results

```{r tree-results}
tree_results
```

### Random Forest Results

```{r rf-results}
rf_results

# Compute ROC Curve for Random Forest
rf_roc <- predict(rf_fit, test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_curve(truth = Class, !!!syms(names(select(., starts_with(".pred_")))))

autoplot(rf_roc)
```

## 7. Visualizing Decision Tree

```{r visualize-tree}
# Extract the trained model from the workflow
tree_model_extracted <- extract_fit_parsnip(tree_fit)

# Plot the decision tree
rpart.plot(tree_model_extracted$fit)
```

## Conclusion

This tutorial demonstrates how to apply **logistic regression** and **decision tree classification** using the **tidyverse** approach in R. We also handled **class imbalance** with **SMOTE** and evaluated models based on accuracy.
