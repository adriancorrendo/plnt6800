---
title: "Bayesian Statistics in R"
author: "Dr. Adrian Correndo & Dr. Josefina Lacasa"
date: "2025-04-02" 
categories: [statistics, frequentism, bayes theorem] 
editor: source
abstract-title: 'Intro' 
abstract: 'This is a follow-up from our "Intro to Bayesian Statistics" article. Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.'
format: 
  html:
    toc: true
    toc-location: left 
    toc-depth: 4 
    number-sections: true 
    table-class: "table table-striped table-hover" 
    editor: source 
execute: 
  echo: true 
  warning: false 
  message: false 
smooth-scroll: true 
---


::: callout-note
## ğŸ“Œ Today's Topics

We'll learn how to compute posterior distributions, step-by-step:

1. ğŸ¯ Acceptance/Rejection Sampling (AR Sampling)
2. ğŸ” Markov Chain Monte Carlo (MCMC) â€” more efficient than AR!

And introduce powerful R packages for Bayesian modeling:

3. ğŸ“¦ `brms` â€” beginner-friendly interface to Stan
4. ğŸ”¬ `rstan` â€” write your own Stan models
5. ğŸ§ª `rjags` â€” Gibbs sampling with BUGS syntax
:::

## ğŸ“¦ Packages we'll use today

```{r include=TRUE, warning=FALSE, message=FALSE}
library(latex2exp)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
library(purrr)
library(brms)
library(tidybayes)
```

## ğŸ² Computing Posterior Distributions

### 1ï¸âƒ£ Acceptance/Rejection Sampling â€” Basics

Here's how AR sampling works:

1. Propose values for parameters
2. Simulate data based on those values
3. Measure how well it fits the observed data
4. Accept if close enough (âœ”ï¸), reject otherwise (âŒ)

#### ğŸŒ½ Simulating Corn Yield vs. Plant Density

We simulate yield using a parabolic function:

$$ y = \beta_0 + x \cdot \beta_1 - x^2 \cdot \beta_2 $$

Then compare simulated data to the real observed values. If the "fit" is good enough, we keep those parameter values.

ğŸ‘€ We'll visualize which parameter sets are accepted and which aren't!

```{r, echo=FALSE}
b0_true <- 5
b1_true <- 2.2
b2_true <- .125

x <- seq(0, 12, by = .2)
mu <- b0_true + x*b1_true - (x^2)*b2_true

set.seed(42)
y <- mu + rnorm(length(x), 0, sqrt(.4))

data_frame <- new_tibble(list(x = x, y = y))

# Plot
data_frame %>% 
ggplot(data=.)+
  geom_point(aes(x = x, y = y), shape = 21, size = 2)+
  geom_smooth(aes(x = x, y = y), method = lm, formula= y ~ x + I(x^2))+
  theme_classic()


```

$$ y = \beta_0 + x \cdot \beta_1 - x^2 \cdot \beta_2$$

```{r settings, include=FALSE}
# Algorithm settings
K_tries <- 10^6  # Number of simulated data sets to make
diff <- matrix(, K_tries, 1)  # Vector to save the measure of discrepancy between simulated data and real data
error <- 200  # Allowable difference between simulated data and real data

# Known random variables and parameters
n <- length(x)
```

```{r prep objects, include=FALSE}
posterior_samp_parameters <- matrix(, K_tries, 4)  # Matrix to samples of save unknown parameters
colnames(posterior_samp_parameters) <- c("b0", "b1", "b2", "sigma")

y_hat <- matrix(, K_tries, n)  # Matrix to samples of save unknown number of whooping cranes
```

```{r demo acc-rej, include=FALSE}
k = 1
```

1.  Generate proposal parameter values **using the prior ditributions**:

$$\beta_0 \sim uniform(4, 6)$$

$$\beta_1 \sim uniform(1, 3)$$

$$\beta_2 \sim uniform(0.1, 2)$$

$$\sigma \sim Gamma(2, 2)$$

```{r demo 1}
set.seed(6767)
b0_try <- runif(1, 4, 6)  # Parameter model for intercept (Uniform)
b1_try <- runif(1, 1, 3)  # Parameter model for slope (Uniform)
b2_try <- rgamma(1, 0.1, 2) # Parameter model for quadratic term (Gamma)
# Mathematical equation for process model
mu_try <- b0_try + x*b1_try - (x^2)*b2_try
sigma_try <- rgamma(1, 2, 2)
```

2.  Generate data with those parameters\

```{r demo 1b}
set.seed(567)
y_try <- rnorm(n, mu_try, sigma_try)  # Process model
```

3.  Compare the simulated data with the observed data = "difference"

```{r demo 1c}
# Record difference between draw of y from prior predictive distribution and
# observed data
diff[k, ] <- sum(abs(y - y_try))
```

```{r demo 1d, include=FALSE}
# Save unkown random variables and parameters
y_hat[k, ] <- y_try

posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)
```

4.  "**Accept**" (gold) that combination of parameters if the difference \< predifined acceptable error. "**Reject**" (red) if the difference \> predifined acceptable error.

```{r demo 1e}
plot(x, y, xlab = "Plant density", 
     ylab = "Corn Yield (Mg/ha)", xlim = c(2, 13), ylim = c(5, 20),
     typ = "b", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))
points(x, y_hat[k,], typ = "b", lwd = 2, 
       col = ifelse(diff[1] < error, "gold", "tomato"))
```

```{r demo 1f, echo=FALSE}
set.seed(567567)
k = 1
b0_try <- runif(1, 4, 6)  # Parameter model
b1_try <- runif(1, 1, 3)  # Parameter model 
b2_try <- rgamma(1, 0.1, 2) # Mathematical equation for process model
mu_try <- b0_try + x*b1_try - (x^2)*b2_try
sigma_try <- rgamma(1, 2, 2)

y_try <- rnorm(n, mu_try, sigma_try)  # Process model

# Record difference between draw of y from prior predictive distribution and
# observed data
diff[k, ] <- sum(abs(y - y_try))

# Save unkown random variables and parameters
y_hat[k, ] <- y_try

posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)

plot(x, y, xlab = "Plant density", 
     ylab = "Observed yield", xlim = c(2, 13), ylim = c(5, 20),
     typ = "b", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))

points(x, y_hat[k,], typ = "b", lwd = 2, 
       col = ifelse(diff[1] < error, "gold", "tomato"))

```

```{r demo 1g, echo=FALSE}
set.seed(76543)
k = 1
b0_try <- runif(1, 4, 6)  # Parameter model for intercept (uniform)
b1_try <- runif(1, 1, 3)  # Parameter model for slope (uniform)
b2_try <- rgamma(1, .5, 2) # Parameter model for quadratice term (gamma)
# Mathematical equation for process model
mu_try <- b0_try + x*b1_try - (x^2)*b2_try
sigma_try <- rgamma(1, 2, 2)

y_try <- rnorm(n, mu_try, sigma_try)  # Process model

# Record difference between draw of y from prior predictive distribution and
# observed data
diff[k, ] <- sum(abs(y - y_try))

# Save unkown random variables and parameters
y_hat[k, ] <- y_try

posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)

plot(x, y, xlab = "Plant density", 
     ylab = "Observed yield", xlim = c(2, 13), ylim = c(5, 20),
     typ = "b", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))

points(x, y_hat[k,], typ = "b", lwd = 2, 
       col = ifelse(diff[1] < error, "gold", "tomato"))

```
 Now, what if whe change the priors:

```{r new_priors, echo=FALSE}
k = 1
b0_try <- rnorm(1, 5, .01)  # Parameter model for intercept (normal or gaussian)
b1_try <- rnorm(1, 2.2, .01)  # Parameter model for slope (normal or gaussian)
b2_try <- rgamma(1, .1, 2) # Parameter model for quad term (gamma)
# Mathematical equation for process model
mu_try <- b0_try + x*b1_try - (x^2)*b2_try
sigma_try <- rgamma(1, 2, 2)

y_try <- rnorm(n, mu_try, sigma_try)  # Process model

# Record difference between draw of y from prior predictive distribution and
# observed data
diff[k, ] <- sum(abs(y - y_try))

# Save unkown random variables and parameters
y_hat[k, ] <- y_try

posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)

plot(x, y, xlab = "Plant density",
     ylab = "Observed yield", xlim = c(2, 13), ylim = c(5, 20),
     typ = "b", cex = 0.8, pch = 20, col = "grey20")

points(x, y_hat[k,], typ = "b",
       lwd = 2, 
       col = ifelse(diff[1] < error, "gold", "tomato"))
```

Now, do many tries

```{r for_loop}
for (k in 1:K_tries) {
    
    b0_try <- runif(1, 2, 10)  # Parameter model for intercept as uniform
    b1_try <- rnorm(1, 2.2, .5)  # Parameter model for slope as normal or gaussian
    b2_try <- rgamma(1, .25, 2) # Parameter model for quad term as gamma
    # Mathematical equation for process model
    mu_try <- b0_try + x*b1_try - (x^2)*b2_try
    sigma_try <- rgamma(1, 2, 2)

    y_try <- rnorm(n, mu_try, sigma_try)  # Process model
    
    # Record difference between draw of y from prior predictive distribution and
    # observed data
    diff[k, ] <- sum(abs(y - y_try))
    
    # Save unkown random variables and parameters
    y_hat[k, ] <- y_try
    
    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)
}
```

Acceptance rate

```{r accept}
length(which(diff < error))/K_tries
```

Priors versus posteriors:

```{r post_b0, echo=FALSE}
# Prepare filtered data
filtered_b0 <- posterior_samp_parameters[which(diff < error), 1]
df_b0 <- data.frame(beta0 = filtered_b0)

# Plot
ggplot(df_b0, aes(x = beta0)) +
  geom_histogram(aes(y = ..density..), fill = "#dde5b6", color = "black", bins = 30) +
  stat_function(fun = dunif, args = list(min = 2, max = 10), color = "tomato", size = 1.2) +
  xlim(1.5, 10.5) +
  labs(
    x = expression(beta[0] ~ "|" ~ y),
    y = expression("[" ~ beta[0] ~ "|" ~ y ~ "]")
  ) +
  theme_classic()
```

```{r post_b1, echo=FALSE}
# Prepare filtered data
filtered_b1 <- posterior_samp_parameters[which(diff < error), 2]
df_b1 <- data.frame(beta1 = filtered_b1)

# Plot
ggplot(df_b1, aes(x = beta1)) +
  # Posterior
  geom_histogram(aes(y = ..density..), fill = "#bfd8bd", color = "black", bins = 30) +
  # Prior
  stat_function(fun = dnorm, args = list(mean = 2.2, sd = 0.5), color = "tomato", size = 1.2) +
  xlim(2, 3) +
  labs(
    x = expression(beta[1] ~ "|" ~ y),
    y = expression("[" ~ beta[1] ~ "|" ~ y ~ "]")
  ) +
  theme_classic()
```

```{r post_b2, echo=FALSE}
# Prepare filtered data
filtered_values <- posterior_samp_parameters[which(diff < error), 3]
df <- data.frame(beta2 = filtered_values)

# Plot
ggplot(df, aes(x = beta2)) +
  # Posterior
  geom_histogram(aes(y = ..density..), fill = "#faf3dd", color = "black", bins = 30) +
  # Prior
  stat_function(fun = dgamma, args = list(shape = 0.25, rate = 2), color = "tomato", size = 1.2) +
  xlim(0, 1) +
  labs(
    x = expression(beta[2] ~ "|" ~ y),
    y = expression("[" ~ beta[2] ~ "|" ~ y ~ "]")
  ) +
  theme_classic()
```
#### ğŸ“Š Plot predictions
```{r histo}
# Prepare data
filtered_yhat <- y_hat[which(diff < error), 50]
df_yhat <- data.frame(pred = filtered_yhat)

# Plot
ggplot(df_yhat, aes(x = pred)) +
  # Posterior
  geom_histogram(aes(y = ..density..), fill = "grey", color = "black", bins = 30) +
  geom_vline(xintercept = y[50], color = "gold", linetype = "dashed", linewidth = 1.2) +
  labs(
    x = expression(hat(y)[50]),
    y = "Density"
  ) +
  theme_classic()
```

```{r final, include=FALSE}
e.y <- colMeans(y_hat[which(diff < error), ])

lwr.CI <- apply(y_hat[which(diff < error), ], 2, FUN = quantile, prob = c(0.025))
upper.CI <- apply(y_hat[which(diff < error), ], 2, FUN = quantile, prob = c(0.975))

# Data setup
df_pred <- data.frame(
  x = x,
  y_obs = y,
  y_mean = e.y,
  lwr = lwr.CI,
  upr = upper.CI
)
```

```{r plot_1, echo=FALSE}
# Plot
ggplot(df_pred, aes(x = x)) +
  geom_point(aes(y = y_obs), color = "grey20", size = 2) +
  geom_line(aes(y = y_obs), color = "grey20", linetype = "solid") +
  geom_line(aes(y = y_mean), color = "black", linewidth = 1.2) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "grey50", alpha = 0.3) +
  labs(
    x = "Plant density",
    y = "Observed yield"
  ) +
  coord_cartesian(xlim = c(2, 13), ylim = c(5, 20)) +
  theme_classic()
```

Let's get started

## ğŸ” Markov Chain Monte Carlo (MCMC)

![](images/Handbook.jpg){width="241"}

MCMC methods changed Bayesian stats forever! ğŸ§ ğŸ”¥

- They let us generate samples from **complex** distributions
- They form a **chain**, where each sample depends on the previous
- Used in packages like `brms`, `rstan`, and `rjags`

ğŸ“š More info:
- [MCMC Handbook](https://www.mcmchandbook.net/)
- [MCMCpack](https://cran.r-project.org/package=MCMCpack)
- [mcmc](https://cran.r-project.org/package=mcmc)
- [Paper: Foundations of MCMC](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243)

::: {align="center"}
<iframe width="560" height="315" src="https://www.youtube.com/embed/Qqz5AJjyugM" frameborder="0" allowfullscreen></iframe>
:::

## `brms`: Bayesian Modeling Made Easy

![](images/brms.png)

ğŸ”— Docs: <https://paul-buerkner.github.io/brms/>  
ğŸ› Issues: <https://github.com/paul-buerkner/brms/issues>

`brms` makes it easy to run complex Bayesian models â€” without writing Stan code manually. Itâ€™s inspired by `lme4`, so syntax feels familiar.

It supports a wide range of models:
- Linear, GLM, survival, zero-inflated, ordinal, count, and more

âœ¨ Weâ€™ll use `brms` as our go-to interface in this session!

ğŸ“š More info:
- [JSS Article on brms](https://www.jstatsoft.org/article/view/v080i01)

![](images/paste-6C740B97.png){width="336"}

### Fit brms

Let's fit the example using the brms package.

### brms pars

```{r priors}
# Set up pars
WU = 1000
IT = 5000
TH = 5
CH = 4
AD = 0.99
```

### Model

```{r model, message=FALSE, warning=FALSE}

# 01. Run model
bayes_model <- 

  brms::brm(
  #Priors
  prior = c(
    #B0, Intercept
    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),
    #B1, Linear Slope
    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),
    #B2, Quadratic coeff
    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),
    # Sigma  
    #prior(prior = 'gamma(15,1.3)', class = "sigma") ),  
    # Population prior (median and sd)
    
    # Formula
  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),
               # Hypothesis
               B0 + B1 + B2 ~ 1,
               nl = TRUE), 
  # Data  
  data = data_frame, sample_prior = "yes",
  # Likelihood of the data
  family = gaussian(link = 'identity'),
  # brms controls
  control = list(adapt_delta = AD),
  warmup = WU, iter = IT, thin = TH,
  chains = CH, cores = CH,
  init_r = 0.1, seed = 1) 

# 02. Save object
# saveRDS(object = bayes_model, file = "bayes_model.RDS")

# Load from file
#bayes_model <- readRDS(file = "bayes_model.RDS")

# 03. Visual Diagnostic
plot(bayes_model)

# Visualize model results
bayes_model
```

#### Compare vs traditional linear model (lm)
```{r compare}
data_frame_q <- data_frame %>% mutate(x2 = x^2)

lm(data = data_frame_q, formula = y ~ x + x2)

```

### Using posterior distributions
#### Prepare summary
```{r plot}
# Create predictions
m1 <- data_frame %>% 
  ungroup() %>% 
  dplyr::select(x) %>% 
  group_by(x) %>% filter(x == max(x)) %>% 
  ungroup() %>% unique() %>% rename(max = x) %>% 
  # Generate a sequence of x values
  mutate(data = max %>% purrr::map(~data.frame(
    x = seq(0,.,length.out = 400)))) %>% 
  unnest() %>% dplyr::select(-max) %>%
  
  #add_linpred_draws(m1, re_formula = NA, n = NULL) %>% ungroup()
  # use ".linpred to summarize"
  tidybayes::add_predicted_draws(bayes_model, 
                                 re_formula = NA, ndraws = NULL) %>% ungroup()

# Summarize
m1_quantiles <- m1 %>% 
  group_by(x) %>% 
  summarise(q025 = quantile(.prediction,.025),
            q010 = quantile(.prediction,.10),
            q250 = quantile(.prediction,.25),
            q500 = quantile(.prediction,.500),
            q750 = quantile(.prediction,.75),
            q900 = quantile(.prediction,.90),
            q975 = quantile(.prediction,.975))

```

#### Plot posterior
```{r plot_posterior}
# Plot
m1_plot <- ggplot()+
  # 95%
  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = "cornsilk1",
              aes(x=x, ymin=q025, ymax=q975))+
  # 80%
  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = "cornsilk3",
              aes(x=x, ymin=q010, ymax=q900))+
  # 50%
  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = "gold3",  
              aes(x=x, ymin=q250, ymax=q750))+
  geom_path(data = m1_quantiles,
            aes(x=x, y=q500, color = "brms()"), size = 1)+
  geom_point(data = data_frame, aes(x=x, y=y, color = "brms()"), alpha = 0.25)+
  # Add LM curve
  geom_smooth(data = data_frame, aes(x=x, y=y, color = "lm()"),  
              method = "lm", formula = y ~ poly(x,2), se = T, 
              linetype = "dashed")+
  scale_color_manual(values=c("purple4", "tomato"))+
  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+
  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+
  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+
  theme_classic()+
  theme(legend.position='right', 
        legend.title = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_text(size = rel(2)),
        axis.text = element_text(size = rel(1)),
        strip.text = element_text(size = rel(1.5)),
        )+
  labs(x = "Plant density (pl/m2)", y = "Corn yield (Mg/ha)")

m1_plot
```

## `rstan`: Full Control with Stan

![](images/stanlogo.png){width="190"}

ğŸ”— Docs: <https://mc-stan.org/rstan/>  
ğŸ› Issues: <https://github.com/stan-dev/rstan/issues>

Stan is a **powerful**, high-performance platform for Bayesian modeling, using:
- **Hamiltonian Monte Carlo** (HMC)
- **No-U-Turn Sampler** (NUTS)

Unlike `brms`, Stan requires writing the full model â€” offering full flexibility and speed.

âœ¨ `brms` can even show the Stan code it generates under the hood!

Stan also supports Python, Julia, MATLAB, and more.

---

## `rjags`: Just Another Gibbs Sampler

![](images/1601161_JAGS.png){width="318"}

ğŸ”— Docs: <https://mcmc-jags.sourceforge.io/>  
ğŸ› Issues: <https://sourceforge.net/projects/mcmc-jags/>

`rjags` uses the classic **Gibbs Sampling** approach and the **BUGS** model syntax (used in WinBUGS, OpenBUGS).

- More manual than `brms`
- Ideal for users who want to write the full statistical model
- Often paired with the `coda` package for diagnostics

---

Happy Bayesian coding! ğŸ’»âœ¨