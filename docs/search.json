[
  {
    "objectID": "coding/week_01/essentials_01.html",
    "href": "coding/week_01/essentials_01.html",
    "title": "Essentials of R coding I",
    "section": "",
    "text": "This page provides an overview of the essential types of elements in R, including examples and explanations for each. Use this as a quick reference to understand the basics of data types and operations.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#numbers",
    "href": "coding/week_01/essentials_01.html#numbers",
    "title": "Essentials of R coding I",
    "section": "01. Numbers",
    "text": "01. Numbers\n\n20\n\n[1] 20",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#math-operations",
    "href": "coding/week_01/essentials_01.html#math-operations",
    "title": "Essentials of R coding I",
    "section": "02. Math Operations",
    "text": "02. Math Operations\n\n20+1 # addition\n\n[1] 21\n\n20-4 # subtraction\n\n[1] 16\n\n20*5 # multiplication\n\n[1] 100\n\n20/5 # division\n\n[1] 4\n\n2^2 # exponentials\n\n[1] 4\n\nsqrt(9) # square root\n\n[1] 3\n\n# Greater exponents for roots\n# notation is: x^(1/n)\n\n# Cubic root of 27\n27^(1/3)  # Result: 3\n\n[1] 3\n\n# 4th root of 16\n16^(1/4)  # Result: 2\n\n[1] 2\n\n# 5th root of 32\n32^(1/5)  # Result: 2\n\n[1] 2",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#text-or-characters-also-called-strings",
    "href": "coding/week_01/essentials_01.html#text-or-characters-also-called-strings",
    "title": "Essentials of R coding I",
    "section": "03. Text or characters (also called strings)",
    "text": "03. Text or characters (also called strings)\n\n\"coding is fun\"\n\n[1] \"coding is fun\"\n\n\nBut these elements are not stored as objects yet:",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#define-objects",
    "href": "coding/week_01/essentials_01.html#define-objects",
    "title": "Essentials of R coding I",
    "section": "04. Define objects",
    "text": "04. Define objects\n\na &lt;- 20\n10 -&gt; b\n# We can also use equal:\nc = 15\n# But using \"&lt;-\", and leave = only for operations (so you can notice the difference) is considered a better coding practice.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#print-objects",
    "href": "coding/week_01/essentials_01.html#print-objects",
    "title": "Essentials of R coding I",
    "section": "05. Print objects",
    "text": "05. Print objects\n\na\n\n[1] 20\n\nprint(a)\n\n[1] 20\n\nb\n\n[1] 10\n\nc\n\n[1] 15",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#vectors",
    "href": "coding/week_01/essentials_01.html#vectors",
    "title": "Essentials of R coding I",
    "section": "06. Vectors",
    "text": "06. Vectors\nA vector is one of the most basic data structures. It is a sequence of elements of the same type, such as numbers, characters, or logical values. Vectors are used to store and manipulate collections of data efficiently. \n\na. Creating a vector\nVectors can be created using the c() function (combine function):\n\n# Numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4.5)\nnumeric_vector\n\n[1] 1.0 2.0 3.0 4.5\n\n# Character vector\ncharacter_vector &lt;- c(\"corn\", \"wheat\", \"soybean\")\ncharacter_vector\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n# Logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE)\nlogical_vector\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\nb. Accessing Elements\nYou can access elements of a vector using square brackets []:\n\n# Access the first element\nnumeric_vector[1]\n\n[1] 1\n\n# Access multiple elements\nnumeric_vector[c(1, 3)]\n\n[1] 1 3\n\n\n\n\nc. Vectorized Operations\nIn R, vector-operations are applied to each element automatically:\n\n# Adding a scalar to a vector\nnumeric_vector + 2\n\n[1] 3.0 4.0 5.0 6.5\n\n# Element-wise addition\nnumeric_vector + c(10, 20, 30, 40)\n\n[1] 11.0 22.0 33.0 44.5\n\n\n\n\nd. Common Functions with Vectors\n\n‘length()’: Get the number of elements in a vector.\n‘typeof()’ or ‘class()’: Determine the type of elements in a vector.\n‘seq()’: Generate a sequence of numbers.\n‘rep()’: Repeat elements to create a vector.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#lists",
    "href": "coding/week_01/essentials_01.html#lists",
    "title": "Essentials of R coding I",
    "section": "07. Lists",
    "text": "07. Lists\nIn R, a list is a versatile data structure that can contain elements of different types, including vectors, matrices, data frames, and even other lists. Unlike vectors, which are homogeneous, lists are heterogeneous, meaning their elements can be of different data types and lengths. \nKey Characteristics of Lists: \n\nHeterogeneous: Lists can store elements of varying types (numeric, character, logical, etc.) and structures (vectors, data frames, functions, etc.). \nIndexed: Elements in a list are accessed using double square brackets [[ ]] or named elements using $.\n\nWhy Use Lists? \n\nFlexibility: Lists can store complex and nested data structures. \nData Wrangling: Useful for handling results from models, nested data, or any mixed-type collections. \nFunctions: Functions in R often return their output as lists (e.g., lm()). \n\n\na. Creating a list\nLists are created using the list() function:\n\n# Create a list with different types of elements\nmy_list &lt;- list(\n  \"numeric_v\" = numeric_vector,\n  \"character_v\" = character_vector,\n  \"single_number\" = 42,\n  \"logical_value\" = TRUE\n)\n\n\n\nb. Accessing Elements in a List\nYou can access elements in a list by their position or name:\nBy Position:\n\n# Access the first element\nmy_list[[1]]\n\n[1] 1.0 2.0 3.0 4.5\n\n# Access the second element\nmy_list[[2]]\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n\nBy name:\n\n# Access by name\nmy_list$numeric_v\n\n[1] 1.0 2.0 3.0 4.5\n\nmy_list$character_v\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n\nSubelements:\n\n# Access the first value in the numeric vector\nmy_list$numeric_vector[1]\n\nNULL\n\n\n\n\nc. Some functions for lists\n\n# Number of elements in the list\nlength(my_list)\n\n[1] 4\n\n# Names of the elements\nnames(my_list)\n\n[1] \"numeric_v\"     \"character_v\"   \"single_number\" \"logical_value\"\n\n# Structure of the list\nstr(my_list)\n\nList of 4\n $ numeric_v    : num [1:4] 1 2 3 4.5\n $ character_v  : chr [1:3] \"corn\" \"wheat\" \"soybean\"\n $ single_number: num 42\n $ logical_value: logi TRUE",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#data-frame",
    "href": "coding/week_01/essentials_01.html#data-frame",
    "title": "Essentials of R coding I",
    "section": "08. Data frame",
    "text": "08. Data frame\nIn R, a data frame is a two-dimensional data structure used for storing tabular data. It is one of the most commonly used data structures in R for data analysis and manipulation. \nKey Characteristics of a Data Frame \n\nTabular Structure: Data is organized in rows and columns. \nHeterogeneous Columns: Each column can contain different data types (e.g., numeric, character, logical), but all elements in a column must be of the same type. \nRow and Column Names: Rows and columns can have names for easier identification. \n\nWhy Use a Data Frame? \n\nData Analysis: It is ideal for representing structured data like spreadsheets or databases. \nFlexible Operations: Columns can be easily added, removed, or modified. \nIntegration with R Functions: Many R functions for statistical modeling and analysis expect data frames as input. \n\n\na. Creating a Data Frame\nA data frame can be created using the data.frame() function:\n\n# Create a data frame\nmy_data &lt;- data.frame(\n  Crop = c(\"Corn\", \"Wheat\", \"Soybean\"), # Character column\n  Yield = c(180, 90, 50), # Numeric column\n  Legume = c(FALSE, FALSE, TRUE) # Logical column\n)\n\nprint(my_data)\n\n     Crop Yield Legume\n1    Corn   180  FALSE\n2   Wheat    90  FALSE\n3 Soybean    50   TRUE\n\n\n\n\nb. Accessing data in a data frame\nAccessing columns:\n\n# Access a column by name\nmy_data$Crop\n\n[1] \"Corn\"    \"Wheat\"   \"Soybean\"\n\n# Access a column by index\nmy_data[, 2]\n\n[1] 180  90  50\n\n\nAccessing rows:\n\n# Access the first row\nmy_data[1, ]\n\n  Crop Yield Legume\n1 Corn   180  FALSE\n\n# Access specific rows\nmy_data[c(1, 3), ]\n\n     Crop Yield Legume\n1    Corn   180  FALSE\n3 Soybean    50   TRUE\n\n\nAccessing specific elements\n\n# Access the element in the 2nd row, 3rd column\nmy_data[2, 3]\n\n[1] FALSE\n\n# Access specific cells by column name\nmy_data[2, \"Crop\"]\n\n[1] \"Wheat\"\n\n\n\n\nc. Adding a new column\n\nmy_data$Season &lt;- c(\"Summer\", \"Winter\", \"Summer\")\n\n\n\nd. Modify a column\n\nmy_data$Yield &lt;- my_data$Yield + 5\n\n\n\ne. Adding a new row\nIn base R, we can use rbind() to add rows:\n\nnew_row &lt;- data.frame(Crop = \"Barley\", Yield = 80, Legume = FALSE, Season = \"Winter\")\nmy_data &lt;- rbind(my_data, new_row)\n\n\n\nf. Filtering (rows)\nIn base R, we can use subset() to filter rows:\n\nsubset(my_data, Yield &gt; 150)\n\n  Crop Yield Legume Season\n1 Corn   185  FALSE Summer\n\n\nWe can also use logical conditions:\n\nmy_data[my_data$Legume == TRUE, ]\n\n     Crop Yield Legume Season\n3 Soybean    55   TRUE Summer\n\n\n\n\ng. Selecting (columns)\nIn base R, there is no function to select columns. We need to use brackets [] and vectors c():\n\nmy_data[c(\"Crop\", \"Yield\")]\n\n     Crop Yield\n1    Corn   185\n2   Wheat    95\n3 Soybean    55\n4  Barley    80\n\n\n\n\nh. Some functions for data frames\n\nnrow(my_data)        # Number of rows\n\n[1] 4\n\nncol(my_data)        # Number of columns\n\n[1] 4\n\ncolnames(my_data)    # Column names\n\n[1] \"Crop\"   \"Yield\"  \"Legume\" \"Season\"\n\nsummary(my_data)     # Summary statistics\n\n     Crop               Yield          Legume           Season         \n Length:4           Min.   : 55.00   Mode :logical   Length:4          \n Class :character   1st Qu.: 73.75   FALSE:3         Class :character  \n Mode  :character   Median : 87.50   TRUE :1         Mode  :character  \n                    Mean   :103.75                                     \n                    3rd Qu.:117.50                                     \n                    Max.   :185.00",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#matrix",
    "href": "coding/week_01/essentials_01.html#matrix",
    "title": "Essentials of R coding I",
    "section": "09. Matrix",
    "text": "09. Matrix\nIn R, a matrix is a two-dimensional, rectangular data structure that stores elements of the same type. It is similar to a data frame in structure but less flexible, as all elements in a matrix must be of a single data type (e.g., numeric, character, or logical). \nKey Characteristics of a Matrix \n\nHomogeneous: All elements in a matrix must be of the same type. \n2D Structure: A matrix has rows and columns, forming a table-like structure. \nDimensions: Defined by the number of rows and columns. \n\nWhy Use a Matrix? \n\nMathematical Operations: Ideal for linear algebra and mathematical modeling. \nEfficient Storage: Matrices use less memory compared to more complex structures like data frames. \nSimpler Operations: Homogeneous data ensures consistent behavior across elements. \n\n\na. Creating a Matrix\nYou can create a matrix using the matrix() function:\n\n# Create a numeric matrix\nmy_matrix &lt;- matrix(\n  data = 1:9,     # Data values\n  nrow = 3,       # Number of rows\n  ncol = 3,       # Number of columns\n)\n\nprint(my_matrix)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\nb. Accessing elements in a matrix\nAccessing rows:\n\n# Access the first row\nmy_matrix[1, ]\n\n[1] 1 4 7\n\n\nAccessing columns:\n\n# Access the second column\nmy_matrix[, 2]\n\n[1] 4 5 6\n\n\nAccessing specific elements:\n\n# Access the element in the 2nd row, 3rd column\nmy_matrix[2, 3]\n\n[1] 8\n\n\n\n\nc. Adding a new column\n\nnew_col &lt;- c(10, 11, 12) # Create the column\nmy_matrix &lt;- cbind(my_matrix, new_col) # Paste it to the existing\n\n\n\nd. Adding a new row\n\nnew_row &lt;- c(13, 14, 15, 16)\nmy_matrix &lt;- rbind(my_matrix, new_row)",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#functions",
    "href": "coding/week_01/essentials_01.html#functions",
    "title": "Essentials of R coding I",
    "section": "10. Functions",
    "text": "10. Functions\n\na. Create a function\nWe need to use the syntax function(x) { x as object of a task }. ‘x’ is considered an “argument”, and the function itself is inside the {}. For example:\n\nmy_function &lt;- function(x) { x + 1 }\n\n\n\nb. Check the function\n\nmy_function(9)\n\n[1] 10\n\n\n\n\nc. Write a function with 3 arguments\n\nmy_xyz_function &lt;- function(x, y, z) { x + y - z }\n\n\n\nd. Order of arguments\nNote: R is order sensitive (if you don’t explicitly specify the argument)\n\nmy_xyz_function(12, 3, 4)\n\n[1] 11\n\nmy_xyz_function(12, 4, 3)\n\n[1] 13\n\n\n\n\ne. Specifying arguments with names\nIf you specify the argument name as = to, the order doesn’t matter:\n\nmy_xyz_function(z = 4, x = 12, y = 3)\n\n[1] 11\n\n\n\n\nf. A more complex function\n\nfx &lt;- function(x, y, remove_na = NULL) {\n        # First operation is a sum, removing NAs\n        first &lt;- sum(c(x, y), na.rm = remove_na)\n        # Add a text message\n        text &lt;- \"This function is so cool\"\n        # Store result\n        result &lt;- first + x\n        # Print output\n        print(list(\"Message\" = text,\n                   \"1st\" = first,\n                   \"end\" = result))\n                   }\n\nRun the function with alternative arguments:\n\nfx(x = a, y = b, remove_na = FALSE)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 50\n\nfx(x = a, y = b, remove_na = TRUE)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 50\n\n\nStore the output in an object:\n\nfoo &lt;- fx(x=b, y=a)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 40",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html",
    "href": "coding/week_06/11_weather.html",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "",
    "text": "Description\nThis lesson provides a step-by-step guide on retrieving and processing daily-weather data using R. It covers downloading data from DAYMET and processing it to generate weather summaries useful for agricultural research.\nThis tutorial focuses on how to:\nRequired packages\nlibrary(pacman)\np_load(dplyr, tidyr, stringr) # Data wrangling\np_load(purrr) # Iteration\np_load(lubridate) # Date operations\np_load(kableExtra) # Table formatting to better display\np_load(daymetr) # Weather data from Daymet\np_load(skimr) # Summarizing weather data\np_load(vegan) # Shannon Diversity Index\np_load(writexl)",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#how-to-cite-the-daymetr-package",
    "href": "coding/week_06/11_weather.html#how-to-cite-the-daymetr-package",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.1 How to cite the daymetr package",
    "text": "2.1 How to cite the daymetr package\n\nHufkens K., Basler J. D., Milliman T. Melaas E., Richardson A.D. 2018 An integrated phenology modelling framework in R: Phenology modelling with phenor. Methods in Ecology & Evolution, 9: 1-10. https://doi.org/10.1111/2041-210X.12970",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#evapotranspiration",
    "href": "coding/week_06/11_weather.html#evapotranspiration",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.2 Evapotranspiration",
    "text": "2.2 Evapotranspiration\nDaymet does not provide data on reference evapotranspiration (\\(\\text{ET}_0\\)). However, it is possible to estimate \\(\\text{ET}_0\\) using the Hargreaves and Samani approach, which only requires temperature information (Hargreaves and Samani, 1985; Raziei and Pereira, 2013). Nonetheless, the \\(\\text{ET}_{0-HS}\\) equation is reported to give unreliable estimates for daily \\(ET0\\) and therefore it should be used for 10-day periods at the shortest (Cobaner et al., 2017). \n\n2.2.1 Constants\n\n# Constants for ET0 (Cobaner et al., 2017)\n# Solar constant:\nGsc &lt;- 0.0820 # (MJ m-2 min-1)\n# Radiation adjustment coefficient (Samani, 2004)\nkRs &lt;- 0.17",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#function-to-retrieve",
    "href": "coding/week_06/11_weather.html#function-to-retrieve",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.3 Function to retrieve",
    "text": "2.3 Function to retrieve\n\n# Name of functions using dots (.) instead of underscore (_)\n# We keep underscore for other objects\nweather.daymet &lt;- function(input, dpp = 0){ \n  # Downloads the daily weather data from the DAYMET database and process it\n  # Args:\n  #  input = input file containing the locations and the start & end dates for the time series\n  #  dpp = days prior to the Start\n  # Returns:\n  #  a tibble of DAYMET weather variables for the requested time period\n  # STEP 1. Make use of metadata (locations and dates)\n  input %&gt;%\n    dplyr::mutate(\n      Weather = purrr::pmap(list(ID = ID,\n                                 # Rename vars to avoid conflicts\n                                 lat = latitude,\n                                 lon = longitude,\n                                 sta = Start - dpp,\n                                 end = End),\n                                        \n   # STEP 2. Retrieving daymet data:\n              function(ID, lat, lon, sta, end) {\n                daymetr::download_daymet(site = ID,\n                                         lat = lat, \n                                         lon = lon,\n                                       # Extracting year from date:\n                                       start = as.numeric(substr(sta, 1, 4)),\n                                       end = as.numeric(substr(end, 1, 4)),\n                                       internal = TRUE, \n                                       simplify = TRUE)})) %&gt;% \n    \n    # STEP 3. Organizing dataframe (Re-arranging rows and columns)\n    dplyr::mutate(Weather = Weather %&gt;% \n                # i. Adjusting dates format with lubridate and map()\n                purrr::map(~ \n                  dplyr::mutate(., \n                   Date = as.Date(as.numeric(yday) - 1, # Day of the year\n                   origin = paste0(year, '-01-01')),\n                   Year = year(Date),\n                   Month = month(Date),\n                   Day = mday(Date))) %&gt;%\n                # ii. Select columns of interest\n                purrr::map(~ \n                  dplyr::select(., yday, Year, Month, Day,\n                                Date, measurement, value)) %&gt;%\n                # iii. Re-arrange columns wider\n                purrr::map(~ \n                  tidyr::pivot_wider(.,\n                      names_from = measurement, values_from = value)) %&gt;%\n                # iv. Renaming variables with rename_with()\n                    purrr::map(~ rename_with(., ~c(\n                      \"DOY\",   # Date as Day of the year\n                      \"Year\",  # Year\n                      \"Month\", # Month \n                      \"Day\",   # Day of the month\n                      \"Date\",  # Date as normal format\n                      \"DL\",    # Day length (sec)\n                      \"PP\",    # Precipitation (mm)\n                      \"Rad\",   # Radiation (W/m2)\n                      \"SWE\",   # Snow water (kg/m2)\n                      \"Tmax\",  # Max. temp. (degC)\n                      \"Tmin\",  # Min. temp. (degC)\n                      \"VP\")))) %&gt;%   # Vap Pres (Pa)\n    # STEP 4. Processing data given start and ending dates:\n    dplyr::mutate(Weather = purrr::pmap(list(sta = Start - dpp, \n                                             end = End, \n                                             data = Weather), # Requested period\n                                        function(sta, end, data) {\n                                          dplyr::filter(data, Date &gt;= sta & Date &lt;= end) \n                                          })) %&gt;%\n    # STEP 5. Unnest\n    tidyr::unnest(cols = c(Weather)) %&gt;% \n    \n    # STEP 6. Converting units and adding extra variables:\n    dplyr::mutate(Rad = Rad*0.000001*DL, # Radiation (W/m2 to MJ/m2)\n                  Tmean = (Tmax+Tmin)/2, # Mean temperature (degC),\n                  VP = VP / 1000, # VP (Pa to kPa),\n                  # Creating variables for ET0 estimation:\n                  lat_rad = latitude*0.0174533,\n                  dr = 1 + 0.033*cos((2*pi/365)*DOY),\n                  Sd = 0.409*sin((2*pi/365)*DOY - 1.39),\n                  ws = acos(-tan(lat_rad)*tan(Sd)),\n                  Ra = (24*60)/(pi) * Gsc * dr * (ws*sin(lat_rad)*sin(Sd) + cos(lat_rad)*sin(ws)),\n                  ET0_HS = 0.0135 * kRs * (Ra / 2.45) * (sqrt(Tmax-Tmin)) * (Tmean + 17.8),\n                  # Extreme PP events\n                  EPE_i = case_when((PP &gt; 25) ~ 1, TRUE ~ 0),\n                  # Extreme Temp events\n                  ETE_i = case_when((Tmax &gt;= 30) ~ 1, TRUE ~ 0),\n                  # Day length (hours)\n                  DL = (DL/60)/60 \n                  ) %&gt;% \n    dplyr::select(-lat_rad, -dr, -Sd, -ws, -Ra)\n  \n}",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-retrieving-function",
    "href": "coding/week_06/11_weather.html#run-retrieving-function",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.4 Run retrieving function",
    "text": "2.4 Run retrieving function\n\n# Specify input = dataframe containing sites-data \n# Specify Days prior planting. Default is dpp = 0. Here we use dpp = 30.\n\ndf_weather_daymet &lt;- weather.daymet(input = df_input, dpp = 30)\n\n# Overview of the variables (useful checking for missing values):\nskimr::skim(df_weather_daymet)\n\n\nData summary\n\n\nName\ndf_weather_daymet\n\n\nNumber of rows\n911\n\n\nNumber of columns\n25\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n5\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart\n0\n1\n2002-04-25\n2010-05-20\n2005-05-01\n3\n\n\nEnd\n0\n1\n2002-09-30\n2010-10-10\n2006-09-30\n3\n\n\nFlo\n0\n1\n2002-07-20\n2010-07-15\n2006-07-21\n3\n\n\nSeFi\n0\n1\n2002-08-08\n2010-08-15\n2006-08-10\n3\n\n\nDate\n0\n1\n2002-03-26\n2010-10-10\n2005-12-23\n911\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlatitude\n0\n1\n43.20\n1.03\n42.45\n42.45\n42.45\n43.65\n45.08\n▇▁▃▁▂\n\n\nlongitude\n0\n1\n-80.33\n2.49\n-81.88\n-81.88\n-81.88\n-80.40\n-75.35\n▇▃▁▁▂\n\n\nDOY\n0\n1\n184.55\n78.43\n1.00\n128.00\n185.00\n242.00\n365.00\n▂▆▇▇▂\n\n\nYear\n0\n1\n2005.63\n2.56\n2002.00\n2005.00\n2005.00\n2006.00\n2010.00\n▆▇▇▁▅\n\n\nMonth\n0\n1\n6.58\n2.57\n1.00\n5.00\n7.00\n8.00\n12.00\n▃▇▇▇▃\n\n\nDay\n0\n1\n15.83\n8.88\n1.00\n8.00\n16.00\n24.00\n31.00\n▇▇▆▇▆\n\n\nDL\n0\n1\n13.17\n1.88\n8.89\n12.06\n13.68\n14.78\n15.44\n▂▂▃▅▇\n\n\nPP\n0\n1\n2.53\n5.31\n0.00\n0.00\n0.00\n2.74\n45.09\n▇▁▁▁▁\n\n\nRad\n0\n1\n16.23\n6.78\n1.40\n11.31\n16.61\n21.63\n29.86\n▃▆▇▇▃\n\n\nSWE\n0\n1\n0.98\n4.86\n0.00\n0.00\n0.00\n0.00\n41.65\n▇▁▁▁▁\n\n\nTmax\n0\n1\n18.74\n9.40\n-7.32\n12.90\n21.21\n26.16\n34.99\n▁▃▅▇▅\n\n\nTmin\n0\n1\n8.95\n8.13\n-15.01\n2.83\n10.31\n15.21\n24.83\n▁▃▅▇▃\n\n\nVP\n0\n1\n1.26\n0.61\n0.19\n0.75\n1.21\n1.70\n3.14\n▇▇▇▃▁\n\n\nTmean\n0\n1\n13.84\n8.60\n-11.16\n7.73\n16.12\n20.52\n28.40\n▁▃▅▇▆\n\n\nET0_HS\n0\n1\n3.33\n1.66\n0.26\n2.03\n3.48\n4.69\n7.13\n▆▆▇▇▂\n\n\nEPE_i\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nETE_i\n0\n1\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\n\n\n# Exporting data as a .csv file\n# write.csv(df.weather.daymet, row.names = FALSE, na = '', file = paste0(path, 'Output_daymet.csv'))",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#define-time-intervals",
    "href": "coding/week_06/11_weather.html#define-time-intervals",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.1 Define time intervals",
    "text": "3.1 Define time intervals\nIn this section we create time intervals during the cropping season using pre-specified dates as columns at the initial data table with site information. \nThe user can apply: i) a unique seasonal interval (season), ii) even intervals (even), or iii) customized intervals (custom). \n\n3.1.1 Full-season interval\n\n# Defining season-intervals\nseason_interval &lt;- \n  df_input %&gt;%\n  dplyr::mutate(Interval = \"Season\") %&gt;%\n  dplyr::rename(Start.in = Start, End.in = End) %&gt;%\n  dplyr::select(ID, Site, Interval, Start.in, End.in)\n\n# Creating a table to visualize results\nkable(season_interval) %&gt;% \n  kable_styling(latex_options = c(\"striped\"),\n                position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nSeason\n2002-04-25\n2002-09-30\n\n\n2\nRidgetown\nSeason\n2005-05-01\n2006-09-30\n\n\n3\nWinchester\nSeason\n2010-05-20\n2010-10-10\n\n\n\n\n\n\n\n\n\n3.1.2 Even-intervals\n\n# Number of intervals:\nn &lt;- 4 \n# Days prior planting:\ndpp &lt;- 30 \n\n# Defining even-intervals:\neven_intervals &lt;- \n  df_input %&gt;% \n  # Create new data:\n  dplyr::mutate(Intervals = \n          purrr::map2(.x = Start, .y = End,\n                      .f = ~ data.frame(\n                    Interval = c(\"Prev\", # Prior to start date \n                                 LETTERS[0:n]), # Each interval from start date\n                      # Start\n                      Start.in = c(.x - dpp, seq.Date(.x, .y + 1, length.out = n + 1)[1:n]),                              # End\n                      End.in = c(.x - 1, seq.Date(.x, .y + 1, length.out = n + 1)[2:(n + 1)]))) ) %&gt;% \n  # Selecting columns:\n  dplyr::select(ID, Site, Intervals) %&gt;% \n  tidyr::unnest(cols = c(Intervals))\n\n# Creating a table to visualize results\nkable(even_intervals) %&gt;% \n  kable_styling(latex_options = c(\"striped\"), \n                position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nPrev\n2002-03-26\n2002-04-24\n\n\n1\nElora\nA\n2002-04-25\n2002-06-03\n\n\n1\nElora\nB\n2002-06-03\n2002-07-13\n\n\n1\nElora\nC\n2002-07-13\n2002-08-22\n\n\n1\nElora\nD\n2002-08-22\n2002-10-01\n\n\n2\nRidgetown\nPrev\n2005-04-01\n2005-04-30\n\n\n2\nRidgetown\nA\n2005-05-01\n2005-09-07\n\n\n2\nRidgetown\nB\n2005-09-07\n2006-01-15\n\n\n2\nRidgetown\nC\n2006-01-15\n2006-05-24\n\n\n2\nRidgetown\nD\n2006-05-24\n2006-10-01\n\n\n3\nWinchester\nPrev\n2010-04-20\n2010-05-19\n\n\n3\nWinchester\nA\n2010-05-20\n2010-06-25\n\n\n3\nWinchester\nB\n2010-06-25\n2010-07-31\n\n\n3\nWinchester\nC\n2010-07-31\n2010-09-05\n\n\n3\nWinchester\nD\n2010-09-05\n2010-10-11\n\n\n\n\n\n\n\n\n\n3.1.3 Custom-intervals\n\n# Count the number of interval columns (assuming intervals start at column \"Start\")\ni &lt;- df_input %&gt;% dplyr::select(Start:last_col()) %&gt;% ncol()\n\n# Defining custom-intervals\ncustom_intervals &lt;- \n  df_input %&gt;% \n  dplyr::mutate(Intervals = # Create\n                  purrr::pmap(# List of object to iterate over\n                              .l = list(x = Start - dpp,\n                                        y = Start,\n                                        z = Flo,\n                                        m = SeFi,\n                                        k = End),\n                              # The function to run\n                              .f = function(x, y, z, m, k) {\n                      data.frame(# New data\n                        Interval = c(LETTERS[1:i]),\n                        Name = c(\"Prev\", \"Plant-Flo\", \"Flo-SeFi\", \"SeFi-End\"),\n                        Start.in = c(x, y, z, m),\n                        End.in = c(y-1, z-1, m-1, k))}\n                    )) %&gt;% \n  # Selecting columns:\n  dplyr::select(ID,, Site, Intervals) %&gt;% \n  tidyr::unnest(cols = c(Intervals))\n\n# Creating a table to visualize results:\nkable(custom_intervals) %&gt;% \n  kable_styling(latex_options = c(\"striped\"), position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nName\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nA\nPrev\n2002-03-26\n2002-04-24\n\n\n1\nElora\nB\nPlant-Flo\n2002-04-25\n2002-07-19\n\n\n1\nElora\nC\nFlo-SeFi\n2002-07-20\n2002-08-07\n\n\n1\nElora\nD\nSeFi-End\n2002-08-08\n2002-09-30\n\n\n2\nRidgetown\nA\nPrev\n2005-04-01\n2005-04-30\n\n\n2\nRidgetown\nB\nPlant-Flo\n2005-05-01\n2006-07-20\n\n\n2\nRidgetown\nC\nFlo-SeFi\n2006-07-21\n2006-08-09\n\n\n2\nRidgetown\nD\nSeFi-End\n2006-08-10\n2006-09-30\n\n\n3\nWinchester\nA\nPrev\n2010-04-20\n2010-05-19\n\n\n3\nWinchester\nB\nPlant-Flo\n2010-05-20\n2010-07-14\n\n\n3\nWinchester\nC\nFlo-SeFi\n2010-07-15\n2010-08-14\n\n\n3\nWinchester\nD\nSeFi-End\n2010-08-15\n2010-10-10",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#summary-function",
    "href": "coding/week_06/11_weather.html#summary-function",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.2 Summary function",
    "text": "3.2 Summary function\nFor each of the period or interval of interest a variety of variables can be created. Here, we present a set of variables that can capture environmental variations that might be missing by analyzing standard weather data (precipitations, temperature, radiation). These variables represent an example that was used for studying influence of weather in corn yields by Correndo et al. (2021). \n\n# Defining the function to summarize DAYMET and/or NASA-POWER\nsummary.daymet &lt;- function(input, intervals) {\n  # Creates summaries of the DAYMET daily data over the requested period\n  # Args:\n  #  input = a weather data object such as df.weather.daymet with the daily weather data\n  #  intervals = a tibble with the start and end date for the summary period\n  # STEP 1. \n  intervals %&gt;%\n    # NOTE: mergeing on ID only as the key, so remove Site:\n    dplyr::select(-Site) %&gt;%\n    # Merging weather data:\n    dplyr::left_join(input %&gt;%\n                     # Nesting weather data back for each site-ID:\n                     dplyr::select_if(names(.) %in% \n                                        c(\"ID\", \"Crop\", \"Site\",\n                                          \"Date\",\"DL\", \"PP\", \n                                          \"Rad\", \"Tmax\", \"Tmin\",\n                                          \"Tmean\", \"VP\", \"ET0_HS\")) %&gt;%\n                     dplyr::group_by(ID, Crop, Site) %&gt;% \n                     tidyr::nest(Weather = -c(ID, Crop, Site)) %&gt;%\n                     dplyr::ungroup(), \n                   by = c(\"ID\")) %&gt;% \n    # STEP 2. Create Weather column filtering for desired period only.\n    dplyr::mutate(Weather = purrr::pmap(\n      .l = list(x = Start.in,\n                y = End.in, \n                data = Weather),\n      # Filter function\n      .f = function(x, y, data) {\n        dplyr::filter(data, Date &gt;= x & Date &lt; y)} ) ) %&gt;% \n    \n    # STEP 3. Calculation of variables (daily) that will be useful to summarize the intervals \n    dplyr::mutate(Weather = Weather %&gt;% \n      # User must adapt depending on the crop (these ay be corn-specific)\n        purrr::map(~ mutate(.,\n                            # Extreme Precip. event:\n                            EPEi = case_when(PP &gt; 25 ~1, TRUE ~ 0),\n                            # Extreme Temp. event:\n                            ETEi = case_when(Tmax &gt;= 30 ~1, TRUE ~ 0), \n                            # Tmax factor,  crop heat units (CHU):\n                            Ymax = case_when(Tmax &lt; 10 ~ 0,\n                                             Tmax &gt;= 30 ~ 0,\n                                    TRUE ~ 3.33*(Tmax-10) - 0.084*(Tmax-10)^2),\n                            # Tmin factor, Crop heat units (CHU):\n                            Ymin = case_when(Tmin &lt; 4.44 ~ 0, \n                                    TRUE ~ 1.8*(Tmin-4.44)), \n                            # Daily CHU:\n                            Yavg = (Ymax + Ymin)/2,\n                            # For WHEAT (diff. base temp and winter negatives)\n                            # # Tmin threshold Growing Degrees:\n                            # Gmin = case_when(Tmin &gt;= 0 ~ Tmin, TRUE ~ 0),\n                            # # Tmax threshold Growing Degrees:\n                            # Gmax = case_when(Tmax &gt; 30 ~ 30,\n                            #                  Tmin &lt; 0 ~ 0, \n                            #                  between(Tmax, 0, 30) ~ Tmax),\n                            # # Daily Growing Degree Units:\n                            # GDU = ((Gmin + Gmax)/2) - 0,\n                            # GDD_c = cumsum(GDU) \n                            # For CORN, SOYBEAN (Base temp = 10)\n                            # Tmin threshold Growing Degrees:\n                            Gmin = case_when(Tmin &gt;= 10 ~ Tmin, TRUE ~ 10),\n                            # Tmax threshold Growing Degrees:\n                            Gmax = case_when(Tmax &lt;= 30 ~ Tmax,\n                                             Tmax &gt; 30 ~ 30,\n                                             Tmin &lt;= 10 ~ 10,\n                                             TRUE ~ 30),\n                            # Daily Growing Degree Units:\n                            GDU = ((Gmin + Gmax)/2) - 10) ) ) %&gt;% \n    \n    # STEP 4. Summary for each variable over the period of interest:\n    dplyr::mutate(\n      # Duration of interval (days):\n      Dur = Weather %&gt;% purrr::map(~nrow(.)),\n      # Accumulated PP (mm):\n      PP = Weather %&gt;% purrr::map(~sum(.$PP)),\n      # Mean Temp (C):\n      Tmean = Weather %&gt;% purrr::map(~mean(.$Tmean)),\n      # Accumulated Rad (MJ/m2):\n      Rad = Weather %&gt;% purrr::map(~sum(.$Rad)),\n      # Accumulated VP (kPa):\n      VP = Weather %&gt;% purrr::map(~sum(.$VP)),\n      # Accumulated ET0 (mm):\n      ET0_HS = Weather %&gt;% purrr::map(~sum(.$ET0_HS)),\n      # Number of ETE (#):\n      ETE = Weather %&gt;% purrr::map(~sum(.$ETEi)),\n      # Number of EPE (#):\n      EPE = Weather %&gt;% purrr::map(~sum(.$EPEi)),\n      # Accumulated Crop Heat Units (CHU):\n      CHU = Weather %&gt;% purrr::map(~sum(.$Yavg)),\n      # Shannon Diversity Index for PP:\n      SDI = Weather %&gt;% purrr::map(~ vegan::diversity(.$PP, index = \"shannon\")/log(length(.$PP))),\n      # Accumulated Growing Degree Days (GDD):\n      GDD =  Weather %&gt;% purrr::map(~sum(.$GDU))) %&gt;% \n    \n    # Additional indices and final units:\n    dplyr::select(-Weather) %&gt;% \n    # DS: `cols` is now required when using unnest()\n    tidyr::unnest(cols = c(Dur, PP, Tmean, Rad, VP, ET0_HS, ETE, EPE, CHU, SDI, GDD)) %&gt;% \n    dplyr::mutate(\n      # Photo-thermal quotient (Q):\n      Q_chu = Rad/CHU,\n      Q_gdd = Rad/GDD,\n      # Abundant and Well Distributed Water:\n      AWDR = PP*SDI) \n  }",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-summaries",
    "href": "coding/week_06/11_weather.html#run-summaries",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.3 Run summaries",
    "text": "3.3 Run summaries\n\n3.3.1 Seasonal\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\nseason_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = season_interval)\n\n# Skim data\nskimr::skim(season_summary_daymet)\n\n\nData summary\n\n\nName\nseason_summary_daymet\n\n\nNumber of rows\n3\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n6\n6\n0\n1\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-04-25\n2010-05-20\n2005-05-01\n3\n\n\nEnd.in\n0\n1\n2002-09-30\n2010-10-10\n2006-09-30\n3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n272.67\n211.73\n143.00\n150.50\n158.00\n337.50\n517.00\n▇▁▁▁▃\n\n\nPP\n0\n1\n683.57\n395.63\n361.53\n462.75\n563.98\n844.58\n1125.19\n▇▇▁▁▇\n\n\nTmean\n0\n1\n16.04\n3.20\n12.55\n14.63\n16.70\n17.78\n18.85\n▇▁▁▇▇\n\n\nRad\n0\n1\n4359.96\n2895.00\n2394.44\n2697.73\n3001.02\n5342.72\n7684.42\n▇▁▁▁▃\n\n\nVP\n0\n1\n358.94\n237.60\n218.15\n221.77\n225.40\n429.33\n633.26\n▇▁▁▁▃\n\n\nET0_HS\n0\n1\n925.30\n517.24\n594.03\n627.29\n660.56\n1090.94\n1521.32\n▇▁▁▁▃\n\n\nETE\n0\n1\n17.67\n3.79\n15.00\n15.50\n16.00\n19.00\n22.00\n▇▁▁▁▃\n\n\nEPE\n0\n1\n2.67\n2.52\n0.00\n1.50\n3.00\n4.00\n5.00\n▇▁▇▁▇\n\n\nCHU\n0\n1\n4323.15\n2564.95\n2740.35\n2843.47\n2946.59\n5114.55\n7282.51\n▇▁▁▁▃\n\n\nSDI\n0\n1\n0.74\n0.03\n0.71\n0.74\n0.76\n0.76\n0.76\n▃▁▁▁▇\n\n\nGDD\n0\n1\n1677.80\n742.37\n1208.96\n1249.85\n1290.73\n1912.22\n2533.71\n▇▁▁▁▃\n\n\nQ_chu\n0\n1\n0.99\n0.15\n0.81\n0.93\n1.06\n1.08\n1.10\n▃▁▁▁▇\n\n\nQ_gdd\n0\n1\n2.46\n0.59\n1.86\n2.17\n2.48\n2.76\n3.03\n▇▁▇▁▇\n\n\nAWDR\n0\n1\n513.54\n307.39\n257.93\n343.01\n428.09\n641.35\n854.62\n▇▇▁▁▇\n\n\n\n\n# Creating a table to visualize results\nkbl(season_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nSeason\n2002-04-25\n2002-09-30\nCorn\nElora\n158\n361.53\n16.70405\n3001.020\n218.1533\n660.5558\n16\n0\n2740.348\n0.7134269\n1208.965\n1.0951238\n2.482305\n257.9252\n\n\n2\nSeason\n2005-05-01\n2006-09-30\nCorn\nRidgetown\n517\n1125.19\n12.55279\n7684.417\n633.2642\n1521.3225\n22\n5\n7282.508\n0.7595347\n2533.715\n1.0551883\n3.032866\n854.6209\n\n\n3\nSeason\n2010-05-20\n2010-10-10\nSoybean\nWinchester\n143\n563.98\n18.85315\n2394.440\n225.3956\n594.0311\n15\n3\n2946.587\n0.7590440\n1290.725\n0.8126148\n1.855113\n428.0856\n\n\n\n\n\n\n\n\n\n3.3.2 Even\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\neven_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = even_intervals)\n\n# Skim data\nskimr::skim(even_summary_daymet)\n\n\nData summary\n\n\nName\neven_summary_daymet\n\n\nNumber of rows\n15\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n1\n4\n0\n5\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-03-26\n2010-09-05\n2005-09-07\n15\n\n\nEnd.in\n0\n1\n2002-04-24\n2010-10-11\n2006-01-15\n15\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n60.53\n43.22\n29.00\n36.00\n39.00\n84.50\n130.00\n▇▁▁▁▃\n\n\nPP\n0\n1\n153.76\n92.73\n45.94\n92.45\n113.04\n208.32\n364.82\n▇▃▂▂▁\n\n\nTmean\n0\n1\n14.46\n6.40\n4.11\n8.35\n18.00\n19.57\n22.11\n▃▃▁▁▇\n\n\nRad\n0\n1\n981.82\n680.51\n402.74\n595.51\n707.50\n924.39\n2563.53\n▇▂▁▁▂\n\n\nVP\n0\n1\n76.45\n62.46\n21.08\n37.52\n62.84\n77.33\n218.10\n▇▆▁▁▂\n\n\nET0_HS\n0\n1\n201.85\n152.45\n63.49\n117.28\n174.13\n198.55\n576.34\n▇▇▁▁▂\n\n\nETE\n0\n1\n3.53\n4.34\n0.00\n0.00\n2.00\n6.00\n14.00\n▇▃▂▁▁\n\n\nEPE\n0\n1\n0.60\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nCHU\n0\n1\n908.91\n846.64\n145.26\n415.56\n782.24\n865.76\n2963.26\n▇▇▁▁▂\n\n\nSDI\n0\n1\n0.64\n0.07\n0.43\n0.61\n0.65\n0.69\n0.73\n▁▁▃▅▇\n\n\nGDD\n0\n1\n347.29\n409.21\n-112.18\n98.38\n319.40\n404.69\n1285.44\n▇▇▁▁▂\n\n\nQ_chu\n0\n1\n1.57\n1.11\n0.73\n0.82\n0.92\n2.28\n3.59\n▇▁▁▁▂\n\n\nQ_gdd\n0\n1\n7.47\n19.56\n-15.99\n1.79\n2.15\n7.00\n74.52\n▇▇▁▁▁\n\n\nAWDR\n0\n1\n102.83\n68.43\n26.06\n57.67\n77.62\n145.51\n263.99\n▇▃▂▂▁\n\n\n\n\n# Creating a table to visualize results\nkbl(even_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nPrev\n2002-03-26\n2002-04-24\nCorn\nElora\n29\n95.53\n5.660000\n505.2583\n21.08116\n63.48611\n0\n1\n154.3458\n0.6357661\n6.780\n3.2735469\n74.521875\n60.73474\n\n\n1\nA\n2002-04-25\n2002-06-03\nCorn\nElora\n40\n113.04\n8.802250\n800.8372\n33.52344\n128.07920\n0\n0\n292.5157\n0.6866269\n84.400\n2.7377578\n9.488593\n77.61630\n\n\n1\nB\n2002-06-03\n2002-07-13\nCorn\nElora\n40\n98.94\n19.117375\n856.4385\n62.83969\n202.20827\n5\n0\n823.2502\n0.6508401\n371.250\n1.0403138\n2.306905\n64.39412\n\n\n1\nC\n2002-07-13\n2002-08-22\nCorn\nElora\n40\n89.36\n20.994750\n752.2604\n69.43155\n194.90035\n7\n0\n893.7629\n0.5993348\n437.950\n0.8416778\n1.717685\n53.55656\n\n\n1\nD\n2002-08-22\n2002-10-01\nCorn\nElora\n39\n60.83\n17.997436\n604.4711\n53.82624\n138.43587\n4\n0\n754.0653\n0.4283610\n324.600\n0.8016164\n1.862203\n26.05720\n\n\n2\nPrev\n2005-04-01\n2005-04-30\nCorn\nRidgetown\n29\n105.04\n7.902759\n522.1453\n21.72264\n75.06876\n0\n0\n145.2592\n0.5923778\n41.800\n3.5945772\n12.491513\n62.22337\n\n\n2\nA\n2005-05-01\n2005-09-07\nCorn\nRidgetown\n130\n225.28\n19.555615\n2563.5280\n216.16182\n576.34481\n14\n0\n2793.4159\n0.6940917\n1285.435\n0.9177037\n1.994288\n156.36498\n\n\n2\nB\n2005-09-07\n2006-01-15\nCorn\nRidgetown\n129\n255.66\n6.956744\n992.3342\n115.05132\n174.13068\n0\n2\n1025.8802\n0.6752569\n112.950\n0.9673003\n8.785606\n172.63617\n\n\n2\nC\n2006-01-15\n2006-05-24\nCorn\nRidgetown\n130\n287.94\n4.109692\n1793.2989\n85.09908\n236.08072\n0\n1\n510.1460\n0.6804557\n-112.175\n3.5152659\n-15.986619\n195.93041\n\n\n2\nD\n2006-05-24\n2006-10-01\nCorn\nRidgetown\n129\n364.82\n19.592442\n2341.9490\n218.10287\n536.30800\n8\n2\n2963.2599\n0.7236240\n1249.530\n0.7903286\n1.874264\n263.99249\n\n\n3\nPrev\n2010-04-20\n2010-05-19\nSoybean\nWinchester\n29\n45.94\n11.114483\n586.5492\n23.85418\n106.48366\n0\n0\n320.9666\n0.5915947\n112.350\n1.8274459\n5.220732\n27.17786\n\n\n3\nA\n2010-05-20\n2010-06-25\nSoybean\nWinchester\n36\n119.18\n18.890417\n685.8864\n51.43909\n177.92863\n2\n1\n782.2378\n0.7315317\n319.395\n0.8768259\n2.147455\n87.18395\n\n\n3\nB\n2010-06-25\n2010-07-31\nSoybean\nWinchester\n36\n85.30\n22.106111\n707.4951\n69.55371\n184.06589\n9\n0\n817.1332\n0.6402241\n426.640\n0.8658260\n1.658295\n54.61112\n\n\n3\nC\n2010-07-31\n2010-09-05\nSoybean\nWinchester\n36\n168.14\n20.660556\n612.1467\n63.55369\n150.74414\n4\n1\n837.7504\n0.6259308\n382.745\n0.7307030\n1.599359\n105.24400\n\n\n3\nD\n2010-09-05\n2010-10-11\nSoybean\nWinchester\n36\n191.36\n13.492083\n402.7351\n41.51125\n83.48533\n0\n1\n519.7147\n0.7037089\n165.755\n0.7749157\n2.429701\n134.66174\n\n\n\n\n\n\n\n\n\n3.3.3 Custom\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\ncustom_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = custom_intervals)\n\n# Skim data\nskimr::skim(custom_summary_daymet)\n\n\nData summary\n\n\nName\ncustom_summary_daymet\n\n\nNumber of rows\n12\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n1\n1\n0\n4\n0\n\n\nName\n0\n1\n4\n9\n0\n4\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-03-26\n2010-08-15\n2005-12-10\n12\n\n\nEnd.in\n0\n1\n2002-04-24\n2010-10-10\n2006-07-30\n12\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n74.92\n118.16\n18.00\n29.00\n40.50\n55.25\n445.00\n▇▁▁▁▁\n\n\nPP\n0\n1\n191.43\n233.36\n45.94\n82.21\n100.29\n188.89\n895.24\n▇▁▁▁▁\n\n\nTmean\n0\n1\n15.79\n5.72\n5.66\n11.36\n16.97\n20.28\n23.58\n▅▅▅▇▇\n\n\nRad\n0\n1\n1214.16\n1723.57\n332.92\n517.92\n653.53\n913.03\n6546.07\n▇▁▁▁▁\n\n\nVP\n0\n1\n94.38\n133.33\n21.08\n30.15\n66.72\n81.44\n507.83\n▇▁▁▁▁\n\n\nET0_HS\n0\n1\n249.35\n330.83\n63.49\n89.12\n151.66\n222.19\n1261.95\n▇▁▁▁▁\n\n\nETE\n0\n1\n4.42\n5.12\n0.00\n0.00\n3.50\n6.25\n18.00\n▇▅▁▁▁\n\n\nEPE\n0\n1\n0.75\n1.48\n0.00\n0.00\n0.00\n1.00\n5.00\n▇▁▁▁▁\n\n\nCHU\n0\n1\n1119.50\n1476.59\n145.26\n389.46\n852.85\n1123.81\n5634.82\n▇▁▁▁▁\n\n\nSDI\n0\n1\n0.64\n0.08\n0.52\n0.58\n0.61\n0.72\n0.75\n▆▆▂▂▇\n\n\nGDD\n0\n1\n427.22\n482.32\n6.78\n178.38\n362.19\n483.34\n1848.98\n▇▇▁▁▁\n\n\nQ_chu\n0\n1\n1.39\n1.02\n0.68\n0.76\n0.86\n1.54\n3.59\n▇▂▁▁▂\n\n\nQ_gdd\n0\n1\n9.28\n20.78\n1.30\n1.79\n1.94\n3.96\n74.52\n▇▁▁▁▁\n\n\nAWDR\n0\n1\n133.24\n178.12\n27.18\n44.11\n61.48\n140.08\n668.48\n▇▁▁▁▁\n\n\n\n\n# Creating a table to visualize results\nkbl(custom_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nName\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nA\nPrev\n2002-03-26\n2002-04-24\nCorn\nElora\n29\n95.53\n5.660000\n505.2583\n21.08116\n63.48611\n0\n1\n154.3458\n0.6357661\n6.780\n3.2735469\n74.521875\n60.73474\n\n\n1\nB\nPlant-Flo\n2002-04-25\n2002-07-19\nCorn\nElora\n85\n211.98\n14.478353\n1767.0401\n105.60612\n358.83259\n7\n0\n1218.2390\n0.7117824\n518.900\n1.4504872\n3.405358\n150.88364\n\n\n1\nC\nFlo-SeFi\n2002-07-20\n2002-08-07\nCorn\nElora\n18\n61.48\n21.249444\n333.0742\n32.24373\n89.48994\n3\n0\n412.2869\n0.5459508\n200.385\n0.8078700\n1.662171\n33.56506\n\n\n1\nD\nSeFi-End\n2002-08-08\n2002-09-30\nCorn\nElora\n53\n88.07\n18.649434\n861.4328\n77.13351\n203.26534\n6\n0\n1063.5927\n0.5490199\n471.480\n0.8099273\n1.827082\n48.35218\n\n\n2\nA\nPrev\n2005-04-01\n2005-04-30\nCorn\nRidgetown\n29\n105.04\n7.902759\n522.1453\n21.72264\n75.06876\n0\n0\n145.2592\n0.5923778\n41.800\n3.5945772\n12.491513\n62.22337\n\n\n2\nB\nPlant-Flo\n2005-05-01\n2006-07-20\nCorn\nRidgetown\n445\n895.24\n11.445236\n6546.0731\n507.83287\n1261.94680\n18\n5\n5634.8226\n0.7467051\n1848.980\n1.1617177\n3.540370\n668.48028\n\n\n2\nC\nFlo-SeFi\n2006-07-21\n2006-08-09\nCorn\nRidgetown\n19\n71.87\n23.582632\n332.9171\n42.49005\n87.99388\n4\n0\n492.0876\n0.5904150\n255.540\n0.6765403\n1.302798\n42.43313\n\n\n2\nD\nSeFi-End\n2006-08-10\n2006-09-30\nCorn\nRidgetown\n51\n158.08\n17.779314\n760.1808\n79.45679\n161.56523\n0\n0\n1104.2616\n0.6849177\n407.350\n0.6884064\n1.866161\n108.27178\n\n\n3\nA\nPrev\n2010-04-20\n2010-05-19\nSoybean\nWinchester\n29\n45.94\n11.114483\n586.5492\n23.85418\n106.48366\n0\n0\n320.9666\n0.5915947\n112.350\n1.8274459\n5.220732\n27.17786\n\n\n3\nB\nPlant-Flo\n2010-05-20\n2010-07-14\nSoybean\nWinchester\n55\n181.19\n19.959273\n1067.8207\n87.38554\n278.97376\n8\n1\n1182.4459\n0.7532597\n538.705\n0.9030610\n1.982199\n136.48312\n\n\n3\nC\nFlo-SeFi\n2010-07-15\n2010-08-14\nSoybean\nWinchester\n30\n85.66\n21.524167\n566.9201\n56.30866\n141.75997\n3\n0\n743.4511\n0.5215225\n346.630\n0.7625520\n1.635519\n44.67362\n\n\n3\nD\nSeFi-End\n2010-08-15\n2010-10-10\nSoybean\nWinchester\n56\n297.13\n16.158571\n720.5205\n77.41198\n163.36417\n4\n2\n962.2477\n0.7257673\n377.755\n0.7487890\n1.907375\n215.64723",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#locations-and-dates-data",
    "href": "coding/week_06/11_weather.html#locations-and-dates-data",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.1 Locations and Dates Data",
    "text": "4.1 Locations and Dates Data\n\n# For historical data (from Jan-01-2000 to Dec-31-2022)\ndf_historical &lt;- data.frame(ID = c('1', '2', '3'),\n                      # Dates as YYYY_MM_DD, using \"_\" to separate\n                      Start = c('2000-01-01', '2000-01-01', '2000-01-01'),\n                      End = c('2022-12-31', '2022-12-31', '2022-12-31')) %&gt;% \n  # Express start and end as dates using \"across\"\n  dplyr::mutate(across(Start:End, ~as.Date(., format = '%Y-%m-%d')))\n\n# Merge for historical weather data\ndf_historical &lt;- df_sites %&gt;% dplyr::left_join(df_historical, by = \"ID\")",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#retrieve-from-daymet",
    "href": "coding/week_06/11_weather.html#retrieve-from-daymet",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.2 Retrieve from DAYMET",
    "text": "4.2 Retrieve from DAYMET\n\n# Specify input = dataframe containing historical dates from sites \nhist_weather_daymet &lt;- weather.daymet(input = df_historical)\n\n# This is a large data frame (21900 obs), so good to have an overview\n# Skim data\nskimr::skim(hist_weather_daymet)\n\n\nData summary\n\n\nName\nhist_weather_daymet\n\n\nNumber of rows\n25185\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n3\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart\n0\n1\n2000-01-01\n2000-01-01\n2000-01-01\n1\n\n\nEnd\n0\n1\n2022-12-31\n2022-12-31\n2022-12-31\n1\n\n\nDate\n0\n1\n2000-01-01\n2022-12-31\n2011-07-02\n8395\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlatitude\n0\n1\n43.73\n1.08\n42.45\n42.45\n43.65\n45.08\n45.08\n▇▁▇▁▇\n\n\nlongitude\n0\n1\n-79.21\n2.80\n-81.88\n-81.88\n-80.40\n-75.35\n-75.35\n▇▇▁▁▇\n\n\nDOY\n0\n1\n183.00\n105.37\n1.00\n92.00\n183.00\n274.00\n365.00\n▇▇▇▇▇\n\n\nYear\n0\n1\n2011.00\n6.63\n2000.00\n2005.00\n2011.00\n2017.00\n2022.00\n▇▆▇▆▇\n\n\nMonth\n0\n1\n6.52\n3.45\n1.00\n4.00\n7.00\n10.00\n12.00\n▇▅▅▅▇\n\n\nDay\n0\n1\n15.72\n8.79\n1.00\n8.00\n16.00\n23.00\n31.00\n▇▇▇▇▆\n\n\nDL\n0\n1\n12.00\n2.26\n8.56\n9.80\n12.00\n14.20\n15.44\n▇▅▅▅▇\n\n\nPP\n0\n1\n2.58\n5.27\n0.00\n0.00\n0.00\n2.92\n85.53\n▇▁▁▁▁\n\n\nRad\n0\n1\n13.40\n7.37\n0.74\n6.84\n12.59\n19.32\n32.59\n▇▇▇▆▂\n\n\nSWE\n0\n1\n16.21\n32.83\n0.00\n0.00\n0.00\n12.65\n211.59\n▇▁▁▁▁\n\n\nTmax\n0\n1\n12.75\n11.54\n-22.58\n2.94\n13.55\n23.18\n36.23\n▁▅▇▇▅\n\n\nTmin\n0\n1\n3.03\n10.49\n-31.93\n-3.83\n3.42\n11.72\n25.02\n▁▂▇▇▅\n\n\nVP\n0\n1\n0.93\n0.59\n0.04\n0.46\n0.77\n1.35\n3.17\n▇▅▃▁▁\n\n\nTmean\n0\n1\n7.89\n10.85\n-26.25\n-0.37\n8.46\n17.42\n29.88\n▁▃▇▇▅\n\n\nET0_HS\n0\n1\n2.42\n1.83\n-0.45\n0.72\n1.97\n4.00\n7.38\n▇▅▅▅▁\n\n\nEPE_i\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nETE_i\n0\n1\n0.03\n0.17\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#summary-functions",
    "href": "coding/week_06/11_weather.html#summary-functions",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.3 Summary functions",
    "text": "4.3 Summary functions\n\n4.3.1 By year\n\n# Defining function to summarize historical weather (years)\n\n# Revised function:\nhistorical.years &lt;- function(hist.data) {\n  # Creates an input tibble with the start and end date for each year a summary is desired\n  # Args:\n  #  hist.data = data frame containing the historical weather data to summarize (must be complete years)\n  # Returns:\n  #  a tibble of monthly summaries for each ID\n  #\n  # By year:\n  hist.data %&gt;% \n    dplyr::group_by(ID, Crop, Site) %&gt;%\n    tidyr::nest() %&gt;%\n    dplyr::mutate(the_Dates = purrr::map(data, function(.data) {.data %&gt;% dplyr::group_by(Year) %&gt;% \n        dplyr::summarise(Start.in = min(Date), End.in = max(Date), .groups = \"drop\")})) %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::select(ID, Site, the_Dates) %&gt;%\n    tidyr::unnest(cols = c(the_Dates))\n}\n\n\n\n4.3.2 By year-month\n\n# Defining function to summarize historical weather (years & months)\n# Revised function:\nhistorical.yearmonths &lt;- function(hist.data) {\n  # Creates an input tibble with the start and end date for each year & month a summary is desired (monthly summaries)\n  # Args:\n  #  hist.data = data frame containing the historical weather data to summarize (must be complete years)\n  # Returns:\n  #  a tibble of monthly summaries for each ID\n  #\n  # By month in year:\n  hist.data %&gt;% \n    dplyr::group_by(ID, Crop, Site) %&gt;%\n    tidyr::nest() %&gt;%\n    dplyr::mutate(the_Dates = purrr::map(data, function(.data) {.data %&gt;% \n        dplyr::group_by(Year, Month) %&gt;% \n        dplyr::summarise(Start.in = min(Date), End.in = max(Date), .groups = \"drop\")})) %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::select(ID, Site, the_Dates) %&gt;%\n    tidyr::unnest(cols = c(the_Dates))\n}",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-historical-summaries",
    "href": "coding/week_06/11_weather.html#run-historical-summaries",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.4 Run Historical Summaries",
    "text": "4.4 Run Historical Summaries\nSummary can be obtained by years or by years.months. User must specify this option at the “intervals” argument of the summary function. \n\n4.4.1 By year\n\n# Specify hist.data = dataframe containing the historical weather data to summarize\nyear_intervals &lt;- historical.years(hist.data = hist_weather_daymet)\n\n# input = dataframe containing the historical weather data.\n# intervals = type of historical intervals (years, years.months)\n\n# Summarizing historical weather\nyear_summary_daymet &lt;-\n  summary.daymet(input = hist_weather_daymet,\n                           intervals = year_intervals)\n\n# Creating a table to visualize data\nkbl(head(year_summary_daymet)) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", latex_options = c(\"scale_down\"))\n\n\n\n\nID\nYear\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\n2000\n2000-01-01\n2000-12-30\nCorn\nElora\n364\n1035.79\n6.784354\n4882.077\n311.9404\n834.4626\n1\n5\n3363.850\n0.7951588\n480.635\n1.451336\n10.157556\n823.6175\n\n\n1\n2001\n2001-01-01\n2001-12-31\nCorn\nElora\n364\n909.30\n7.786786\n4956.964\n321.2663\n874.0620\n11\n2\n3266.985\n0.7815342\n644.145\n1.517290\n7.695417\n710.6491\n\n\n1\n2002\n2002-01-01\n2002-12-31\nCorn\nElora\n364\n795.34\n7.495824\n4869.331\n327.4677\n869.5653\n16\n2\n3140.274\n0.7811108\n630.985\n1.550607\n7.717032\n621.2487\n\n\n1\n2003\n2003-01-01\n2003-12-31\nCorn\nElora\n364\n948.80\n6.212733\n4976.636\n300.1867\n841.3591\n4\n2\n3158.797\n0.7825177\n388.790\n1.575484\n12.800318\n742.4528\n\n\n1\n2004\n2004-01-01\n2004-12-30\nCorn\nElora\n364\n952.48\n6.537857\n4883.104\n308.5060\n827.3507\n0\n3\n3248.058\n0.8086498\n401.735\n1.503392\n12.155037\n770.2228\n\n\n1\n2005\n2005-01-01\n2005-12-31\nCorn\nElora\n364\n842.09\n7.185206\n5117.729\n300.0098\n921.0563\n24\n3\n3242.154\n0.7836771\n681.280\n1.578497\n7.511932\n659.9266\n\n\n\n\n\n\n\n\n\n4.4.2 By year-month\n\n# Specify hist.data = dataframe containing the historical weather data to summarize\nyearmonth_intervals &lt;- historical.yearmonths(hist.data = hist_weather_daymet)\n\n# input = dataframe containing the historical weather data.\n# intervals = type of historical intervals (years, years.months)\n\n# Summarizing historical weather\nyearmonth_summary_daymet &lt;-\n  summary.daymet(input = hist_weather_daymet,\n                           intervals = yearmonth_intervals)\n\n# Creating a table to visualize data\nkbl(head(yearmonth_summary_daymet)) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", latex_options = c(\"scale_down\"))\n\n\n\n\nID\nYear\nMonth\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\n2000\n1\n2000-01-01\n2000-01-31\nCorn\nElora\n30\n44.22\n-7.187333\n191.5335\n8.62155\n12.22926\n0\n0\n0.9515712\n0.7172157\n-190.010\n201.2813416\n-1.008018\n31.71528\n\n\n1\n2000\n2\n2000-02-01\n2000-02-29\nCorn\nElora\n28\n57.08\n-4.463036\n269.1460\n9.91130\n19.01688\n0\n0\n6.6954018\n0.7480953\n-141.315\n40.1986311\n-1.904582\n42.70128\n\n\n1\n2000\n3\n2000-03-01\n2000-03-31\nCorn\nElora\n30\n44.97\n2.868500\n441.0486\n16.34147\n47.73356\n0\n0\n70.8484896\n0.6801023\n-33.830\n6.2252371\n-13.037205\n30.58420\n\n\n1\n2000\n4\n2000-04-01\n2000-04-30\nCorn\nElora\n29\n64.87\n5.135345\n552.8653\n18.09003\n65.36530\n0\n1\n100.1956452\n0.5036470\n-1.030\n5.5178573\n-536.762404\n32.67158\n\n\n1\n2000\n5\n2000-05-01\n2000-05-31\nCorn\nElora\n30\n147.43\n12.970667\n584.3621\n31.84376\n114.94758\n0\n2\n398.9491488\n0.6533741\n140.385\n1.4647534\n4.162568\n96.32695\n\n\n1\n2000\n6\n2000-06-01\n2000-06-30\nCorn\nElora\n29\n199.88\n17.281207\n560.0957\n41.55868\n133.48935\n1\n1\n576.7064388\n0.7668216\n217.885\n0.9711973\n2.570603\n153.27230",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html",
    "href": "coding/week_08/models_01.html",
    "title": "Statistical Models with R: I - Essentials",
    "section": "",
    "text": "Statistical modeling is a process of developing and analyzing mathematical models to represent real-world phenomena. In agricultural research, statistical modeling plays a crucial role in understanding the relationships between environmental variables, management practices, and crop responses. By leveraging statistical models, researchers can make informed decisions about optimizing yield, improving resource efficiency, and enhancing sustainability in agriculture.\nIn this lesson, we focus on the essentials of statistical modeling using R, with examples relevant to ag-data science. We will explore the use of statistical models to analyze field experiments, evaluate treatment effects, and understand interactions between genotype, environment, and management practices. The examples will utilize datasets from the agridat package, particularly the lasrosas.corn dataset, and introduce key functions from stats, nlme, lme4, car, multcomp, and agricolae.\n\n\n\nData Collection and Exploratory Data Analysis (EDA)\n\nStatistical modeling starts with data collection and EDA. In agricultural experiments, this involves gathering data on yield, soil properties, weather conditions, and management practices. EDA helps identify patterns, trends, and relationships between variables while detecting outliers or anomalies.\n\nTypes of Statistical Models\n\nIn agricultural research, common models include:\n\nRegression Models: For predicting continuous outcomes like crop yield based on variables such as soil nutrients or precipitation.\nTime Series Models: To analyze temporal data like seasonal growth patterns or yield trends over years.\nMixed-Effects Models: Ideal for experimental designs with hierarchical structures, such as split-plot designs or repeated measures.\n\n\nModel Selection and Assumptions\n\nThe choice of model depends on the data type, research question, and assumptions about the data. For example:\n\nLinear Regression assumes a linear relationship between predictors and outcome, suitable for continuous variables like yield or biomass. We call it linear because we are basically comparing “lines”, where the lines represent the “means”.\nGeneralized Linear Models (GLM) are used for non-normal distributions, such as count data (e.g., pest counts) or binary outcomes (e.g., disease presence).\n\n\nModel Evaluation\n\nEvaluating model performance is crucial to ensure accurate predictions and inferences. In agricultural modeling, common metrics include:\n\nR-squared (R²): Measures the proportion of variation explained by the model. But it is NOT always recommended as a criterion the “select models”.\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE): Assess prediction accuracy.\nAIC and BIC: For model comparison and selection. These two are recommended when selecting a model.\n\n\nApplication in Agricultural Research\n\nStatistical models provide insights into complex agricultural systems, enabling researchers to:\n\nIdentify Key Drivers: Determine which factors most influence crop performance, such as genotype-environment interactions.\nPredict Future Trends: Forecast yield potential under different climate scenarios or management practices.\nOptimize Inputs: Inform decision-making for fertilizer application, irrigation scheduling, or pest management.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#introduction",
    "href": "coding/week_08/models_01.html#introduction",
    "title": "Statistical Models with R: I - Essentials",
    "section": "",
    "text": "Statistical modeling is a process of developing and analyzing mathematical models to represent real-world phenomena. In agricultural research, statistical modeling plays a crucial role in understanding the relationships between environmental variables, management practices, and crop responses. By leveraging statistical models, researchers can make informed decisions about optimizing yield, improving resource efficiency, and enhancing sustainability in agriculture.\nIn this lesson, we focus on the essentials of statistical modeling using R, with examples relevant to ag-data science. We will explore the use of statistical models to analyze field experiments, evaluate treatment effects, and understand interactions between genotype, environment, and management practices. The examples will utilize datasets from the agridat package, particularly the lasrosas.corn dataset, and introduce key functions from stats, nlme, lme4, car, multcomp, and agricolae.\n\n\n\nData Collection and Exploratory Data Analysis (EDA)\n\nStatistical modeling starts with data collection and EDA. In agricultural experiments, this involves gathering data on yield, soil properties, weather conditions, and management practices. EDA helps identify patterns, trends, and relationships between variables while detecting outliers or anomalies.\n\nTypes of Statistical Models\n\nIn agricultural research, common models include:\n\nRegression Models: For predicting continuous outcomes like crop yield based on variables such as soil nutrients or precipitation.\nTime Series Models: To analyze temporal data like seasonal growth patterns or yield trends over years.\nMixed-Effects Models: Ideal for experimental designs with hierarchical structures, such as split-plot designs or repeated measures.\n\n\nModel Selection and Assumptions\n\nThe choice of model depends on the data type, research question, and assumptions about the data. For example:\n\nLinear Regression assumes a linear relationship between predictors and outcome, suitable for continuous variables like yield or biomass. We call it linear because we are basically comparing “lines”, where the lines represent the “means”.\nGeneralized Linear Models (GLM) are used for non-normal distributions, such as count data (e.g., pest counts) or binary outcomes (e.g., disease presence).\n\n\nModel Evaluation\n\nEvaluating model performance is crucial to ensure accurate predictions and inferences. In agricultural modeling, common metrics include:\n\nR-squared (R²): Measures the proportion of variation explained by the model. But it is NOT always recommended as a criterion the “select models”.\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE): Assess prediction accuracy.\nAIC and BIC: For model comparison and selection. These two are recommended when selecting a model.\n\n\nApplication in Agricultural Research\n\nStatistical models provide insights into complex agricultural systems, enabling researchers to:\n\nIdentify Key Drivers: Determine which factors most influence crop performance, such as genotype-environment interactions.\nPredict Future Trends: Forecast yield potential under different climate scenarios or management practices.\nOptimize Inputs: Inform decision-making for fertilizer application, irrigation scheduling, or pest management.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#essential-r-packages",
    "href": "coding/week_08/models_01.html#essential-r-packages",
    "title": "Statistical Models with R: I - Essentials",
    "section": "2 Essential R Packages",
    "text": "2 Essential R Packages\n\nlibrary(pacman)\np_load(agridat, agricolae)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(nlme, lme4, car, multcomp)",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#data",
    "href": "coding/week_08/models_01.html#data",
    "title": "Statistical Models with R: I - Essentials",
    "section": "3 Data",
    "text": "3 Data\n\ndata_corn &lt;- agridat::lasrosas.corn\n# Check data structure and variables\nglimpse(data_corn)\n\nRows: 3,443\nColumns: 9\n$ year  &lt;int&gt; 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   &lt;dbl&gt; -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  &lt;dbl&gt; -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield &lt;dbl&gt; 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro &lt;dbl&gt; 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    &lt;dbl&gt; 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    &lt;fct&gt; N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#key-statistical-models",
    "href": "coding/week_08/models_01.html#key-statistical-models",
    "title": "Statistical Models with R: I - Essentials",
    "section": "4 Key Statistical Models",
    "text": "4 Key Statistical Models\n\n4.1 Linear Models (LM)\nLinear regression models are fundamental for analyzing relationships between variables. The term “regression” could be confusing because it means we are working with a “continous response variable”, but it could also mean we using a “continuous covariate” (or independent / or explanatory variable) (e.g. a “regressor”).\n\n4.1.1 Categorical covariate/s as independent variable/s\n\n# Complete Randomized\nlm_fit_01 &lt;- lm(yield ~ nf, data = data_corn)\n# See summary\nsummary(lm_fit_01)\n\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9729     0.8218  79.060  &lt; 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,   Adjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n\n# Alternative models\n# Blocks (as fixed)\nlm_fit_02 &lt;- lm(yield ~ nf + rep , data = data_corn)\n# Add year (as fixed)\nlm_fit_03 &lt;- lm(yield ~ nf + rep + year, data = data_corn)\n# Add topography (as fixed)\nlm_fit_04 &lt;- lm(yield ~ nf + rep + year + topo, data = data_corn)\n# Different order \nlm_fit_05 &lt;- lm(yield ~ nf + year + topo + rep, data = data_corn)\n\n\n\n4.1.2 Continuous covariate/s as independent variable/s\n\n# Nitrogen (independent variable) as continuous predictor\nlm_reg_01 &lt;- lm(yield ~ nitro, data = data_corn)\n# See summary\nsummary(lm_reg_01)\n\n\nCall:\nlm(formula = yield ~ nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n# Compare to N as a categorical variable\nsummary(lm_fit_01)\n\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9729     0.8218  79.060  &lt; 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,   Adjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n\n\n\n\n\n4.2 Generalized Linear Models (GLM)\nGLMs extend linear models to handle non-normal response distributions. In agricultural research, they are useful for modeling yield data with non-constant variance or non-normal residuals.\n\n4.2.1 Example using GLM as an LM\nlm() is just a special case of glm where the distribution of error is assumed to be Gaussian (i.e. normal)\n\nglm_fit_01 &lt;- glm(yield ~ nf + rep, data = data_corn, family = gaussian)\n# See summary\nsummary(glm_fit_01)\n\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian, data = data_corn)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8527)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328839  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 2\n\n# Compare to lm\nsummary(lm_fit_02)\n\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n\n\n\n\n4.2.2 Example using the Gaussian family with log link:\nThese approaches are particularly useful when yield data exhibit heteroscedasticity or skewness.\n\nFor this first approach, the model assumes a multiplicative relationship between predictors and yield, modeling the expected value as an exponential function.\n\nThus, the underlying model is:\n\\[E(Y) = \\exp(X\\beta)\\] , where the expected value of yield, \\(E(Y)\\), is modeled as an exponential function of the predictors \\(\\exp(X\\beta)\\).\nIf you believe the relationship between predictors and the expected value of yield is multiplicative, use this approach.\n\n# Using log link without manually transforming yield\nglm_fit_02 &lt;- glm(yield ~ nf + rep, data = data_corn, \n                  family = gaussian(link = \"log\"))\nsummary(glm_fit_02)\n\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian(link = \"log\"), \n    data = data_corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.170242   0.014369 290.223  &lt; 2e-16 ***\nnfN1         0.054586   0.017387   3.140  0.00171 ** \nnfN2         0.069457   0.017308   4.013 6.12e-05 ***\nnfN3         0.079305   0.017202   4.610 4.17e-06 ***\nnfN4         0.110495   0.016981   6.507 8.79e-11 ***\nnfN5         0.114107   0.016934   6.738 1.87e-11 ***\nrepR2       -0.004492   0.011824  -0.380  0.70404    \nrepR3        0.015601   0.011702   1.333  0.18254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8745)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328857  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 4\n\n\nAlternatively, you may want to manually log-transform the response:\n\nFor this second approach, the log-transformed yield is modeled as a linear function of the predictors, stabilizing variance or normalizing residuals.\n\nIn the second case, the model is:\n\\[\\log(Y) = X\\beta + \\epsilon \\],\nwhich is a linear model \\(X\\beta + \\epsilon\\) on the log-transformed outcome, \\(\\log(Y)\\).\nIf you want to stabilize variance or normalize the residuals, use this second approach.\n\n# Manually log-transforming yield\nglm_fit_03 &lt;- glm(log(yield) ~ nf + rep, data = lasrosas.corn, \n                  family = gaussian(link = \"identity\"))\nsummary(glm_fit_03)\n\n\nCall:\nglm(formula = log(yield) ~ nf + rep, family = gaussian(link = \"identity\"), \n    data = lasrosas.corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.114234   0.013869 296.650  &lt; 2e-16 ***\nnfN1         0.070491   0.016959   4.157 3.31e-05 ***\nnfN2         0.086082   0.017003   5.063 4.35e-07 ***\nnfN3         0.096942   0.016974   5.711 1.22e-08 ***\nnfN4         0.130277   0.016996   7.665 2.31e-14 ***\nnfN5         0.129767   0.016974   7.645 2.69e-14 ***\nrepR2       -0.004546   0.012007  -0.379    0.705    \nrepR3        0.019599   0.012002   1.633    0.103    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.08268533)\n\n    Null deviance: 291.07  on 3442  degrees of freedom\nResidual deviance: 284.02  on 3435  degrees of freedom\nAIC: 1198.4\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\n\n4.3 Mixed-Effects Models\nMixed-effects models account for both fixed and random effects, often used in agricultural experiments.\nUsing nlme:\n\nlme_fit &lt;- lme(yield ~ nf, random = ~1 | rep, data = data_corn)\nsummary(lme_fit)\n\nLinear mixed-effects model fit by REML\n  Data: data_corn \n       AIC      BIC    logLik\n  30286.69 30335.83 -15135.34\n\nRandom effects:\n Formula: ~1 | rep\n        (Intercept) Residual\nStdDev:   0.4656023 19.66857\n\nFixed effects:  yield ~ nf \n               Value Std.Error   DF  t-value p-value\n(Intercept) 64.97387 0.8645219 3435 75.15584  0.0000\nnfN1         3.64192 1.1599967 3435  3.13960  0.0017\nnfN2         4.67371 1.1630343 3435  4.01855  0.0001\nnfN3         5.36182 1.1610011 3435  4.61827  0.0000\nnfN4         7.59070 1.1625194 3435  6.52953  0.0000\nnfN5         7.85772 1.1610011 3435  6.76805  0.0000\n Correlation: \n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.6547157 -0.7871554 -0.1563133  0.6960595  2.2882930 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n\n\nUsing lme4:\n\nlmer_fit &lt;- lmer(yield ~ nf + (1 | rep), data = data_corn)\nsummary(lmer_fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ nf + (1 | rep)\n   Data: data_corn\n\nREML criterion at convergence: 30270.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6547 -0.7872 -0.1563  0.6961  2.2883 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n rep      (Intercept)   0.2168  0.4656 \n Residual             386.8526 19.6686 \nNumber of obs: 3443, groups:  rep, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  64.9739     0.8645  75.156\nnfN1          3.6419     1.1600   3.140\nnfN2          4.6737     1.1630   4.019\nnfN3          5.3618     1.1610   4.618\nnfN4          7.5907     1.1625   6.530\nnfN5          7.8577     1.1610   6.768\n\nCorrelation of Fixed Effects:\n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n\n\n\n\n4.4 Choosing Between nlme and lme4\n\nnlme: Suitable for models that are linear and nonlinear mixed-effects models. It provides robust tools for analyzing data with nested random effects and handling different types of correlation structures within the data. It can handle heterogeneous variance models.\nlme4: Best for fitting large linear mixed-effects models. It does not handle nonlinear mixed-effects models or autoregressive correlation structures but is highly efficient with large datasets and complex random effects structures. It cannot handle heterogeneous variance models.\n\n\n\n4.5 Analysis of Variance (ANOVA)\nAnalysis of Variance (ANOVA) is widely used in agricultural research to compare the means of multiple groups and to understand the influence of categorical factors on continuous outcomes, such as yield or biomass. In R, there are multiple ways to perform ANOVA:\n\nanova(): Sequential (Type I) ANOVA\naov(): Similar for balanced designs\ncar::Anova(): Flexible ANOVA with options for Type II and Type III Sum of Squares\n\n\n4.5.1 Using anova()\nanova() performs Type I Sum of Squares (sequential). It tests each term sequentially, considering the order of the terms in the model.\n\nlm_fit &lt;- lm(yield ~ rep + nf + year + topo, data = data_corn)\nanova(lm_fit)  # Type I Sum of Squares\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(&gt;F)    \nrep          2   1271     635    3.7264 0.02418 *  \nnf           5  23987    4797   28.1334 &lt; 2e-16 ***\nyear         1  97313   97313  570.6692 &lt; 2e-16 ***\ntopo         3 646456  215485 1263.6625 &lt; 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.2 Using aov()\naov() is similar to lm() but is designed for balanced experimental designs. It also uses Type I Sum of Squares.\n\naov_fit &lt;- aov(yield ~ nf + year + topo + rep, data = data_corn)\nsummary(aov_fit)  # Type I Sum of Squares\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nnf             5  23987    4797   28.13  &lt; 2e-16 ***\nyear           1  97321   97321  570.71  &lt; 2e-16 ***\ntopo           3 643667  214556 1258.21  &lt; 2e-16 ***\nrep            2   4053    2027   11.88 7.18e-06 ***\nResiduals   3431 585070     171                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.3 Using car::Anova()\nThe Anova() function from the car package allows for Type II and Type III Sum of Squares:\n\nType II: Assumes no interaction between factors and tests each main effect after the other main effects.\nType III: Tests each main effect and interaction after all other terms, typically used with dummy coding.\n\n\ncar::Anova(lm_fit, type = 2)  # Type II Sum of Squares\n\nAnova Table (Type II tests)\n\nResponse: yield\n          Sum Sq   Df  F value    Pr(&gt;F)    \nrep         4053    2   11.885 7.183e-06 ***\nnf         21727    5   25.483 &lt; 2.2e-16 ***\nyear      120660    1  707.584 &lt; 2.2e-16 ***\ntopo      646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals 585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit, type = 3)  # Type III Sum of Squares\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.4 Comparison of anova() vs. Anova()\n\n# For the anova(), the order of factors matter\nanova(lm_fit_01)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nnf           5   23987  4797.4  12.396 6.075e-12 ***\nResiduals 3437 1330110   387.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_02)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nnf           5   23987  4797.4 12.4011 6.01e-12 ***\nrep          2    1271   635.6  1.6429   0.1936    \nResiduals 3435 1328839   386.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_03)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nnf           5   23987    4797  13.3771 6.105e-13 ***\nrep          2    1271     636   1.7722    0.1701    \nyear         1   97313   97313 271.3489 &lt; 2.2e-16 ***\nResiduals 3434 1231526     359                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_04)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(&gt;F)    \nnf           5  23987    4797   28.1331 &lt; 2e-16 ***\nrep          2   1271     636    3.7272 0.02416 *  \nyear         1  97313   97313  570.6692 &lt; 2e-16 ***\ntopo         3 646456  215485 1263.6625 &lt; 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_05)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nnf           5  23987    4797   28.133 &lt; 2.2e-16 ***\nyear         1  97321   97321  570.714 &lt; 2.2e-16 ***\ntopo         3 643667  214556 1258.209 &lt; 2.2e-16 ***\nrep          2   4053    2027   11.885 7.183e-06 ***\nResiduals 3431 585070     171                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# For the Anova(type=3), the order of factors doesn't matter\ncar::Anova(lm_fit_01, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 2418907    1 6250.447 &lt; 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_02, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 1800690    1 4654.7170 &lt; 2.2e-16 ***\nnf            23987    5   12.4012 6.009e-12 ***\nrep            1271    2    1.6429    0.1936    \nResiduals   1328839 3435                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_03, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept)   96132    1 268.0552 &lt; 2.2e-16 ***\nnf            23836    5  13.2930 7.436e-13 ***\nrep            1264    2   1.7616    0.1719    \nyear          97313    1 271.3489 &lt; 2.2e-16 ***\nResiduals   1231526 3434                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_04, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_05, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn agricultural research, Type III Sum of Squares is particularly useful for unbalanced designs, such as field trials with missing data or unequal replications.\n\n\n\n4.6 Post-hoc Tests\nAfter detecting significant differences with ANOVA, post-hoc tests can be conducted to identify specific group differences.\nUsing multcomp for multiple comparisons:\n\n# Using glht() function\ncomp &lt;- glht(aov_fit, linfct = mcp(nf = \"Tukey\"))\nsummary(comp)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = yield ~ nf + year + topo + rep, data = data_corn)\n\nLinear Hypotheses:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nN1 - N0 == 0   3.8048     0.7702   4.940  &lt; 0.001 ***\nN2 - N0 == 0   4.8232     0.7722   6.246  &lt; 0.001 ***\nN3 - N0 == 0   5.0724     0.7709   6.580  &lt; 0.001 ***\nN4 - N0 == 0   7.4876     0.7718   9.701  &lt; 0.001 ***\nN5 - N0 == 0   7.3672     0.7709   9.556  &lt; 0.001 ***\nN2 - N1 == 0   1.0184     0.7708   1.321  0.77337    \nN3 - N1 == 0   1.2676     0.7696   1.647  0.56708    \nN4 - N1 == 0   3.6828     0.7705   4.779  &lt; 0.001 ***\nN5 - N1 == 0   3.5624     0.7697   4.628  &lt; 0.001 ***\nN3 - N2 == 0   0.2492     0.7716   0.323  0.99954    \nN4 - N2 == 0   2.6644     0.7726   3.449  0.00754 ** \nN5 - N2 == 0   2.5440     0.7717   3.297  0.01271 *  \nN4 - N3 == 0   2.4152     0.7712   3.132  0.02166 *  \nN5 - N3 == 0   2.2948     0.7702   2.980  0.03450 *  \nN5 - N4 == 0  -0.1204     0.7712  -0.156  0.99999    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\n4.7 Nonlinear Models\nNonlinear models are useful when the relationship between the predictor and response variables is not linear. In agricultural research, these models are commonly used to model yield response to inputs, such as nitrogen fertilizer.\nFor nonlinear relationships, we could use nls(). Let’s see an example using a power function:\n\nnls_fit &lt;- nls(yield ~ a * nitro^b, data = data_corn, start = list(a = 1, b = 1))\nsummary(nls_fit)\n\n\nFormula: yield ~ a * nitro^b\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 55.71689    4.33102  12.865  &lt; 2e-16 ***\nb  0.05641    0.01811   3.116  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.99 on 3441 degrees of freedom\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.822e-07\n\n# Alternative exponential\n# nls_mitscherlich &lt;- nls(yield ~ a * (1 - exp(-b * nitro)), data = data_corn, start = list(a = 55, b = 0.05))\n\nVisualizing the model’s predictions can help in understanding the fitted curve and the data’s behavior.\n\n# Creating a data frame with predictions\ndata_corn &lt;- data_corn %&gt;% \n  mutate(pred = predict(nls_fit))\n\n# Plotting observed vs. predicted yield\ndata_corn %&gt;% \nggplot(aes(x = nitro, y = yield)) +\n  geom_point(color = \"blue\", size = 2) +  # Observed data\n  geom_line(aes(y = pred), color = \"red\", size = 1) +  # Fitted curve\n  geom_smooth()+\n  labs(title = \"Yield Response to Nitrogen\",\n       x = \"Nitrogen (kg/ha)\",\n       y = \"Yield (qq/ha)\") +\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#conclusion",
    "href": "coding/week_08/models_01.html#conclusion",
    "title": "Statistical Models with R: I - Essentials",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lesson introduced essential statistical models in R for agricultural research, providing practical code examples. In the next session, we will delve deeper into model diagnostics and interpretation.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html",
    "href": "coding/week_09/models_03.html",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "",
    "text": "Linear models are foundational in statistical analysis, particularly in agricultural data science. These models allow researchers to evaluate relationships between variables and assess treatment effects in experiments. This document covers the essentials of linear modeling in R using stats, car, broom, emmeans, multcomp, and cld for statistical inference and means comparisons.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#introduction",
    "href": "coding/week_09/models_03.html#introduction",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "",
    "text": "Linear models are foundational in statistical analysis, particularly in agricultural data science. These models allow researchers to evaluate relationships between variables and assess treatment effects in experiments. This document covers the essentials of linear modeling in R using stats, car, broom, emmeans, multcomp, and cld for statistical inference and means comparisons.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#what-is-a-linear-model",
    "href": "coding/week_09/models_03.html#what-is-a-linear-model",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "2 What is a Linear Model?",
    "text": "2 What is a Linear Model?\nA linear model is a mathematical equation describing the relationship between a response variable (dependent) and one or more explanatory variables (independent). The simplest form is:\n\\[ Y = \\beta_0 + \\beta_1 X + ... + \\beta_2 X ... + \\epsilon \\]\nwhere:\n\\[Y\\] is the dependent variable (response variable),  \\[X\\] is the independent variable (matrix of experimental design),  \\[ \\beta_0 \\] is the intercept,  \\[ \\beta_1 \\] is the effect of factor #1 on Y (i.e. slope (regression) or mean effect (anova)),  \\[ \\beta_2 \\] is the effect of factor #2 on Y (i.e. slope (regression) or mean effect (anova)),  \\[ \\epsilon \\] represents error (unexplained variation). \nNote: A polynomial term–a quadratic (squared or \\(2^{nd}\\) order) or cubic (\\(3^{rd}\\) order) term turns a linear regression model into a curve. But because it is X that is squared or cubed, not the Beta coefficient ($ $), it still qualifies as a linear model.\n\n2.1 Regression vs. ANOVA\n\nRegression Analysis: Used when the explanatory variable(s) are continuous (e.g., predicting yield from nitrogen levels).\nANOVA (Analysis of Variance): Used when at least one explanatory variable is categorical (e.g., comparing mean yields across different treatments).\nTechnically, ANOVA is a type of linear model, but its focus is on comparing means across groups (made from categorical predictors), while regression aims to quantify the relationship between continuous variables.\n\nWe will use the data_corn dataset from the agridat package.\n\n# Load required packages\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(agridat)\np_load(broom)\np_load(emmeans)\np_load(multcomp, multcompView)\np_load(car) # for assumption checks\np_load(performance) # for assumption checks\n\nLoad dataset:\n\ndata_corn &lt;- agridat::lasrosas.corn\n\n# Inspect dataset\nglimpse(data_corn)\n\nRows: 3,443\nColumns: 9\n$ year  &lt;int&gt; 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   &lt;dbl&gt; -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  &lt;dbl&gt; -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield &lt;dbl&gt; 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro &lt;dbl&gt; 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    &lt;dbl&gt; 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    &lt;fct&gt; N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-fitting-linear-regression-anova",
    "href": "coding/week_09/models_03.html#model-fitting-linear-regression-anova",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "3 Model Fitting: Linear Regression & ANOVA",
    "text": "3 Model Fitting: Linear Regression & ANOVA\n\n3.1 Simple Linear Regression (Continuous Predictor)\n\nreg_fit &lt;- lm(yield ~ 1 + nitro, data = data_corn)\nsummary(reg_fit)\n\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n# Comparing analysis of variance options\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nnitro        1   23790 23790.3  61.537 5.754e-15 ***\nResiduals 3441 1330307   386.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(reg_fit, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n           Sum Sq   Df F value    Pr(&gt;F)    \nnitro       23790    1  61.537 5.754e-15 ***\nResiduals 1330307 3441                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(reg_fit, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 4525464    1 11705.665 &lt; 2.2e-16 ***\nnitro         23790    1    61.537 5.754e-15 ***\nResiduals   1330307 3441                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2 Linear Regression, no-intercept\n\nreg_noint &lt;- lm(yield ~ -1 + nitro, data = data_corn)\nsummary(reg_fit)\n\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\nsummary(reg_noint)\n\n\nCall:\nlm(formula = yield ~ -1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.435  -7.134  20.741  45.962 108.840 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nnitro 0.772271   0.009088   84.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.25 on 3442 degrees of freedom\nMultiple R-squared:  0.6772,    Adjusted R-squared:  0.6771 \nF-statistic:  7222 on 1 and 3442 DF,  p-value: &lt; 2.2e-16\n\n# Comparing analysis of variance options\nanova(reg_noint)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nnitro        1 12286371 12286371  7221.9 &lt; 2.2e-16 ***\nResiduals 3442  5855771     1701                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(reg_noint, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(&gt;F)    \nnitro     12286371    1  7221.9 &lt; 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(reg_noint, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(&gt;F)    \nnitro     12286371    1  7221.9 &lt; 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3 ANOVA (Categorical Predictors)\n\nanova_crd &lt;- lm(yield ~ nf, data = data_corn) # assuming CRD\nanova_fit &lt;- lm(yield ~ nf + rep, data = data_corn) # assuming RCBD\nanova_03 &lt;- lm(yield ~ nf*rep, data = data_corn) # assuming RCBD\n\nAnova(anova_crd, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 2418907    1 6250.447 &lt; 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Anova(anova_fit, type = 3)\nAnova(anova_03, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept)  799899    1 2063.2074 &lt; 2.2e-16 ***\nnf             7035    5    3.6290  0.002818 ** \nrep              88    2    0.1139  0.892378    \nnf:rep          977   10    0.2520  0.990548    \nResiduals   1327862 3425                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.4 ANOVA, no-intercept\n\nanova_noint &lt;- lm(yield ~ -1 + nf + rep, data = data_corn)\nAnova(anova_noint, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n           Sum Sq   Df   F value Pr(&gt;F)    \nnf        5575937    6 2402.2655 &lt;2e-16 ***\nrep          1271    2    1.6429 0.1936    \nResiduals 1328839 3435                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(anova_fit)\n\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n\nsummary(anova_noint)\n\n\nCall:\nlm(formula = yield ~ -1 + nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nnfN0   64.7216     0.9486  68.225   &lt;2e-16 ***\nnfN1   68.3611     0.9464  72.235   &lt;2e-16 ***\nnfN2   69.3895     0.9495  73.082   &lt;2e-16 ***\nnfN3   70.0816     0.9478  73.940   &lt;2e-16 ***\nnfN4   72.3132     0.9491  76.195   &lt;2e-16 ***\nnfN5   72.5775     0.9478  76.573   &lt;2e-16 ***\nrepR2  -0.3301     0.8213  -0.402    0.688    \nrepR3   1.0915     0.8210   1.329    0.184    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.9268,    Adjusted R-squared:  0.9266 \nF-statistic:  5433 on 8 and 3435 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-assumptions",
    "href": "coding/week_09/models_03.html#model-assumptions",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "4 Model Assumptions",
    "text": "4 Model Assumptions\nLinear models assume:\n\nLinearity: Relationship between predictors and response is linear (continuous).\n\n\n# Residual diagnostics\npar(mfrow=c(2,2))\nplot(anova_fit)\n\n\n\n\n\n\n\n\n\nNormality of Residuals: Residuals should be normally distributed.\n\n\n# Normality test\nshapiro.test(resid(anova_fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(anova_fit)\nW = 0.94724, p-value &lt; 2.2e-16\n\n\n\nHomoscedasticity: Equal variance across all levels of predictors.\n\n\n# Homoscedasticity check\nleveneTest(anova_crd) #1-way anova, only nf and CRD\nleveneTest(anova_fit) #2-way anova including blocks\n\n\nIndependence: Observations are independent (i.e. the “error” of replications is independent).\n\nThere is no test for independence. You have to make sure you specify the error-structure correctly for potential autocorrelation (e.g. blocks, split-plots, repeated measures, etc.).\n\nPerformance package With the performance package, we could check all at once.\n\n\nperformance::check_model(anova_fit)",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-selection-aicbic-criteria",
    "href": "coding/week_09/models_03.html#model-selection-aicbic-criteria",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "5 Model Selection: AIC/BIC Criteria",
    "text": "5 Model Selection: AIC/BIC Criteria\n\n5.1 Candidate models\nThese are all fixed-effect models.\n\n# null model\nlm_00 &lt;- lm(yield ~ 1, data = data_corn)\n# Simplest model\nlm_01 &lt;- lm(yield ~ nf + rep, data = data_corn)\n# Add year\nlm_02 &lt;- lm(yield ~ nf + year + rep, data = data_corn)\n# Add topo\nlm_03 &lt;- lm(yield ~ nf + topo + rep, data = data_corn)\n# Add year and topo\nlm_04 &lt;- lm(yield ~ nf + year + topo + rep, data = data_corn)\n\n# Main effects and interactions\nlm_05 &lt;- lm(yield ~ nf*year*topo + rep, data = data_corn)\n#lm_05 &lt;- lm(yield ~ nf + year + topo + nf:year + nf:topo + year:topo + nf:year:topo + rep, data = data_corn)\n\n\n\n5.2 Selection criteria\n\n5.2.1 F-Test\n\nF-tests evaluate if added predictors significantly improve model fit via sum of squares and degrees of freedom.\n\n\nanova(lm_01, lm_02, lm_03, lm_04, lm_05)\n\nAnalysis of Variance Table\n\nModel 1: yield ~ nf + rep\nModel 2: yield ~ nf + year + rep\nModel 3: yield ~ nf + topo + rep\nModel 4: yield ~ nf + year + topo + rep\nModel 5: yield ~ nf * year * topo + rep\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   3435 1328839                                    \n2   3434 1231526  1     97313  914.137 &lt; 2.2e-16 ***\n3   3432  705730  2    525796 2469.604 &lt; 2.2e-16 ***\n4   3431  585070  1    120660 1133.457 &lt; 2.2e-16 ***\n5   3393  361197 38    223873   55.342 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.2.2 AIC (Akaike Information Criterion)\n\nAIC balances goodness of fit with model complexity.\n\n\nAIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n\n      df      AIC\nlm_01  9 30294.35\nlm_02 10 30034.50\nlm_03 12 28121.52\nlm_04 13 27477.95\nlm_05 51 25893.36\n\n\n\n\n5.2.3 BIC (Bayesian Information criterion)\n\nBIC applies a stricter penalty for complexity, favoring simpler models.\n\n\nBIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n\n      df      BIC\nlm_01  9 30349.64\nlm_02 10 30095.94\nlm_03 12 28195.25\nlm_04 13 27557.82\nlm_05 51 26206.71",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#significance-of-effects",
    "href": "coding/week_09/models_03.html#significance-of-effects",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "6 Significance of effects",
    "text": "6 Significance of effects\n\n# Compare Anova sum of squares\nAnova(lm_05, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \nnf            24028   20   11.2855 &lt; 2.2e-16 ***\nyear         120553    1 1132.4508 &lt; 2.2e-16 ***\ntopo         648289   18  338.3269 &lt; 2.2e-16 ***\nrep            3995    2   18.7623 7.877e-09 ***\nnf:year        1486    5    2.7919   0.01601 *  \nnf:topo        2126   15    1.3315   0.17376    \nyear:topo    218811    3  685.1532 &lt; 2.2e-16 ***\nnf:year:topo   1869   15    1.1704   0.28739    \nResiduals    361197 3393                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(lm_05, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept)   26405    1 248.0418 &lt; 2.2e-16 ***\nnf              322    5   0.6050    0.6961    \nyear          26539    1 249.2978 &lt; 2.2e-16 ***\ntopo          38269    3 119.8315 &lt; 2.2e-16 ***\nrep            3995    2  18.7623 7.877e-09 ***\nnf:year         321    5   0.6039    0.6970    \nnf:topo        1868   15   1.1701    0.2876    \nyear:topo     38337    3 120.0429 &lt; 2.2e-16 ***\nnf:year:topo   1869   15   1.1704    0.2874    \nResiduals    361197 3393                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#means-comparisons-with-emmeans-and-cld",
    "href": "coding/week_09/models_03.html#means-comparisons-with-emmeans-and-cld",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "7 Means Comparisons with emmeans and cld",
    "text": "7 Means Comparisons with emmeans and cld\n\n7.1 Interaction\n\n# Pairwise comparisons among treatment means\nemmeans(lm_05, pairwise ~ year:topo) %&gt;% \n  cld(., level = 0.05, decreasing = F)\n\n year topo emmean    SE   df lower.CL upper.CL .group  \n 2001 HT     44.6 0.497 3393     44.6     44.6  1      \n 1999 HT     53.4 0.549 3393     53.4     53.5   2     \n 1999 E      64.8 0.538 3393     64.7     64.8    3    \n 1999 W      66.0 0.438 3393     65.9     66.0    34   \n 2001 W      67.7 0.467 3393     67.7     67.8     4   \n 1999 LO     71.3 0.481 3393     71.3     71.3      5  \n 2001 E      92.7 0.543 3393     92.6     92.7       6 \n 2001 LO     99.9 0.501 3393     99.9     99.9        7\n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 8 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n7.2 Alternative\nUsing the same model, same sum of squares we could group comparisons differently. In this case, showing comparisons of topo means, grouped by “year”\n\nemmeans(lm_05, pairwise ~ topo, by = \"year\") %&gt;% \n  cld(., level = 0.05, decreasing = FALSE, Letters = letters) # add letters\n\nyear = 1999:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     53.4 0.549 3393     53.4     53.5  a    \n E      64.8 0.538 3393     64.7     64.8   b   \n W      66.0 0.438 3393     65.9     66.0   b   \n LO     71.3 0.481 3393     71.3     71.3    c  \n\nyear = 2001:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     44.6 0.497 3393     44.6     44.6  a    \n W      67.7 0.467 3393     67.7     67.8   b   \n E      92.7 0.543 3393     92.6     92.7    c  \n LO     99.9 0.501 3393     99.9     99.9     d \n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n# By default in R\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#interpreting-coefficients",
    "href": "coding/week_09/models_03.html#interpreting-coefficients",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "8 Interpreting Coefficients",
    "text": "8 Interpreting Coefficients\n\n8.1 Regression\n\nIntercept (\\(\\beta_0\\)): Baseline value of the dependent variable. Is the value of Y, when X = 0.\nSlope (\\(\\beta_1\\)): Change in response variable (Y) per unit increase in predictor (X).\np-value: Significance of predictor effect.\n\nWe can extract regression coefficient estimates with the ‘coef()’ function, or with the ‘tidy()’ function of the “broom” package.\n\n# Extracting coefficients with coef()\ncoef(reg_fit)\n\n(Intercept)       nitro \n65.84321305  0.06171718 \n\n# Tidy summary of coefficients\nreg_coefs &lt;- broom::tidy(reg_fit)\nreg_coefs\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  65.8      0.609      108.   0       \n2 nitro         0.0617   0.00787      7.84 5.75e-15\n\n\n\n\n8.2 ANOVA\n\nIntercept: the reference or benchmark level (baseline mean) for categorical predictors.\nFactor Levels: estimates of mean differences “with respect to” the intercept (if any, if not, from zero).\np-value: whether a factor significantly differs from baseline.\n\nWe can also extract ANOVA coefficient estimates with the ‘coef()’ function, or with the ‘tidy()’ function of the “broom” package.\n\n# Extracting coefficients with coef()\ncoef(lm_05)\n\n     (Intercept)             nfN1             nfN2             nfN3 \n   -2.930444e+04     7.134866e+01     4.085152e+02     3.143485e+03 \n            nfN4             nfN5             year           topoHT \n    2.469591e+03     2.724513e+03     1.468933e+01     3.840595e+04 \n          topoLO            topoW            repR2            repR3 \n   -8.387929e+02     2.518130e+04     4.372000e-01     2.476942e+00 \n       nfN1:year        nfN2:year        nfN3:year        nfN4:year \n   -3.417600e-02    -2.023608e-01    -1.569826e+00    -1.232124e+00 \n       nfN5:year      nfN1:topoHT      nfN2:topoHT      nfN3:topoHT \n   -1.359608e+00    -3.857242e+03    -2.925203e+03    -4.044360e+03 \n     nfN4:topoHT      nfN5:topoHT      nfN1:topoLO      nfN2:topoLO \n   -1.041123e+03     1.878725e+03     3.636709e+03     1.417504e+03 \n     nfN3:topoLO      nfN4:topoLO      nfN5:topoLO       nfN1:topoW \n   -2.679130e+03    -1.098798e+02    -1.411964e+03    -5.869613e+01 \n      nfN2:topoW       nfN3:topoW       nfN4:topoW       nfN5:topoW \n    3.338172e+03    -2.204795e+03     3.275604e+03     1.455853e+03 \n     year:topoHT      year:topoLO       year:topoW nfN1:year:topoHT \n   -1.921966e+01     4.227740e-01    -1.259712e+01     1.930477e+00 \nnfN2:year:topoHT nfN3:year:topoHT nfN4:year:topoHT nfN5:year:topoHT \n    1.464313e+00     2.024633e+00     5.232640e-01    -9.370874e-01 \nnfN1:year:topoLO nfN2:year:topoLO nfN3:year:topoLO nfN4:year:topoLO \n   -1.818932e+00    -7.089521e-01     1.339684e+00     5.517450e-02 \nnfN5:year:topoLO  nfN1:year:topoW  nfN2:year:topoW  nfN3:year:topoW \n    7.067462e-01     2.969667e-02    -1.668718e+00     1.102522e+00 \n nfN4:year:topoW  nfN5:year:topoW \n   -1.636535e+00    -7.268434e-01 \n\n# Tidy summary of coefficients\nanova_coefs &lt;- broom::tidy(lm_05)\nanova_coefs\n\n# A tibble: 50 × 5\n   term        estimate std.error statistic  p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept) -29304.   1861.     -15.7    5.43e-54\n 2 nfN1            71.3  2621.       0.0272 9.78e- 1\n 3 nfN2           409.   2621.       0.156  8.76e- 1\n 4 nfN3          3143.   2653.       1.18   2.36e- 1\n 5 nfN4          2470.   2637.       0.937  3.49e- 1\n 6 nfN5          2725.   2671.       1.02   3.08e- 1\n 7 year            14.7     0.930   15.8    3.01e-54\n 8 topoHT       38406.   2590.      14.8    2.95e-48\n 9 topoLO        -839.   2527.      -0.332  7.40e- 1\n10 topoW        25181.   2438.      10.3    1.23e-24\n# ℹ 40 more rows\n\nperformance::r2(lm_00)\n\n# R2 for Linear Regression\n       R2: 0.000\n  adj. R2: 0.000\n\nperformance::r2(lm_01)\n\n# R2 for Linear Regression\n       R2: 0.019\n  adj. R2: 0.017\n\nperformance::r2(lm_02)\n\n# R2 for Linear Regression\n       R2: 0.091\n  adj. R2: 0.088\n\nperformance::r2(lm_03)\n\n# R2 for Linear Regression\n       R2: 0.479\n  adj. R2: 0.477\n\nperformance::r2(lm_04)\n\n# R2 for Linear Regression\n       R2: 0.568\n  adj. R2: 0.567\n\nperformance::r2(lm_05)\n\n# R2 for Linear Regression\n       R2: 0.733\n  adj. R2: 0.729",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#conclusion",
    "href": "coding/week_09/models_03.html#conclusion",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nLinear models are essential for agricultural research, helping to quantify relationships and test hypotheses. This quick guide covered essentials for regression (continuous predictors), ANOVA (categorical predictors), assumption checks, model selection, and means comparisons using emmeans, and coefficients’ extraction with broom.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html",
    "href": "coding/week_13/bayes_01.html",
    "title": "Intro to Bayesian Statistics",
    "section": "",
    "text": "Important\n\n\n\nNeither Frequentist nor Bayesian approaches are universally superior — each has strengths depending on the context. 😉\nToday, we’ll explore and contrast both paradigms.",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#frequentism-vs-bayesianism-video",
    "href": "coding/week_13/bayes_01.html#frequentism-vs-bayesianism-video",
    "title": "Intro to Bayesian Statistics",
    "section": "1 Frequentism vs Bayesianism Video",
    "text": "1 Frequentism vs Bayesianism Video\n\n\n\n\nWhat are your thoughts?\n\nLet’s open the floor for discussion!",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#main-differences",
    "href": "coding/week_13/bayes_01.html#main-differences",
    "title": "Intro to Bayesian Statistics",
    "section": "2 Main Differences",
    "text": "2 Main Differences\nThe key divergence lies in their treatment of TRUTH.\nFrequentism assumes the existence of a fixed, true value. Parameters are fixed, and randomness comes from data variation. It’s called “frequentism” because it relies on the frequency of events under repeated sampling.\n\n🎲 Example: To estimate the probability of rolling a 6, Frequentists say: “If we roll a die infinitely many times, the proportion of 6s will approach 1/6.”\n\nInference is based on idealized, repeated experiments — even ones we’ve never performed.\nBayesianism does not assume a fixed truth. It centers on:\nPROBABILITIES: All knowledge is probabilistic — not true/false, but likely/unlikely.\nBELIEFS: Prior knowledge is incorporated into analysis via prior distributions. Even when we know nothing (👉 uninformative priors!), Bayesianism lets us model uncertainty.\n\n🎲 Bayesian Dice: “We are a priori 16.7% certain we’ll get a 6.”\n\nBayesian inference updates beliefs based on observed data:\n\\[\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n\\]\nAnd to compare competing models or beliefs, we use the Bayes Factor:\n\\[\n\\text{Bayes Factor} = \\frac{\\text{Posterior}}{\\text{Prior}} = \\frac{0.334}{0.167} = 2\n\\]\nThis means the updated belief is twice as likely, given the data.\n\n2.1 Summary:\n\nFrequentism: Models are fixed; randomness is in the data.\nBayesianism: Data is fixed; uncertainty lies in the model.\n\n\n\n\n\n\n\nTip\n\n\n\nFor many simple models, both approaches yield similar conclusions.\n💡 Hint: Consider uninformative priors!\n\n\n\n\n\n\n\n\nNote\n\n\n\nBayesian logic mirrors human reasoning:\n\nWe collect data.\nWe have prior beliefs.\nWe combine both to update our beliefs.",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#bayes-theorem",
    "href": "coding/week_13/bayes_01.html#bayes-theorem",
    "title": "Intro to Bayesian Statistics",
    "section": "3 Bayes Theorem",
    "text": "3 Bayes Theorem\n\\[\nP(A_{\\text{true}} \\mid B) = \\frac{P(B \\mid A_{\\text{true}}) \\cdot P(A_{\\text{true}})}{P(B)}\n\\]\n\\[\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n\\]\n\n3.1 Video: Bayes’ Rule",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#the-priors",
    "href": "coding/week_13/bayes_01.html#the-priors",
    "title": "Intro to Bayesian Statistics",
    "section": "4 The Priors",
    "text": "4 The Priors\nPriors formalize our beliefs as probability distributions — based on:\n\nThe nature of the variable (discrete/continuous)\nOur prior knowledge or lack thereof",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#credible-vs.-confidence-intervals",
    "href": "coding/week_13/bayes_01.html#credible-vs.-confidence-intervals",
    "title": "Intro to Bayesian Statistics",
    "section": "5 Credible vs. Confidence Intervals",
    "text": "5 Credible vs. Confidence Intervals\nA crucial difference lies in interpreting uncertainty:\n\nConfidence Interval (Frequentist): “If we repeat the experiment infinitely, 95% of the calculated intervals will contain the true value of \\(\\theta\\).”\n\n⚠️ \\(\\theta\\) is fixed — it’s either in the interval or not.\n\nCredible Interval (Bayesian): “There is a 95% probability that \\(\\theta\\) lies within the interval.”\n\n🔑 Conditional on the prior being correct.",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_01.html#useful-resources",
    "href": "coding/week_13/bayes_01.html#useful-resources",
    "title": "Intro to Bayesian Statistics",
    "section": "6 Useful Resources",
    "text": "6 Useful Resources\n\n6.1 🧠 Introductory Theory\n\nBayesian Models: A Statistical Primer for Ecologists – Hobbs & Hooten\nBringing Bayesian Models to Life – Hooten & Hefley\nStatistical Rethinking – McElreath (PDF)\n\n\n\n6.2 🎓 Bayesian Workflow & Philosophy\n\nBayesian workflow – Gelman et al.\nScientific Reasoning: The Bayesian Approach – Howson & Urbach\n\n\n\n6.3 📘 Advanced Theory\n\nBayesian Data Analysis (3rd Edition)\n\n\n\n6.4 🌱 Agronomy Applications\n\nMakowski et al., 2020 (EJA)\n\n\n\n6.5 🗣️ Miscellaneous\n\nStatistical Modeling Blog – Gelman et al.\nLearning Bayesian Statistics Podcast",
    "crumbs": [
      "Lessons",
      "20-Bayesian intro"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html",
    "href": "coding/week_05/09_ggplot2_plus.html",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "",
    "text": "Description\nIn this class, we will explore how to create geographic maps using ggplot2, sf, maps, leaflet, and geojson. We’ll cover techniques for plotting data points on US and Canada maps, customizing map aesthetics, and working with spatial data.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#us-map-with-state-boundaries",
    "href": "coding/week_05/09_ggplot2_plus.html#us-map-with-state-boundaries",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "2.1 US Map with State Boundaries",
    "text": "2.1 US Map with State Boundaries\n# Load US map data\nus_map &lt;- map_data(\"state\")\n\n# Plot US map\nus_plot &lt;- \n  ggplot() +\n  geom_polygon(data = us_map, aes(x = long, y = lat, group = group),\n               fill = \"grey90\", color = \"grey35\") +\n  coord_fixed(1.3) +\n  labs(title = \"Map of the United States\") +\n  theme_base()\n\nus_plot",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#canada-map-with-provincial-boundaries",
    "href": "coding/week_05/09_ggplot2_plus.html#canada-map-with-provincial-boundaries",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "2.2 Canada Map with Provincial Boundaries",
    "text": "2.2 Canada Map with Provincial Boundaries\n# Load Canada map data\ncanada_map &lt;- map_data(\"world\", region = \"Canada\")\n\n# Plot Canada map\ncanada_plot &lt;- ggplot() +\n  geom_polygon(data = canada_map, aes(x = long, y = lat, group = group),\n               fill = \"steelblue\", color = \"black\") +\n  coord_fixed(1.3) +\n  labs(title = \"Map of Canada\")\n\ncanada_plot\n\ncanada_cut &lt;- canada_plot +\n  # Cut limits of map\n  scale_y_continuous(limits = c(41, 48))+\n  scale_x_continuous(limits = c(-87, -75))+\n  theme_minimal()\n\ncanada_cut",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-us-map",
    "href": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-us-map",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "3.1 Plotting Cities on US Map",
    "text": "3.1 Plotting Cities on US Map\n# Sample city data\ncities_us &lt;- data.frame(\n  city = c(\"New Jersey\", \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"),\n  lon = c(-74, -74.006, -118.2437, -87.6298, -95.3698, -112.074),\n  lat = c(40.9, 40.7128, 34.0522, 41.8781, 29.7604, 33.4484)\n)\n\n# Plot US map with cities\nus_plot +\n  geom_point(data = cities_us, aes(x = lon, y = lat), \n             color = \"red\", size = 3) +\n  # geom_label(data = cities_us, aes(x = lon, y = lat, label = city), \n  #                  size = 3, color = \"black\")\n  geom_label_repel(data = cities_us, aes(x = lon, y = lat, label = city), \n                    size = 3, color = \"black\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-canada-map",
    "href": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-canada-map",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "3.2 Plotting Cities on Canada Map",
    "text": "3.2 Plotting Cities on Canada Map\n# Sample city data for Canada\ncities_canada &lt;- data.frame(\n  city = c(\"Toronto\", \"Vancouver\", \"Montreal\", \"Calgary\", \"Ottawa\"),\n  lon = c(-79.3832, -123.1216, -73.5673, -114.0719, -75.6972),\n  lat = c(43.6511, 49.2827, 45.5017, 51.0447, 45.4215)\n)\n\n# Plot Canada map with cities\ncanada_plot +\n  geom_point(data = cities_canada, aes(x = lon, y = lat), \n             color = \"blue\", size = 3) +\n  geom_label_repel(data = cities_canada, aes(x = lon, y = lat, label = city), \n                   size = 3, color = \"black\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#loading-and-plotting-shapefiles",
    "href": "coding/week_05/09_ggplot2_plus.html#loading-and-plotting-shapefiles",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.1 Loading and Plotting Shapefiles",
    "text": "4.1 Loading and Plotting Shapefiles\n# Load shapefile (replace 'path_to_shapefile' with your actual path)\n# usa_shapefile &lt;- st_read(\"path_to_shapefile/usa_shapefile.shp\")\n\n# Example using built-in dataset from `sf`\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\n# Plot shapefile\nggplot(nc) +\n  geom_sf(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Shapefile Example: North Carolina Counties\") +\n  # Adjust scales for limits\n  scale_y_continuous(limits = c(34.5, 36))+\n  scale_x_continuous(limits = c(-82, -78))+\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#read-shp-of-canada-and-us",
    "href": "coding/week_05/09_ggplot2_plus.html#read-shp-of-canada-and-us",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.2 Read shp of Canada and US",
    "text": "4.2 Read shp of Canada and US\n## Load shp files\nusa_shp &lt;- st_read(\"shp_map/us/usa.shp\") %&gt;%\n  # Remove non-contiguous and territories\n  filter(!(NAME %in% c(\"Alaska\", \"District of Columbia\", \"Hawaii\", \"Puerto Rico\")))\n\ncan_shp &lt;- st_read(\"shp_map/can/canada.shp\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#create-objects-for-maps",
    "href": "coding/week_05/09_ggplot2_plus.html#create-objects-for-maps",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.3 Create objects for maps",
    "text": "4.3 Create objects for maps\n# Create list of selected provinces\nselected_provinces &lt;- c(\"Ontario\", \"Manitoba\", \"Quebec\")\n\n# Define coordinates for Ontario cities\nontario_cities &lt;- data.frame(\n  city = c(\"Toronto\", \"Ottawa\", \"Hamilton\", \"London\", \"Kingston\"),\n  lon = c(-79.3832, -75.6972, -79.8711, -81.2497, -76.4880),\n  lat = c(43.6511, 45.4215, 43.2557, 42.9834, 44.2312)\n)",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#define-function-to-customize-plot",
    "href": "coding/week_05/09_ggplot2_plus.html#define-function-to-customize-plot",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.4 Define function to customize plot",
    "text": "4.4 Define function to customize plot\ngeo_plot &lt;- function(x, y, z, title = NULL){\n  ggplot()+\n    geom_sf(data=x, fill = \"white\", color = \"black\") + # Provinces map\n  geom_sf(data=y, fill = \"white\", color = \"black\")+ # US map\n  # Adjust scales for lat and lon\n  scale_y_continuous(limits = c(41.8, 46))+\n  scale_x_continuous(limits = c(-84, -75), breaks = seq(-84,-74, by=1)) +\n  # Add cities with points\n  geom_point(data = z, aes(x = lon, y = lat, fill = city), \n             color = \"grey25\", shape = 21, size = 3, alpha = 0.95) +\n  # Scalebar\n  annotation_scale(tick_height = 0.3)+\n  # Text Notes for names of cities\n  geom_text_repel(data = z, \n                  aes(x=lon, y=lat, label = city), size = 3)+\n  # Name of PROVINCE\n  annotate(\"text\", x = -78, y = 45, label = \"ONTARIO\", \n           size = 4, fontface = \"bold\")+\n  # Name of Lakes\n  ## Ontario\n  annotate(\"text\", x = -77.8, y = 43.7, label = \"Lake Ontario\", \n           size = 3, fontface = \"italic\")+\n  ## Huron \n  annotate(\"text\", x = -82.5, y = 44.5, label = \"Lake \\nHuron\", \n           size = 3, fontface = \"italic\")+\n  # Add labels\n  labs(title = title, \n       x = \"Longitude\", y = \"Latitude\")+\n  # Adjust theme\n  theme_base()+\n  # reduce axis text size\n  theme(\n    panel.background = element_rect(fill = \"#bde0fe\"),\n    title = element_text(size = rel(.7)),\n    axis.title =  element_text(size = rel(.9), face = \"bold\"),\n    axis.text = element_text(size = rel(.5))\n    )\n}",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plot",
    "href": "coding/week_05/09_ggplot2_plus.html#plot",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.5 Plot",
    "text": "4.5 Plot\n# Plot from function\ngeo_plot(x = can_shp, y = usa_shp, z = ontario_cities, \n         title = \"Ontario Map from SHP\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#loading-geojson-data",
    "href": "coding/week_05/09_ggplot2_plus.html#loading-geojson-data",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.1 Loading GeoJSON Data",
    "text": "5.1 Loading GeoJSON Data\n# Load Canada GeoJSON (replace with actual file path if available)\n# Read geojson for Canada\ncan_geojson &lt;- read_sf(\"geojson_maps/canada_provinces.geojson\")  %&gt;%  \n  # dplyr::filter(name == \"Ontario\")  %&gt;% \n  dplyr::filter(name %in% selected_provinces)  %&gt;% \n  dplyr::mutate(Province = name, \n                GEOID = cartodb_id)  %&gt;%  \n  dplyr::select(GEOID, Province, geometry)\n\n# Read geojson for US\nusa_geojson &lt;- read_sf(\"geojson_maps/us-states.json\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plot-geojson",
    "href": "coding/week_05/09_ggplot2_plus.html#plot-geojson",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.2 Plot GEOJSON",
    "text": "5.2 Plot GEOJSON\n# Plot from function\ngeo_plot(x = can_geojson, y = usa_geojson, z = ontario_cities, \n         title = \"Ontario Map from GEOJSON\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#explanation",
    "href": "coding/week_05/09_ggplot2_plus.html#explanation",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.3 Explanation:",
    "text": "5.3 Explanation:\n\ngeojson_read(): Reads the GeoJSON file.\nst_as_sf(): Converts the data into an sf object for plotting.\ngeom_sf(): Plots the GeoJSON data.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#ontario-map-with-leaflet",
    "href": "coding/week_05/09_ggplot2_plus.html#ontario-map-with-leaflet",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "6.1 Ontario Map with leaflet",
    "text": "6.1 Ontario Map with leaflet\n{r echo=TRUE, message=FALSE, warning=FALSE} # Create interactive map ontario_cities %&gt;% leaflet() %&gt;%   addTiles() %&gt;% # Add PINS addMarkers(~lon, ~lat, popup = ~city) %&gt;% addCircleMarkers(~lon, ~lat, popup = ~city, radius = 5, color = \"gold\",                   fillOpacity = 0.7) %&gt;% # Configure initial view of the map setView(lng = -80, lat = 44, zoom = 6)",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#explanation-1",
    "href": "coding/week_05/09_ggplot2_plus.html#explanation-1",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "6.2 Explanation:",
    "text": "6.2 Explanation:\n\naddTiles(): Adds the base map layer.\naddMarkers(): Plots city locations with popups displaying city names.\nsetView(): Centers the map on Toronto with a specified zoom level.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html",
    "href": "coding/week_02/02_fundamentals_rpackages.html",
    "title": "Fundamentals of R Packages",
    "section": "",
    "text": "i. What Are R Packages?\nR packages are collections of functions, data, and documentation that extend the capabilities of R. They are designed to solve specific problems or add functionalities, such as data visualization, statistical modeling, or handling specific types of data.\nAnalogy: Think of R as a toolbox 🧰 and packages as individual tools 🔧 you can add to enhance its utility.\n\n\n\n\n\n\nImportant\n\n\n\nCore Components:\n\nFunctions: Ready-made commands to perform tasks.\nData: Preloaded datasets for analysis or examples.\nDocumentation: Manuals explaining how to use the package.\n\n\n\nii. Why Are R Packages Important?\n\nThey extend functionality beyond base features.\nEnable efficient workflows by using pre-written and optimized code.\nProvide community-contributed solutions for a wide variety of domains (e.g., agriculture, bioinformatics, machine learning).\n\niii. Where to Find R Packages?\n\nCRAN (Comprehensive R Archive Network): The primary repository for R packages. Well-maintained and includes thousands of packages.\nBioconductor: Specialized in bioinformatics and genomics.\nGitHub: A platform where developers host and share experimental or in-development packages.",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#a.-installing-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#a.-installing-packages",
    "title": "Fundamentals of R Packages",
    "section": "a. Installing packages",
    "text": "a. Installing packages\nTo install a package from CRAN, use:\n\ninstall.packages(\"dplyr\")\n\nTo install a development version of a package from GitHub, use:\n\ndevtools::install_github(\"rstudio/ggplot2\")\n\nChecking installed packages\n\ninstalled.packages()",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#b.-loading-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#b.-loading-packages",
    "title": "Fundamentals of R Packages",
    "section": "b. Loading Packages",
    "text": "b. Loading Packages\nOnce installed, load a package using:\n\nlibrary(dplyr)\n\nOr alternatively, you may use packages like pacman:\n\nlibrary(pacman)\npacman::p_load(dplyr, ggplot2)\n\nOr easypackages:\n\nlibrary(easypackages)\neasypackages::libraries(dplyr, ggplot2)",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#c.-updating-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#c.-updating-packages",
    "title": "Fundamentals of R Packages",
    "section": "c. Updating packages",
    "text": "c. Updating packages\n\nupdate.packages()",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#d.-unload-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#d.-unload-packages",
    "title": "Fundamentals of R Packages",
    "section": "d. Unload packages",
    "text": "d. Unload packages\n\ndetach(\"package:ggplot2\", unload = TRUE)",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#e.-uninstall-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#e.-uninstall-packages",
    "title": "Fundamentals of R Packages",
    "section": "e. Uninstall packages",
    "text": "e. Uninstall packages\n\nremove.packages(\"ggplot2\")",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#a.-start-with-the-right-packages",
    "href": "coding/week_02/02_fundamentals_rpackages.html#a.-start-with-the-right-packages",
    "title": "Fundamentals of R Packages",
    "section": "a. Start with the Right Packages:",
    "text": "a. Start with the Right Packages:\nUse foundational and well supported packages (e.g. tidyverse, data.table).",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#b.-stay-curious",
    "href": "coding/week_02/02_fundamentals_rpackages.html#b.-stay-curious",
    "title": "Fundamentals of R Packages",
    "section": "b. Stay Curious:",
    "text": "b. Stay Curious:\nExplore new packages via CRAN Task Views (e.g., Agriculture Task View).",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#c.-version-control",
    "href": "coding/week_02/02_fundamentals_rpackages.html#c.-version-control",
    "title": "Fundamentals of R Packages",
    "section": "c. Version Control:",
    "text": "c. Version Control:\nWhen becoming an advanced user, you could implement renv or packrat to manage package versions for reproducible analysis.",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#a.-ggplot2-for-data-visualization",
    "href": "coding/week_02/02_fundamentals_rpackages.html#a.-ggplot2-for-data-visualization",
    "title": "Fundamentals of R Packages",
    "section": "a. ggplot2 for data visualization:",
    "text": "a. ggplot2 for data visualization:\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = mpg, y = hp)) + geom_point()",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#b.-dplyr-for-data-manipulation",
    "href": "coding/week_02/02_fundamentals_rpackages.html#b.-dplyr-for-data-manipulation",
    "title": "Fundamentals of R Packages",
    "section": "b. dplyr for data manipulation:",
    "text": "b. dplyr for data manipulation:\n\nlibrary(dplyr)\nmtcars %&gt;% filter(mpg &gt; 20) %&gt;% summarize(mean_hp = mean(hp))\n\n  mean_hp\n1    88.5",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#c.-agridat",
    "href": "coding/week_02/02_fundamentals_rpackages.html#c.-agridat",
    "title": "Fundamentals of R Packages",
    "section": "c. agridat",
    "text": "c. agridat\nThe agridat package contains datasets related to agriculture, such as crop yields, experimental designs, and climate data. Let’s use an example from this package:\n\n# Load the package\nlibrary(agridat)\n# Load an example dataset\ndata(rothamsted.oats, package = 'agridat')\nhead(rothamsted.oats)\n\n  block trt  grain straw row col\n1     x  oa 61.375  83.0  12   1\n2     x 2me 68.750 130.0  12   2\n3     x 2sl 64.250 100.0  12   3\n4     x  ob 65.500  96.0  12   4\n5     w 2sl 79.625 130.5  12   5\n6     w  oa 79.250 122.0  12   6\n\n# Visualize crop yields\nlibrary(ggplot2)\nggplot(rothamsted.oats, aes(x = trt, y = grain, fill = as.factor(block))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Grain Yields in Rothamsted Oats Experiment\",\n    x = \"Treatment\",\n    y = \"Grain Yield (grams/plot)\",\n    fill = \"Block\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#d.-agricolae",
    "href": "coding/week_02/02_fundamentals_rpackages.html#d.-agricolae",
    "title": "Fundamentals of R Packages",
    "section": "d. agricolae",
    "text": "d. agricolae\nThe agricolae package in R is a comprehensive toolset designed for statistical analysis and visualization of agricultural experiments. It is particularly useful for researchers and practitioners working in crop science, agronomy, and other fields of agricultural research.\nIt provides methods for designing experiments, analyzing experimental data, and visualizing results, particularly for data collected in agricultural and biological research.\n\n\n\n\n\n\nNote\n\n\n\nIt supports:\n\nExperimental Designs: Generate designs for field experiments like randomized complete block design (RCBD), Latin square, factorial experiments, and others.\nStatistical Analysis: Analyze variance (ANOVA), perform post-hoc tests (e.g., LSD, Tukey HSD), and assess experimental data.\nVisualization: Create plots for results, including mean comparisons, dendrograms, and histograms.\nAgronomic Tools: Calculate indices like stability for crop yields or pest/disease control measures.\n\n\n\n\ni. Data analysis\n\n# Load agricolae package\nlibrary(agricolae)\n\n# Simulate agricultural data\ndata &lt;- data.frame(\n  treatment = rep(c(\"A\", \"B\", \"C\"), each = 5),\n  yield = c(50, 55, 52, 51, 54,   # Yields for Treatment A\n            60, 62, 59, 61, 63,   # Yields for Treatment B\n            48, 46, 50, 49, 47)   # Yields for Treatment C\n)\n\n# Display the dataset\nprint(data)\n\n   treatment yield\n1          A    50\n2          A    55\n3          A    52\n4          A    51\n5          A    54\n6          B    60\n7          B    62\n8          B    59\n9          B    61\n10         B    63\n11         C    48\n12         C    46\n13         C    50\n14         C    49\n15         C    47\n\n# Perform ANOVA\nanova_result &lt;- aov(yield ~ treatment, data = data)\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    2  437.2   218.6   70.52 2.32e-07 ***\nResiduals   12   37.2     3.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Perform LSD test for pairwise comparison\nlsd_result &lt;- LSD.test(anova_result, \"treatment\", console = TRUE)\n\n\nStudy: anova_result ~ \"treatment\"\n\nLSD t Test for yield \n\nMean Square Error:  3.1 \n\ntreatment,  means and individual ( 95 %) CI\n\n  yield      std r        se     LCL     UCL Min Max Q25 Q50 Q75\nA  52.4 2.073644 5 0.7874008 50.6844 54.1156  50  55  51  52  54\nB  61.0 1.581139 5 0.7874008 59.2844 62.7156  59  63  60  61  62\nC  48.0 1.581139 5 0.7874008 46.2844 49.7156  46  50  47  48  49\n\nAlpha: 0.05 ; DF Error: 12\nCritical Value of t: 2.178813 \n\nleast Significant Difference: 2.426223 \n\nTreatments with the same letter are not significantly different.\n\n  yield groups\nB  61.0      a\nA  52.4      b\nC  48.0      c\n\n# Could also run Tukey HSD, & Duncan's Test\n\n# Plot LSD results\nplot(lsd_result, main = \"LSD Test Results for Treatment Yields\")\n\n\n\n\n\n\n\n\n\n\nii. Design of experiments\nYou can generate layouts for randomized complete block designs (RCBD), Latin squares, Graeco-Latin squares, factorial experiments, and split-plot designs\n\ndesign &lt;- design.rcbd(trt = c(\"A\", \"B\", \"C\"), r = 3)\nprint(design)\n\n$parameters\n$parameters$design\n[1] \"rcbd\"\n\n$parameters$trt\n[1] \"A\" \"B\" \"C\"\n\n$parameters$r\n[1] 3\n\n$parameters$serie\n[1] 2\n\n$parameters$seed\n[1] 2000902035\n\n$parameters$kinds\n[1] \"Super-Duper\"\n\n$parameters[[7]]\n[1] TRUE\n\n\n$sketch\n     [,1] [,2] [,3]\n[1,] \"A\"  \"B\"  \"C\" \n[2,] \"A\"  \"C\"  \"B\" \n[3,] \"C\"  \"A\"  \"B\" \n\n$book\n  plots block c(\"A\", \"B\", \"C\")\n1   101     1                A\n2   102     1                B\n3   103     1                C\n4   201     2                A\n5   202     2                C\n6   203     2                B\n7   301     3                C\n8   302     3                A\n9   303     3                B",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_02/02_fundamentals_rpackages.html#e.-soiltestcorr",
    "href": "coding/week_02/02_fundamentals_rpackages.html#e.-soiltestcorr",
    "title": "Fundamentals of R Packages",
    "section": "e. soiltestcorr",
    "text": "e. soiltestcorr\nThis package assists users on reproducible regression analysis of relationships between crop relative yield (ry) and soil test values (stv) under different approaches.\nFor example, we can fit a linear-plateu model the a dataset with:\n\nlibrary(soiltestcorr)\n\ndata_freitas &lt;- soiltestcorr::freitas1966\n\nplot_lp &lt;- linear_plateau(data = data_freitas,\n                          stv = STK, ry = RY, plot = TRUE)\n\nplot_lp",
    "crumbs": [
      "Lessons",
      "02-Fundamentals of packages"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html",
    "href": "coding/week_03/04_datawrangling_01.html",
    "title": "Transforming Ag data with dplyr",
    "section": "",
    "text": "This lesson introduces the concept of tidy data, and a few basic data wrangling techniques using dplyr package. Today, we are using dplyr and datasets from the agridat package. If you don’t have them installed, you can do so by running:\n\n\n\nlibrary(pacman)\np_load(agridat) # Agridat datasets\np_load(dplyr) # dplyr for data wrangling\np_load(skimr) # skimr for quick exploration of the data",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#required-packages-for-today",
    "href": "coding/week_03/04_datawrangling_01.html#required-packages-for-today",
    "title": "Transforming Ag data with dplyr",
    "section": "",
    "text": "library(pacman)\np_load(agridat) # Agridat datasets\np_load(dplyr) # dplyr for data wrangling\np_load(skimr) # skimr for quick exploration of the data",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#what-is-a-data-frame",
    "href": "coding/week_03/04_datawrangling_01.html#what-is-a-data-frame",
    "title": "Transforming Ag data with dplyr",
    "section": "2.1 What is a Data Frame?",
    "text": "2.1 What is a Data Frame?\nA data frame is a two-dimensional table-like structure in R, where columns can contain different types of data (e.g., numeric, character). It is the default structure for datasets loaded from CSV files or data packages.\n\n2.1.1 open a data frame\n\n# Load the wheat dataset from agricolae (which is a data frame)\nwheat_data &lt;- agridat::payne.wheat\n\n# Check the structure of the data frame\nstr(wheat_data)\n\n'data.frame':   480 obs. of  4 variables:\n $ rotation: Factor w/ 6 levels \"AB\",\"AF\",\"Lc3\",..: 1 1 1 1 2 2 2 2 5 5 ...\n $ nitro   : int  0 70 140 210 0 70 140 210 0 70 ...\n $ year    : int  1981 1981 1981 1981 1981 1981 1981 1981 1981 1981 ...\n $ yield   : num  3.84 6.59 7.49 7.39 3.06 6.32 7.61 7.78 5.82 7.52 ...",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#what-is-a-tibble",
    "href": "coding/week_03/04_datawrangling_01.html#what-is-a-tibble",
    "title": "Transforming Ag data with dplyr",
    "section": "2.2 What is a Tibble?",
    "text": "2.2 What is a Tibble?\nA tibble is a modern version of a data frame, introduced by the tibble package. It offers several improvements:\n•   Tibbles don’t convert characters to factors by default.\n•   Printing is more concise and doesn’t overwhelm you with too much data.\n•   Tibbles are more explicit with column types when printed.\n\n2.2.1 create a tibble\n\n# Convert the wheat data frame to a tibble\nwheat_tibble &lt;- as_tibble(wheat_data)\n\n# Check the structure of the tibble\nwheat_tibble\n\n# A tibble: 480 × 4\n   rotation nitro  year yield\n   &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 AB           0  1981  3.84\n 2 AB          70  1981  6.59\n 3 AB         140  1981  7.49\n 4 AB         210  1981  7.39\n 5 AF           0  1981  3.06\n 6 AF          70  1981  6.32\n 7 AF         140  1981  7.61\n 8 AF         210  1981  7.78\n 9 Ln3          0  1981  5.82\n10 Ln3         70  1981  7.52\n# ℹ 470 more rows",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#iii.-key-differences-between-data-frames-and-tibbles",
    "href": "coding/week_03/04_datawrangling_01.html#iii.-key-differences-between-data-frames-and-tibbles",
    "title": "Transforming Ag data with dplyr",
    "section": "2.3 iii. Key Differences between Data Frames and Tibbles",
    "text": "2.3 iii. Key Differences between Data Frames and Tibbles\n1.  **Printing**:\n• Data Frames print the entire dataset unless you limit the number of rows. No information about column types is displayed. • Tibbles print only the first 10 rows and automatically show column types.\n\n2.3.1 Example:\n\n# Print the entire data frame\nprint(wheat_data)\n\n    rotation nitro year yield\n1         AB     0 1981  3.84\n2         AB    70 1981  6.59\n3         AB   140 1981  7.49\n4         AB   210 1981  7.39\n5         AF     0 1981  3.06\n6         AF    70 1981  6.32\n7         AF   140 1981  7.61\n8         AF   210 1981  7.78\n9        Ln3     0 1981  5.82\n10       Ln3    70 1981  7.52\n11       Ln3   140 1981  8.12\n12       Ln3   210 1981  7.40\n13       Ln8     0 1981  4.71\n14       Ln8    70 1981  6.52\n15       Ln8   140 1981  8.03\n16       Ln8   210 1981  7.83\n17       Lc3     0 1981  5.35\n18       Lc3    70 1981  6.70\n19       Lc3   140 1981  7.69\n20       Lc3   210 1981  7.53\n21       Lc8     0 1981  6.47\n22       Lc8    70 1981  7.84\n23       Lc8   140 1981  7.98\n24       Lc8   210 1981  7.68\n25        AB     0 1982  4.47\n26        AB    70 1982  6.38\n27        AB   140 1982  7.82\n28        AB   210 1982  8.13\n29        AF     0 1982  4.30\n30        AF    70 1982  6.82\n31        AF   140 1982  8.16\n32        AF   210 1982  8.52\n33       Ln3     0 1982  5.37\n34       Ln3    70 1982  7.91\n35       Ln3   140 1982  7.53\n36       Ln3   210 1982  8.46\n37       Ln8     0 1982  5.55\n38       Ln8    70 1982  8.04\n39       Ln8   140 1982  8.27\n40       Ln8   210 1982  7.31\n41       Lc3     0 1982  5.16\n42       Lc3    70 1982  7.81\n43       Lc3   140 1982  8.38\n44       Lc3   210 1982  7.40\n45       Lc8     0 1982  6.56\n46       Lc8    70 1982  8.36\n47       Lc8   140 1982  8.60\n48       Lc8   210 1982  8.41\n49        AB     0 1983  4.11\n50        AB    70 1983  6.28\n51        AB   140 1983  8.70\n52        AB   210 1983  8.17\n53        AF     0 1983  3.76\n54        AF    70 1983  6.79\n55        AF   140 1983  8.50\n56        AF   210 1983  9.43\n57       Ln3     0 1983  3.86\n58       Ln3    70 1983  7.05\n59       Ln3   140 1983  8.06\n60       Ln3   210 1983  8.28\n61       Ln8     0 1983  4.45\n62       Ln8    70 1983  7.23\n63       Ln8   140 1983  7.48\n64       Ln8   210 1983  6.93\n65       Lc3     0 1983  6.36\n66       Lc3    70 1983  9.67\n67       Lc3   140 1983  9.34\n68       Lc3   210 1983  8.40\n69       Lc8     0 1983  7.39\n70       Lc8    70 1983  9.64\n71       Lc8   140 1983  8.80\n72       Lc8   210 1983  8.66\n73        AB     0 1984  3.66\n74        AB    70 1984  6.56\n75        AB   140 1984  7.74\n76        AB   210 1984  9.41\n77        AF     0 1984  4.28\n78        AF    70 1984  8.94\n79        AF   140 1984  9.12\n80        AF   210 1984  9.35\n81       Ln3     0 1984  4.92\n82       Ln3    70 1984  7.66\n83       Ln3   140 1984  9.75\n84       Ln3   210 1984 10.35\n85       Ln8     0 1984  5.46\n86       Ln8    70 1984  8.68\n87       Ln8   140 1984  9.20\n88       Ln8   210 1984 10.33\n89       Lc3     0 1984  7.18\n90       Lc3    70 1984 11.06\n91       Lc3   140 1984 10.52\n92       Lc3   210 1984  9.88\n93       Lc8     0 1984  7.51\n94       Lc8    70 1984  9.66\n95       Lc8   140 1984 11.04\n96       Lc8   210 1984  9.36\n97        AB     0 1985  2.39\n98        AB    70 1985  5.90\n99        AB   140 1985  7.76\n100       AB   210 1985  8.62\n101       AF     0 1985  2.03\n102       AF    70 1985  5.46\n103       AF   140 1985  7.72\n104       AF   210 1985  9.20\n105      Ln3     0 1985  4.24\n106      Ln3    70 1985  7.26\n107      Ln3   140 1985  8.26\n108      Ln3   210 1985  9.69\n109      Ln8     0 1985  4.07\n110      Ln8    70 1985  6.98\n111      Ln8   140 1985  8.39\n112      Ln8   210 1985  8.55\n113      Lc3     0 1985  4.97\n114      Lc3    70 1985  7.64\n115      Lc3   140 1985  9.57\n116      Lc3   210 1985  8.84\n117      Lc8     0 1985  4.44\n118      Lc8    70 1985  8.08\n119      Lc8   140 1985  8.76\n120      Lc8   210 1985 10.19\n121       AB     0 1986  4.17\n122       AB    70 1986  6.91\n123       AB   140 1986  7.21\n124       AB   210 1986  8.53\n125       AF     0 1986  4.08\n126       AF    70 1986  5.08\n127       AF   140 1986  6.32\n128       AF   210 1986  7.88\n129      Ln3     0 1986  3.36\n130      Ln3    70 1986  5.65\n131      Ln3   140 1986  6.62\n132      Ln3   210 1986  6.05\n133      Ln8     0 1986  4.68\n134      Ln8    70 1986  6.55\n135      Ln8   140 1986  7.20\n136      Ln8   210 1986  6.84\n137      Lc3     0 1986  6.14\n138      Lc3    70 1986  7.15\n139      Lc3   140 1986  6.89\n140      Lc3   210 1986  6.20\n141      Lc8     0 1986  6.09\n142      Lc8    70 1986  7.31\n143      Lc8   140 1986  6.85\n144      Lc8   210 1986  6.75\n145       AB     0 1987  4.39\n146       AB    70 1987  6.18\n147       AB   140 1987  6.75\n148       AB   210 1987  7.84\n149       AF     0 1987  3.02\n150       AF    70 1987  5.56\n151       AF   140 1987  6.60\n152       AF   210 1987  6.43\n153      Ln3     0 1987  4.41\n154      Ln3    70 1987  6.55\n155      Ln3   140 1987  7.59\n156      Ln3   210 1987  7.13\n157      Ln8     0 1987  4.80\n158      Ln8    70 1987  6.74\n159      Ln8   140 1987  7.86\n160      Ln8   210 1987  7.00\n161      Lc3     0 1987  5.51\n162      Lc3    70 1987  7.24\n163      Lc3   140 1987  7.74\n164      Lc3   210 1987  7.61\n165      Lc8     0 1987  5.26\n166      Lc8    70 1987  7.48\n167      Lc8   140 1987  8.31\n168      Lc8   210 1987  8.13\n169       AB     0 1988  2.98\n170       AB    70 1988  6.28\n171       AB   140 1988  6.77\n172       AB   210 1988  6.20\n173       AF     0 1988  3.09\n174       AF    70 1988  6.60\n175       AF   140 1988  6.63\n176       AF   210 1988  6.61\n177      Ln3     0 1988  4.01\n178      Ln3    70 1988  6.77\n179      Ln3   140 1988  7.12\n180      Ln3   210 1988  6.14\n181      Ln8     0 1988  4.34\n182      Ln8    70 1988  6.73\n183      Ln8   140 1988  7.46\n184      Ln8   210 1988  7.23\n185      Lc3     0 1988  5.68\n186      Lc3    70 1988  7.39\n187      Lc3   140 1988  7.54\n188      Lc3   210 1988  7.51\n189      Lc8     0 1988  5.26\n190      Lc8    70 1988  7.87\n191      Lc8   140 1988  6.94\n192      Lc8   210 1988  7.06\n193       AB     0 1989  1.16\n194       AB    70 1989  3.94\n195       AB   140 1989  4.58\n196       AB   210 1989  4.74\n197       AF     0 1989  2.80\n198       AF    70 1989  4.92\n199       AF   140 1989  5.17\n200       AF   210 1989  5.82\n201      Ln3     0 1989  4.04\n202      Ln3    70 1989  5.94\n203      Ln3   140 1989  6.10\n204      Ln3   210 1989  6.04\n205      Ln8     0 1989  3.77\n206      Ln8    70 1989  5.58\n207      Ln8   140 1989  5.56\n208      Ln8   210 1989  4.91\n209      Lc3     0 1989  5.45\n210      Lc3    70 1989  6.28\n211      Lc3   140 1989  6.12\n212      Lc3   210 1989  5.81\n213      Lc8     0 1989  4.91\n214      Lc8    70 1989  6.69\n215      Lc8   140 1989  6.39\n216      Lc8   210 1989  5.06\n217       AB     0 1990  1.47\n218       AB    70 1990  4.94\n219       AB   140 1990  5.83\n220       AB   210 1990  6.33\n221       AF     0 1990  1.38\n222       AF    70 1990  5.72\n223       AF   140 1990  6.30\n224       AF   210 1990  5.18\n225      Ln3     0 1990  1.73\n226      Ln3    70 1990  4.94\n227      Ln3   140 1990  5.43\n228      Ln3   210 1990  6.17\n229      Ln8     0 1990  2.62\n230      Ln8    70 1990  5.79\n231      Ln8   140 1990  5.08\n232      Ln8   210 1990  5.25\n233      Lc3     0 1990  3.59\n234      Lc3    70 1990  6.06\n235      Lc3   140 1990  7.20\n236      Lc3   210 1990  6.42\n237      Lc8     0 1990  3.31\n238      Lc8    70 1990  6.51\n239      Lc8   140 1990  6.65\n240      Lc8   210 1990  6.99\n241       AB     0 1991  4.48\n242       AB    70 1991  8.56\n243       AB   140 1991  9.94\n244       AB   210 1991 10.23\n245       AF     0 1991  3.46\n246       AF    70 1991  8.00\n247       AF   140 1991  9.75\n248       AF   210 1991 10.57\n249      Ln3     0 1991  6.75\n250      Ln3    70 1991  8.85\n251      Ln3   140 1991  9.96\n252      Ln3   210 1991 10.41\n253      Ln8     0 1991  5.94\n254      Ln8    70 1991  8.83\n255      Ln8   140 1991  9.64\n256      Ln8   210 1991  9.75\n257      Lc3     0 1991  6.47\n258      Lc3    70 1991  9.37\n259      Lc3   140 1991 10.46\n260      Lc3   210 1991 10.48\n261      Lc8     0 1991  6.08\n262      Lc8    70 1991  8.81\n263      Lc8   140 1991  9.63\n264      Lc8   210 1991 10.10\n265       AB     0 1992  6.31\n266       AB    70 1992  7.84\n267       AB   140 1992  7.21\n268       AB   210 1992  6.81\n269       AF     0 1992  3.82\n270       AF    70 1992  8.05\n271       AF   140 1992  8.21\n272       AF   210 1992  7.59\n273      Ln3     0 1992  2.73\n274      Ln3    70 1992  6.47\n275      Ln3   140 1992  7.49\n276      Ln3   210 1992  7.26\n277      Ln8     0 1992  4.19\n278      Ln8    70 1992  7.17\n279      Ln8   140 1992  7.54\n280      Ln8   210 1992  6.67\n281      Lc3     0 1992  6.33\n282      Lc3    70 1992  7.48\n283      Lc3   140 1992  6.13\n284      Lc3   210 1992  4.79\n285      Lc8     0 1992  7.11\n286      Lc8    70 1992  6.65\n287      Lc8   140 1992  6.45\n288      Lc8   210 1992  6.14\n289       AB     0 1993  3.11\n290       AB    70 1993  5.92\n291       AB   140 1993  5.89\n292       AB   210 1993  6.63\n293       AF     0 1993  2.86\n294       AF    70 1993  5.79\n295       AF   140 1993  6.72\n296       AF   210 1993  7.37\n297      Ln3     0 1993  3.13\n298      Ln3    70 1993  5.40\n299      Ln3   140 1993  6.60\n300      Ln3   210 1993  6.52\n301      Ln8     0 1993  3.42\n302      Ln8    70 1993  5.16\n303      Ln8   140 1993  6.47\n304      Ln8   210 1993  6.55\n305      Lc3     0 1993  5.58\n306      Lc3    70 1993  7.01\n307      Lc3   140 1993  7.69\n308      Lc3   210 1993  7.91\n309      Lc8     0 1993  6.08\n310      Lc8    70 1993  7.03\n311      Lc8   140 1993  7.20\n312      Lc8   210 1993  7.69\n313       AB     0 1994  0.93\n314       AB    70 1994  3.94\n315       AB   140 1994  4.04\n316       AB   210 1994  3.51\n317       AF     0 1994  1.80\n318       AF    70 1994  5.32\n319       AF   140 1994  8.08\n320       AF   210 1994  8.55\n321      Ln3     0 1994  4.76\n322      Ln3    70 1994  6.16\n323      Ln3   140 1994  7.35\n324      Ln3   210 1994  7.14\n325      Ln8     0 1994  3.64\n326      Ln8    70 1994  5.14\n327      Ln8   140 1994  7.00\n328      Ln8   210 1994  7.16\n329      Lc3     0 1994  5.06\n330      Lc3    70 1994  6.00\n331      Lc3   140 1994  6.28\n332      Lc3   210 1994  7.50\n333      Lc8     0 1994  3.46\n334      Lc8    70 1994  6.48\n335      Lc8   140 1994  6.07\n336      Lc8   210 1994  7.53\n337       AB     0 1995  1.30\n338       AB    70 1995  4.21\n339       AB   140 1995  4.35\n340       AB   210 1995  4.35\n341       AF     0 1995  1.27\n342       AF    70 1995  3.82\n343       AF   140 1995  4.60\n344       AF   210 1995  4.96\n345      Ln3     0 1995  2.17\n346      Ln3    70 1995  5.01\n347      Ln3   140 1995  5.39\n348      Ln3   210 1995  5.79\n349      Ln8     0 1995  2.52\n350      Ln8    70 1995  5.71\n351      Ln8   140 1995  5.36\n352      Ln8   210 1995  6.53\n353      Lc3     0 1995  2.57\n354      Lc3    70 1995  5.70\n355      Lc3   140 1995  6.46\n356      Lc3   210 1995  5.78\n357      Lc8     0 1995  3.52\n358      Lc8    70 1995  6.60\n359      Lc8   140 1995  6.36\n360      Lc8   210 1995  6.14\n361       AB     0 1996  1.19\n362       AB    70 1996  7.24\n363       AB   140 1996  7.80\n364       AB   210 1996  8.43\n365       AF     0 1996  0.65\n366       AF    70 1996  6.60\n367       AF   140 1996  7.69\n368       AF   210 1996  7.79\n369      Ln3     0 1996  3.82\n370      Ln3    70 1996  7.19\n371      Ln3   140 1996  7.15\n372      Ln3   210 1996  8.41\n373      Ln8     0 1996  6.37\n374      Ln8    70 1996  8.23\n375      Ln8   140 1996  8.77\n376      Ln8   210 1996  8.46\n377      Lc3     0 1996  5.23\n378      Lc3    70 1996  7.76\n379      Lc3   140 1996  8.19\n380      Lc3   210 1996  8.67\n381      Lc8     0 1996  5.73\n382      Lc8    70 1996  7.97\n383      Lc8   140 1996  8.48\n384      Lc8   210 1996  8.28\n385       AB     0 1997  1.58\n386       AB    70 1997  5.73\n387       AB   140 1997  7.37\n388       AB   210 1997  7.88\n389       AF     0 1997  2.40\n390       AF    70 1997  6.52\n391       AF   140 1997  9.25\n392       AF   210 1997  9.24\n393      Ln3     0 1997  1.74\n394      Ln3    70 1997  3.83\n395      Ln3   140 1997  5.15\n396      Ln3   210 1997  5.02\n397      Ln8     0 1997  2.53\n398      Ln8    70 1997  6.20\n399      Ln8   140 1997  6.93\n400      Ln8   210 1997  7.25\n401      Lc3     0 1997  4.40\n402      Lc3    70 1997  7.70\n403      Lc3   140 1997  8.01\n404      Lc3   210 1997  8.30\n405      Lc8     0 1997  4.10\n406      Lc8    70 1997  6.78\n407      Lc8   140 1997  7.36\n408      Lc8   210 1997  7.43\n409       AB     0 1998  3.21\n410       AB    70 1998  6.70\n411       AB   140 1998  9.35\n412       AB   210 1998 10.26\n413       AF     0 1998  2.52\n414       AF    70 1998  6.35\n415       AF   140 1998  8.80\n416       AF   210 1998  9.72\n417      Ln3     0 1998  3.77\n418      Ln3    70 1998  7.13\n419      Ln3   140 1998  8.67\n420      Ln3   210 1998  9.62\n421      Ln8     0 1998  4.97\n422      Ln8    70 1998  7.77\n423      Ln8   140 1998  9.21\n424      Ln8   210 1998  9.24\n425      Lc3     0 1998  4.78\n426      Lc3    70 1998  7.48\n427      Lc3   140 1998  8.50\n428      Lc3   210 1998  8.75\n429      Lc8     0 1998  4.11\n430      Lc8    70 1998  7.55\n431      Lc8   140 1998  9.01\n432      Lc8   210 1998  8.98\n433       AB     0 1999  0.00\n434       AB    70 1999  1.97\n435       AB   140 1999  3.44\n436       AB   210 1999  2.28\n437       AF     0 1999  0.52\n438       AF    70 1999  6.55\n439       AF   140 1999  7.53\n440       AF   210 1999  8.48\n441      Ln3     0 1999  1.69\n442      Ln3    70 1999  6.58\n443      Ln3   140 1999  7.58\n444      Ln3   210 1999  7.83\n445      Ln8     0 1999  3.42\n446      Ln8    70 1999  6.59\n447      Ln8   140 1999  8.26\n448      Ln8   210 1999  6.51\n449      Lc3     0 1999  4.42\n450      Lc3    70 1999  7.27\n451      Lc3   140 1999  8.65\n452      Lc3   210 1999  9.54\n453      Lc8     0 1999  1.79\n454      Lc8    70 1999  4.65\n455      Lc8   140 1999  5.54\n456      Lc8   210 1999  4.95\n457       AB     0 2000  1.45\n458       AB    70 2000  4.54\n459       AB   140 2000  4.52\n460       AB   210 2000  5.53\n461       AF     0 2000  0.96\n462       AF    70 2000  4.87\n463       AF   140 2000  6.28\n464       AF   210 2000  7.39\n465      Ln3     0 2000  3.40\n466      Ln3    70 2000  7.06\n467      Ln3   140 2000  8.64\n468      Ln3   210 2000  8.71\n469      Ln8     0 2000  3.42\n470      Ln8    70 2000  6.58\n471      Ln8   140 2000  7.22\n472      Ln8   210 2000  7.49\n473      Lc3     0 2000  5.05\n474      Lc3    70 2000  8.24\n475      Lc3   140 2000  8.96\n476      Lc3   210 2000 10.33\n477      Lc8     0 2000  4.31\n478      Lc8    70 2000  7.47\n479      Lc8   140 2000  8.95\n480      Lc8   210 2000  9.65\n\n# Print the tibble (shows only first 10 rows and column types)\nprint(wheat_tibble)\n\n# A tibble: 480 × 4\n   rotation nitro  year yield\n   &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 AB           0  1981  3.84\n 2 AB          70  1981  6.59\n 3 AB         140  1981  7.49\n 4 AB         210  1981  7.39\n 5 AF           0  1981  3.06\n 6 AF          70  1981  6.32\n 7 AF         140  1981  7.61\n 8 AF         210  1981  7.78\n 9 Ln3          0  1981  5.82\n10 Ln3         70  1981  7.52\n# ℹ 470 more rows",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#why-using-r-packages",
    "href": "coding/week_03/04_datawrangling_01.html#why-using-r-packages",
    "title": "Transforming Ag data with dplyr",
    "section": "3.1 Why using R Packages? 💡",
    "text": "3.1 Why using R Packages? 💡\n\nEfficiency: Avoid rewriting code for common tasks.\nConsistency: Standardized code structure and naming conventions.\nReproducibility: Ensures that your work is easier to share and reproduce.\nIntuitive: R packages use functions with intuitive names of functions, so you can spend less time learning the code & more time learning to solve practical problems.",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#example-1-mutate",
    "href": "coding/week_03/04_datawrangling_01.html#example-1-mutate",
    "title": "Transforming Ag data with dplyr",
    "section": "3.2 Example 1: mutate()",
    "text": "3.2 Example 1: mutate()\n\nPackages like dplyr simplify tasks by providing clean, concise code for data manipulation.\n\n\n\n1. Create a new column: total, which is the sum of two existing columns (var1 and var2).\n\n\n\n3.2.0.1 Base R version\n\n\n# Sample data\ndf &lt;- data.frame(var1 = c(1, 2, 3), var2 = c(4, 5, 6))\n\n# Adding a new column using base R\ndf$total &lt;- df$var1 + df$var2\n\n\n\n\n\n3.2.0.2 dplyr package (Tidyverse)\nlibrary(dplyr)\n\n# Using mutate to add a new column\ndf &lt;- df %&gt;%\n  mutate(total = var1 + var2)",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#example-2-filter",
    "href": "coding/week_03/04_datawrangling_01.html#example-2-filter",
    "title": "Transforming Ag data with dplyr",
    "section": "3.3 Example 2: filter()",
    "text": "3.3 Example 2: filter()\n\n2. Filtering: get values of var1 greater than 2.\n\n\n\n3.3.0.1 Base R version\n\n# Filter rows using base R\nfiltered_df &lt;- df[df$var1 &gt; 2, ]\n\n\n\n\n\n3.3.0.2 dplyr package (Tidyverse)\n# Filter rows using dplyr\nfiltered_df &lt;- filter(data = df, var1 &gt; 2)",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#example-3-select",
    "href": "coding/week_03/04_datawrangling_01.html#example-3-select",
    "title": "Transforming Ag data with dplyr",
    "section": "3.4 Example 3: select()",
    "text": "3.4 Example 3: select()\n\n3. Select specific variables: get var1 and var3.\n\n\n\n\n3.4.0.1 Base R version\n# Select columns using base R\nselected_df &lt;- df[ , c(\"var1\",\"var3\")]\n\n\n\n\n\n3.4.0.2 dplyr package (Tidyverse)\n# Filter rows using dplyr\nfiltered_df &lt;- select(data = df, var1, var3)",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#inspect-data",
    "href": "coding/week_03/04_datawrangling_01.html#inspect-data",
    "title": "Transforming Ag data with dplyr",
    "section": "4.1 Inspect data",
    "text": "4.1 Inspect data\n\n4.1.1 glimpse\nThe glimpse() function provides an overview of the dataset, including variable names and data types.\n\n# Inspect the dataset\nglimpse(corn_data)\n\nRows: 3,443\nColumns: 9\n$ year  &lt;int&gt; 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   &lt;dbl&gt; -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  &lt;dbl&gt; -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield &lt;dbl&gt; 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro &lt;dbl&gt; 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    &lt;dbl&gt; 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    &lt;fct&gt; N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …\n\n\n\n\n4.1.2 skim\nThe skim() function from the ‘skimr’ allows to take a deeper look to all the variables (columns), creating a quick summary that reports the presence of missing values, etc., etc.\n\nskimr::skim(corn_data)\n\n\nData summary\n\n\nName\ncorn_data\n\n\nNumber of rows\n3443\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ntopo\n0\n1\nFALSE\n4\nW: 1043, LO: 885, HT: 785, E: 730\n\n\nrep\n0\n1\nFALSE\n3\nR3: 1149, R1: 1147, R2: 1147\n\n\nnf\n0\n1\nFALSE\n6\nN1: 577, N3: 575, N5: 575, N0: 573\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1999.99\n1.00\n1999.00\n1999.00\n1999.00\n2001.00\n2001.00\n▇▁▁▁▇\n\n\nlat\n0\n1\n-33.05\n0.00\n-33.05\n-33.05\n-33.05\n-33.05\n-33.05\n▃▇▆▃▁\n\n\nlong\n0\n1\n-63.85\n0.00\n-63.85\n-63.85\n-63.85\n-63.84\n-63.84\n▇▇▇▇▇\n\n\nyield\n0\n1\n69.83\n19.83\n12.66\n54.54\n66.63\n84.68\n117.90\n▁▅▇▃▃\n\n\nnitro\n0\n1\n64.57\n42.60\n0.00\n29.00\n66.00\n106.00\n131.50\n▅▇▇▂▇\n\n\nbv\n0\n1\n174.42\n9.68\n91.74\n168.48\n173.08\n179.39\n213.82\n▁▁▁▇▁",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#adding-new-variables-with-mutate",
    "href": "coding/week_03/04_datawrangling_01.html#adding-new-variables-with-mutate",
    "title": "Transforming Ag data with dplyr",
    "section": "4.2 Adding New Variables with mutate()",
    "text": "4.2 Adding New Variables with mutate()\nYou can add new columns to your dataset with mutate(). Let’s calculate the yield in tons per hectare (assuming the current yield is in kilograms):\n\n# Add a column for yield in tons\ncorn_data &lt;- corn_data %&gt;% \n  # New column, `yield_tons`, with the transformed yield values.\n  mutate(yield_tons = yield / 1000)\n\nhead(corn_data)\n\n  year       lat      long yield nitro topo     bv rep nf yield_tons\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5    0.07214\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5    0.07379\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5    0.07725\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5    0.07635\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5    0.07555\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5    0.07024",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#filtering-data",
    "href": "coding/week_03/04_datawrangling_01.html#filtering-data",
    "title": "Transforming Ag data with dplyr",
    "section": "4.3 Filtering Data",
    "text": "4.3 Filtering Data\nTo focus on specific data, we can use filter(). For example, let’s filter the data to include only rows where the nitrogen applied (nitro) is greater than 100:\n\n# Filter rows with nitro &gt; 100\nhigh_nitro &lt;- corn_data %&gt;%\n  # only rows where the `nitro` column has values greater than 100.\n  filter(nitro &gt; 100)\n\nhead(high_nitro)\n\n  year       lat      long yield nitro topo     bv rep nf yield_tons\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5    0.07214\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5    0.07379\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5    0.07725\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5    0.07635\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5    0.07555\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5    0.07024",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#selecting-columns",
    "href": "coding/week_03/04_datawrangling_01.html#selecting-columns",
    "title": "Transforming Ag data with dplyr",
    "section": "4.4 Selecting columns",
    "text": "4.4 Selecting columns\nTo select specific variables, we use select(), which selects specific columns from the dataset.\n\n# Select specific columns\nselected_data &lt;- corn_data %&gt;% select(yield, nitro, topo)\nhead(selected_data)\n\n  yield nitro topo\n1 72.14 131.5    W\n2 73.79 131.5    W\n3 77.25 131.5    W\n4 76.35 131.5    W\n5 75.55 131.5    W\n6 70.24 131.5    W",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#renaming-columns",
    "href": "coding/week_03/04_datawrangling_01.html#renaming-columns",
    "title": "Transforming Ag data with dplyr",
    "section": "4.5 Renaming columns",
    "text": "4.5 Renaming columns\nWhen we need to change names of columns, we can use rename():\n\n# Rename a column\nrenamed_data &lt;- corn_data %&gt;% rename(Nitrogen = nitro)\nhead(renamed_data)\n\n  year       lat      long yield Nitrogen topo     bv rep nf yield_tons\n1 1999 -33.05113 -63.84886 72.14    131.5    W 162.60  R1 N5    0.07214\n2 1999 -33.05115 -63.84879 73.79    131.5    W 170.49  R1 N5    0.07379\n3 1999 -33.05116 -63.84872 77.25    131.5    W 168.39  R1 N5    0.07725\n4 1999 -33.05117 -63.84865 76.35    131.5    W 176.68  R1 N5    0.07635\n5 1999 -33.05118 -63.84858 75.55    131.5    W 171.46  R1 N5    0.07555\n6 1999 -33.05120 -63.84851 70.24    131.5    W 170.56  R1 N5    0.07024",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#arranging-data",
    "href": "coding/week_03/04_datawrangling_01.html#arranging-data",
    "title": "Transforming Ag data with dplyr",
    "section": "4.6 Arranging data",
    "text": "4.6 Arranging data\nTo reorder the data based on specific criteria, we can use arrange(), which will arrange rows by a variable in ascending or descending order.\n\n# Arrange data by yield in descending order\narranged_data &lt;- corn_data %&gt;% arrange(desc(yield))\nhead(arranged_data)\n\n  year       lat      long  yield nitro topo     bv rep nf yield_tons\n1 2001 -33.05086 -63.84317 117.90  99.8   LO 162.17  R3 N4    0.11790\n2 2001 -33.05125 -63.84245 117.19 124.6   LO 165.81  R3 N5    0.11719\n3 2001 -33.05181 -63.84323 116.64 124.6   LO 159.75  R1 N5    0.11664\n4 2001 -33.05084 -63.84324 114.94  99.8   LO 166.27  R3 N4    0.11494\n5 2001 -33.05134 -63.84299 114.46  99.8   LO 164.58  R2 N4    0.11446\n6 2001 -33.05127 -63.84238 114.08 124.6   LO 170.94  R3 N5    0.11408",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#finding-unique-values",
    "href": "coding/week_03/04_datawrangling_01.html#finding-unique-values",
    "title": "Transforming Ag data with dplyr",
    "section": "4.7 Finding unique values",
    "text": "4.7 Finding unique values\nTo find out what are the unique values of a variable, we can use distinct(), which will return the unique values within a column.\n\n# Unique values in topo\nunique_topo &lt;- corn_data %&gt;% distinct(topo)\nunique_topo\n\n  topo\n1    W\n2   HT\n3    E\n4   LO",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#counting",
    "href": "coding/week_03/04_datawrangling_01.html#counting",
    "title": "Transforming Ag data with dplyr",
    "section": "4.8 Counting",
    "text": "4.8 Counting\nThe function count() counts the number of observations within a group.\n\n# Count observations by topo\ntopo_count &lt;- corn_data %&gt;% count(topo)\ntopo_count\n\n  topo    n\n1    E  730\n2   HT  785\n3   LO  885\n4    W 1043",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_03/04_datawrangling_01.html#summarizing-data",
    "href": "coding/week_03/04_datawrangling_01.html#summarizing-data",
    "title": "Transforming Ag data with dplyr",
    "section": "4.9 Summarizing Data",
    "text": "4.9 Summarizing Data\nTo get a quick overview of your data, you can use summarize() in combination with group_by(). For example, let’s calculate the average yield for each topographical category (topo):\n\n# Average yield by topography\naverage_yield_topo &lt;- corn_data %&gt;%\n  group_by(topo) %&gt;%\n  summarize(avg_yield = mean(yield, na.rm = TRUE))\naverage_yield_topo\n\n# A tibble: 4 × 2\n  topo  avg_yield\n  &lt;fct&gt;     &lt;dbl&gt;\n1 E          78.7\n2 HT         48.6\n3 LO         84.9\n4 W          66.8\n\n# Average yield by year and topography\naverage_yield_topoyear &lt;- corn_data %&gt;%\n  group_by(year, topo) %&gt;%\n  summarize(avg_yield = mean(yield, na.rm = TRUE))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\naverage_yield_topoyear\n\n# A tibble: 8 × 3\n# Groups:   year [2]\n   year topo  avg_yield\n  &lt;int&gt; &lt;fct&gt;     &lt;dbl&gt;\n1  1999 E          64.8\n2  1999 HT         53.4\n3  1999 LO         71.2\n4  1999 W          66.0\n5  2001 E          92.7\n6  2001 HT         44.7\n7  2001 LO         99.9\n8  2001 W          67.7\n\n\nThis groups the data by topo and calculates the mean yield for each group.",
    "crumbs": [
      "Lessons",
      "04-Data Wrangling I"
    ]
  },
  {
    "objectID": "coding/week_04/06_datawrangling_03.html",
    "href": "coding/week_04/06_datawrangling_03.html",
    "title": "Advanced Data Wrangling in R",
    "section": "",
    "text": "Description\nThis class dives deeper into the world of R data wrangling, covering advanced techniques and their applications. We’ll explore complex functions and join operations essential for real-world data analysis.\nRequired packages for today\nlibrary(pacman)\np_load(dplyr, tidyr, stringr, lubridate, janitor)",
    "crumbs": [
      "Lessons",
      "06-Data Wrangling III"
    ]
  },
  {
    "objectID": "coding/week_04/06_datawrangling_03.html#example-merging-weather-and-crop-yield-data",
    "href": "coding/week_04/06_datawrangling_03.html#example-merging-weather-and-crop-yield-data",
    "title": "Advanced Data Wrangling in R",
    "section": "2.1 Example: Merging Weather and Crop Yield Data",
    "text": "2.1 Example: Merging Weather and Crop Yield Data\n\nleft_join(): Includes all records from the left dataset and the matched records from the right dataset. If there is no match, the result is NA in the columns of the right dataset.\n\n\nweather_data &lt;- tibble(\n  date = seq(as.Date(\"2024-04-01\"), as.Date(\"2024-10-01\"), by = \"month\"),\n  precipitation = c(20, 40, 60, 80, 50, 30, 2),\n  temperature = c(15, 18, 25, 30, 22, 16, 12)\n)\n\nforage_data &lt;- tibble(\n  date = seq(as.Date(\"2024-04-01\"), as.Date(\"2024-10-01\"), by = \"month\"),\n  forage_yield = c(500, 1200, 3000, 4000, 2800, 1500, 0)\n)\n\n# Merge data\nleft_joined_data &lt;- left_join(weather_data, forage_data, by = \"date\")\n\n\nright_join(): Includes all records from the right dataset and the matched records from the left dataset. If there is no match, the result is NA in the columns of the left dataset.\n\n\n# Merge data\nright_joined_data &lt;- right_join(weather_data, forage_data, by = \"date\")\n\n\nfull_join(): Includes all records when there is a match in the keys of the left or right datasets. If there is no match, the result is NA in the columns of the dataset that does not have a match.\n\n\n# Merge data\nfull_joined_data &lt;- full_join(weather_data, forage_data, by = \"date\")",
    "crumbs": [
      "Lessons",
      "06-Data Wrangling III"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html",
    "href": "coding/week_10/models_05.html",
    "title": "Models IV: Mixed Models II",
    "section": "",
    "text": "Repeated measures models are a type of statistical model used when the same experimental unit is measured multiple times under different conditions or over time. These models account for the fact that repeated observations from the same unit (e.g., plant, plot, soil location) are not independent but correlated.\nIn agriculture, repeated measures designs are common in various scenarios, including:\n•   Time-based measurements: Tracking crop growth, soil nutrients, or yield over multiple time points. Or also treatments applied to the same unit over time such as fertilization trials where the same plots receive different treatments over multiple years.\n•   Spatially repeated measurements: Soil properties at different depths, plant characteristics at multiple canopy levels, or yield components across field zones.\n\n\nLinear mixed models (LMMs) are typically used for analyzing repeated measures because they allow us to include: 1. Fixed effects – Factors of interest, such as treatments, depths, or environmental conditions. 2. Random effects – Sources of variability, such as plot-to-plot differences or repeated observations on the same soil core.",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#mixed-models-for-repeated-measures",
    "href": "coding/week_10/models_05.html#mixed-models-for-repeated-measures",
    "title": "Models IV: Mixed Models II",
    "section": "",
    "text": "Linear mixed models (LMMs) are typically used for analyzing repeated measures because they allow us to include: 1. Fixed effects – Factors of interest, such as treatments, depths, or environmental conditions. 2. Random effects – Sources of variability, such as plot-to-plot differences or repeated observations on the same soil core.",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#required-packages-for-today",
    "href": "coding/week_10/models_05.html#required-packages-for-today",
    "title": "Models IV: Mixed Models II",
    "section": "2.1 Required packages for today",
    "text": "2.1 Required packages for today\n\nlibrary(pacman)\np_load(\"readxl\") # To open/save excel files\np_load('dplyr', \"tidyr\",\"purrr\", \"forcats\", \"stringr\") # Data wrangling\np_load(\"nlme\", \"lme4\") # Mixed models libraries\np_load(\"broom\", \"broom.mixed\") # Managing models results\np_load(\"performance\") # Check assumptions and performance\np_load(\"emmeans\",\"multcomp\",\"multcompView\",\n         \"car\", \"multilevelmod\") # Aov and mult comp\np_load(\"ggplot2\") # Figures\np_load(\"agricolae\") # Miscellaneous functions\np_load(\"agridat\") # For data",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#data",
    "href": "coding/week_10/models_05.html#data",
    "title": "Models IV: Mixed Models II",
    "section": "2.2 Data",
    "text": "2.2 Data\n\n# Read data\n# File online? Try this...\nurl_rm &lt;- \"https://raw.githubusercontent.com/adriancorrendo/tidymixedmodelsweb/refs/heads/master/data/02_repeated_measures_data.csv\"\n\n# Read file\nrm_data_00 &lt;- \n  read.csv(url_rm) \n\nrm_data_01 &lt;- \n  rm_data_00 %&gt;% \n  # We need to create PLOT column to identify subject (Exp. Unit for Rep. Measures)\n  ## Option 1, use \"unite\"\n  unite(data = ., col = PLOT, BLOCK,TREAT, sep = \"_\", remove=FALSE) %&gt;%\n  # OR\n  ## Option 2, use cur_group_id\n  # Identify Subplot\n  ungroup() %&gt;% \n  group_by(BLOCK, TREAT) %&gt;% # Don't need to add SITE here\n  # Create unique plot ID # Needed for Repeated Measures\n  mutate(plot = cur_group_id(), .after = PLOT) %&gt;% \n  ungroup() %&gt;% \n  ## Transform to factor if needed\n  mutate(depth = as.integer(DEPTH), # Needed for CorAR1\n         DEPTH = as.factor(DEPTH),\n         plot = factor(plot),\n         BLOCK = factor(BLOCK),\n         SITE = factor(SITE), \n                       #levels = c(\"site_1\", \"site_2\", \"site_3\", \"site_4\"), \n                       #labels = c(\"Balducchi\", \"San Alfredo\", \"La Blanca\", \"La Hansa\")\n         TREAT = factor(TREAT)\n         )",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#exploratory-analysis",
    "href": "coding/week_10/models_05.html#exploratory-analysis",
    "title": "Models IV: Mixed Models II",
    "section": "2.3 Exploratory analysis",
    "text": "2.3 Exploratory analysis\nNow, let’s use several functions to explore the data.\n\n2.3.1 glimpse()\nFirst, the glimpse() function from dplyr\n\n# Glimpse from dplyr\ndplyr::glimpse(rm_data_01)\n\nRows: 180\nColumns: 8\n$ SITE  &lt;fct&gt; site_1, site_1, site_1, site_1, site_1, site_1, site_1, site_1, …\n$ PLOT  &lt;chr&gt; \"I_Control\", \"I_Control\", \"I_Control\", \"I_Control\", \"I_Control\",…\n$ plot  &lt;fct&gt; 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 2, 2, 2, 2, 2, 5, 5…\n$ TREAT &lt;fct&gt; Control, Control, Control, Control, Control, Control, Control, C…\n$ BLOCK &lt;fct&gt; I, I, I, I, I, II, II, II, II, II, III, III, III, III, III, I, I…\n$ DEPTH &lt;fct&gt; 10, 30, 50, 70, 90, 10, 30, 50, 70, 90, 10, 30, 50, 70, 90, 10, …\n$ STK   &lt;int&gt; 105, 83, 103, 110, 127, 119, 98, 106, 107, 109, 132, 100, 96, 10…\n$ depth &lt;int&gt; 10, 30, 50, 70, 90, 10, 30, 50, 70, 90, 10, 30, 50, 70, 90, 10, …\n\n\n\n\n2.3.2 skim()\nThen, the skim() function from skmir\n\n# Skim from skimr\nskimr::skim(rm_data_01)\n\n\nData summary\n\n\nName\nrm_data_01\n\n\nNumber of rows\n180\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n5\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nPLOT\n0\n1\n5\n12\n0\n9\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSITE\n0\n1\nFALSE\n4\nsit: 45, sit: 45, sit: 45, sit: 45\n\n\nplot\n0\n1\nFALSE\n9\n1: 20, 2: 20, 3: 20, 4: 20\n\n\nTREAT\n0\n1\nFALSE\n3\nCon: 60, NPS: 60, Pri: 60\n\n\nBLOCK\n0\n1\nFALSE\n3\nI: 60, II: 60, III: 60\n\n\nDEPTH\n0\n1\nFALSE\n5\n10: 36, 30: 36, 50: 36, 70: 36\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSTK\n0\n1\n181.84\n63.09\n74\n138.75\n174\n221.25\n406\n▅▇▅▂▁\n\n\ndepth\n0\n1\n50.00\n28.36\n10\n30.00\n50\n70.00\n90\n▇▇▇▇▇\n\n\n\n\n\n\n\n2.3.3 ggplot()\nAnd let’s use ggplot2 for a better look\n\n# Boxplot\nrm_data_01 %&gt;% \n  dplyr::select(-depth) %&gt;% \n  # Plot\nggplot() + \n  # Boxplots\n  geom_boxplot(aes(x = reorder(DEPTH, desc(DEPTH)), y = STK, fill = TREAT))+\n  # Axis labels\n  labs(x = \"Soil depth (cm)\", y = \"STK (g/m2)\")+\n  # Plot by site\n  facet_wrap(~SITE)+\n  # Flip axes\n  coord_flip()+\n  # Set scale type\n  scale_x_discrete()+\n  # Change theme\n  tidybayes::theme_tidybayes()\n\n\n\n\n\n\n\n\n\n\n2.3.4 Additional data manipulation?\n\nrm_data_02 &lt;-rm_data_01 %&gt;% \n  # Create a grouping variable (WHY?) # Needed for HetVar\n  mutate(GROUP = case_when(TREAT == \"Pristine\" ~ \"Pristine\",\n                           TRUE ~ \"Agriculture\"))",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#candidate-models",
    "href": "coding/week_10/models_05.html#candidate-models",
    "title": "Models IV: Mixed Models II",
    "section": "2.4 Candidate Models",
    "text": "2.4 Candidate Models\nI’m sorry for this, but the most important step is ALWAYS to write down the model.\n\n2.4.1 Formulae\n\n2.4.1.1 m0. Block Fixed\nIn a traditional approach blocks are defined as fixed, affecting the mean of the expected value. Yet there is no consensus about treating blocks as fixed or as random. For more information, read Dixon (2016).\nLet’s define the model. For simplification (and avoid writing interaction terms), here we are going to consider that \\(\\tau_i\\) is the “treatment”.\n\\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\]\n\\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] where \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\n\n# SIMPLEST MODEL\nfit_block_fixed &lt;- function(x){\n  lm(# Response variable\n     STK ~ \n       # Fixed\n       TREAT + DEPTH + TREAT:DEPTH + BLOCK,\n     # Data\n     data = x)\n  }\n\n\n\n2.4.1.2 m1. Block Random\nAn alternative approach is considering a MIXED MODEL, where blocks are considered “random”. Basically, we add a term to the model that it is expected to show a “null” overall effect over the mean of the variable of interest but introduces “noise”. By convention, a random effect is expected to have an expected value equal to zero but a positive variance as follows: \\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\] \\[ \\beta_j \\sim N(0, \\sigma^2_{b} )\\] \\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] Similar than before, \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the “random” effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\nSo what’s the difference? Simply specifying this component: \\[ \\beta_j \\sim N(0, \\sigma^2_b) \\], which serves to model the variance.\nHow do we write that?\n\n# RANDOM BLOCK\nfit_block_random &lt;- function(x){\n  nlme::lme(\n    # Fixed\n    STK ~ TREAT + DEPTH + TREAT:DEPTH,\n    # Random\n    random = ~1|BLOCK,\n    # Data\n    data = x)\n  }\n\n\n\n\n2.4.2 Models w/ correlated ERRORS\nUntil here all sounds easy. However, we are (potentially) missing a key component. All measures involving DEPTH have been taken from the same “subjects” (experimental units/plots). So we do have “repeated measures” over space. Thus, it is highly likely that using depth implies the need to adjust the error correlation and covariance structure. Let’s explore some options…\n\n2.4.2.1 m2. m1 + CompSymm\nCompound symmetry is the simplest covariance structure, where we include a within-subject correlated errors. It is basically the same we do with including BLOCKS as random. We are telling the model that the observations within a given “depth” “share” something, they have something in common (the error).\n\n# RANDOM BLOCK w/compound symmetry error correlation structure\nfit_corsymm &lt;- function(x){\n  lme(# Response Variable\n      STK ~\n        # Fixed\n        TREAT + DEPTH + TREAT:DEPTH,\n        # Random\n        random = ~1|BLOCK,\n        # Identify subject where repeated measure happens\n        # Plots nested within blocks.\n        correlation = corCompSymm(form = ~ DEPTH |BLOCK/PLOT), \n     # Data   \n     data=x) }\n\n\n\n2.4.2.2 m3. m1 + CorAR1\nThe autoregressive of first order structure (CorAR1) considers correlations dependent of the “distance”. Thus, correlation of error is expected to be the highest between adjacent depths (e.g. 0-20 and 20-40 cm), and a systematically decrease with the distance. For example, the correlation between depth 1 and depth 2 would be \\(\\rho^{depth_2-depth_1}\\), and then less and less, ending with the correlation between depth 5 and depth 1 equal to \\(\\rho^{depth_5-depth_1}\\).\n\n\n\n\n\n\nCaution\n\n\n\nAn important detail here is that CorAR1 structure is only applicable for evenly spaced intervals!\n\n\n\n# RANDOM BLOCK w/ auto-regressive of 1st order as error correlation structure\nfit_ar1 &lt;- function(x){lme(STK ~ TREAT + DEPTH + TREAT:DEPTH,\n                       random = ~1|BLOCK,\n                       correlation=corAR1(form=~depth|BLOCK/PLOT),\n                       data=x)}\n\n\n\n2.4.2.3 m4. m2 + HetVar\nDid you know that we can “relax” the assumption about homogeneity of variance? Oftentimes we have data that shows different variability depending on the level of a given factor or variable.\nIn the STK dataset, we observed that the “Pristine” treatment (or agriculture condition) present a higher variability compared to Control and NPS treatments, probably linked to higher values of STK. Variance is modeled by adding a “weight”. This “weight” could be a function of a continuous variable (e.g. fertilizer rate?) or, like in our case, based on a “categorical” variable.\n\n# RANDOM BLOCK w/compound symmetry error correlation structure + Heterogeneous Variance\nfit_corsymm_hetvar &lt;- function(x){\n  lme(# Response variable\n      STK ~ \n        # Fixed\n        TREAT + DEPTH + TREAT:DEPTH,\n        # Random  \n        random = ~1|BLOCK,\n        # Correlation\n        correlation = corCompSymm(form = ~ DEPTH |BLOCK/PLOT),\n        # Variance\n        weights = varComb(varIdent(form= ~1|GROUP)),\n      # Data\n      data=x) }\n\n\n\n\n2.4.3 Fit\nRun the candidate models\n\nSTK_models &lt;- \n  rm_data_02 %&gt;% \n  # Let's group data to run multiple locations|datasets at once\n  group_by(SITE) %&gt;% \n  # Store the data per location using nested arrangement\n  nest() %&gt;% \n  # BLOCK as FIXED \n  mutate(model_0 = map(data, fit_block_fixed)) %&gt;% \n  # BLOCK as RANDOM\n  mutate(model_1 = map(data, fit_block_random)) %&gt;% \n  # COMPOUND SYMMETRY\n  mutate(model_2 = map(data, fit_corsymm)) %&gt;% \n  # AUTO-REGRESSIVE ORDER 1\n  mutate(model_3 = map(data, fit_ar1)) %&gt;% \n  # COMPOUND SYMMETRY + HETEROSKEDASTIC\n  mutate(model_4 = map(data,  fit_corsymm_hetvar) ) %&gt;%\n    \n  # Data wrangling\n  pivot_longer(cols = c(model_0:model_4), # show alternative 'contains' model\n               names_to = \"model_id\",\n               values_to = \"model\") %&gt;% \n  # Map over model column\n  mutate(results = map(model, broom.mixed::augment )) %&gt;% \n  # Performance\n  mutate(performance = map(model, broom.mixed::glance )) %&gt;% \n  # Extract AIC\n  mutate(AIC = map(performance, ~.x$AIC)) %&gt;% \n  # Extract coefficients\n  mutate(coef = map(model, ~coef(.x))) %&gt;% \n  # Visual-check plots\n  mutate(checks = map(model, ~performance::check_model(.))) %&gt;% \n  ungroup()\n\n\n\n2.4.4 Check assumptions\nChecking assumptions is always important. To learn more about data exploration, tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, among others, I highly recommend reading (Zuur, Ieno, and Elphick 2010).\n\n# Extracting by site\nsite_1_models &lt;- STK_models %&gt;% dplyr::filter(SITE == \"site_1\")\nsite_2_models &lt;- STK_models %&gt;% dplyr::filter(SITE == \"site_3\")\nsite_3_models &lt;- STK_models %&gt;% dplyr::filter(SITE == \"site_4\")\nsite_4_models &lt;- STK_models %&gt;% dplyr::filter(SITE == \"site_2\")\n\n\nSite 1Site 2Site 3Site 4\n\n\n\n(site_1_models %&gt;% dplyr::filter(model_id == \"model_0\"))$checks[[1]]\n\n\n\n\n\n\n\n\n\n\n\n(site_2_models %&gt;% dplyr::filter(model_id == \"model_0\"))$checks[[1]]\n\n\n\n\n\n\n\n\n\n\n\n(site_3_models %&gt;% dplyr::filter(model_id == \"model_0\"))$checks[[1]]\n\n\n\n\n\n\n\n\n\n\n\n(site_4_models %&gt;% dplyr::filter(model_id == \"model_0\"))$checks[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.5 Model Selection\nCompare models performance\n\n# Visual model selection\nbest_STK_models &lt;- \n  STK_models %&gt;% \n  group_by(SITE) %&gt;% \n  # Use case_when to identify the best model\n  mutate(best_model = \n           case_when(AIC == min(as.numeric(AIC)) ~ \"Yes\",\n                     TRUE ~ \"No\")) %&gt;% \n  ungroup()\n\n# Plot\nbest_STK_models %&gt;% \n  ggplot()+\n  geom_point(aes(x = model_id, y = as.numeric(AIC), \n                 color = best_model, shape = best_model), \n             size = 3)+\n  facet_wrap(~SITE)\n\n\n\n\n\n\n\n# Final models\nselected_models &lt;- best_STK_models %&gt;% dplyr::filter(best_model == \"Yes\")\n\n\n\n2.4.6 ANOVA\nEstimate the effects of factors under study (and their interaction)\n\nmodels_effects &lt;- \n  selected_models %&gt;%\n  # Type 3 Sum of Squares (Partial SS, when interactions are present)\n  mutate(ANOVA = map(model, ~Anova(., type = 3)) )\n\n# Extract ANOVAS\nmodels_effects$ANOVA[[1]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: STK\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 326.261  1  &lt; 2.2e-16 ***\nTREAT        39.994  2  2.067e-09 ***\nDEPTH        12.637  4   0.013191 *  \nTREAT:DEPTH  21.827  8   0.005247 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[2]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: STK\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 794.721  1  &lt; 2.2e-16 ***\nTREAT        21.394  2  2.261e-05 ***\nDEPTH        67.224  4  8.743e-14 ***\nTREAT:DEPTH  52.957  8  1.099e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[3]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: STK\n             Chisq Df Pr(&gt;Chisq)    \n(Intercept) 84.604  1  &lt; 2.2e-16 ***\nTREAT       50.791  2  9.353e-12 ***\nDEPTH       42.200  4  1.516e-08 ***\nTREAT:DEPTH 21.773  8   0.005355 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[4]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: STK\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 876.934  1  &lt; 2.2e-16 ***\nTREAT       262.921  2  &lt; 2.2e-16 ***\nDEPTH        22.364  4  0.0001696 ***\nTREAT:DEPTH 107.841  8  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#means-comparison",
    "href": "coding/week_10/models_05.html#means-comparison",
    "title": "Models IV: Mixed Models II",
    "section": "2.5 Means comparison",
    "text": "2.5 Means comparison\n\n# MULTCOMPARISON\n# emmeans and cld multcomp\n# We need to specify ourselves the most important interaction to perform the comparisons\nmult_comp &lt;- \n  models_effects %&gt;% \n  # Comparisons estimates (emmeans)\n  mutate(mc_estimates = map(model, ~emmeans(., ~ TREAT*DEPTH))) %&gt;% \n  # Assign letters and p-value adjustment (multcomp)\n  mutate(mc_letters = \n           map(mc_estimates, \n               ~as.data.frame( \n                 # By specifies a strata or level to assign the letters\n                 cld(., by = \"DEPTH\", decreasing = TRUE, details=FALSE,\n                     reversed=TRUE, alpha=0.05,  adjust = \"tukey\", Letters=LETTERS))))",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_10/models_05.html#plot",
    "href": "coding/week_10/models_05.html#plot",
    "title": "Models IV: Mixed Models II",
    "section": "2.6 Plot",
    "text": "2.6 Plot\nNow, we are going to reproduce Figure 2\n\n# Create data frame for plot\nplot_df &lt;- mult_comp %&gt;% \n  dplyr::select(SITE, mc_letters) %&gt;% \n  unnest(mc_letters)\n\n# Define your own colors\nmy_colors &lt;- c(\"#ffaa00\", \"#7E5AA0\", \"#5c9c8c\")\n\n# Create the plot\nSTK_plot &lt;-\n  plot_df %&gt;% \n  # We need to re-express DEPTH from factor to character, and then to numeric\n  mutate(DEPTH = as.numeric(as.character(DEPTH)))  %&gt;% \n  # Re-order levels of the factors\n  mutate(TREAT = fct_relevel(TREAT,\"Control\", \"NPS\", \"Pristine\")) %&gt;% \n  mutate(SITE = fct_relevel(SITE,\"site_1\", \"site_2\", \"site_3\", \"site_4\")) %&gt;% \n  # Create plot\n  ggplot()+\n  # 01. LAYOUT\n  ## Subplots\n  facet_wrap(~SITE, nrow = 2)+\n  ## Axis titles\n  labs(x = \"Soil depth (cm)\", y = bquote(~NH[4]*'OAc-K (g' ~m^-2*')'))+\n  # 02. GEOMETRIES\n  ## i. Points\n  geom_point(aes(x = DEPTH, y = emmean,\n                 fill= TREAT,\n                 shape = TREAT),\n             size = 3, col = \"black\")+\n  ## Adjust shape aesthetics\n  scale_shape_manual(name=\"Fertilizer Scenario\", values=c(24,23,21),\n                     guide=\"legend\")+\n  scale_colour_manual(name=\"Fertilizer Scenario\",\n                    values = my_colors,\n                    guide='legend')+\n  scale_fill_manual(name=\"Fertilizer Scenario\",\n                    values = my_colors,\n                    guide='legend')+\n  ## ii. Add error bar\n  geom_errorbar(width = 0.25, aes(x = DEPTH, color = TREAT, \n                                 ymin = emmean-2*SE, ymax = emmean+2*SE))+\n  ## iii. Add line\n  geom_line(size = 0.5,aes(x = DEPTH, y = emmean, color = TREAT))+\n  # 03. ADJUST XY AXIS\n  ## Reverse the scale\n  scale_x_reverse(breaks=seq(0, 100, 20), limits = c(100,0))+\n  coord_flip()+\n  # 04. THEME\n  theme_bw()+\n  theme(strip.text = element_text(size = rel(1.25)),\n        strip.background = element_blank(),\n        # Grid\n        panel.grid = element_blank(),\n        # Axis\n        axis.title = element_text(size = rel(1.5)),\n        axis.text = element_text(size = rel(1.25), color = \"black\"),\n        # Legend\n        legend.position = \"top\", legend.title = element_blank(),\n        legend.text = element_text(size = rel(1.25))        )\n\nSTK_plot\n\n\n\n\n\n\n\n\n\n2.6.1 Figure with caption\n\n\nSTK_plot\n\n\n\n\n\n\n\n\nFigure 2. Soil profiles of STK (\\(g~m^{-2}\\)) under three different conditions: pristine soils (green circles), under grain cropping from 2000 to 2009 with no fertilizers added (Control, orange triangles), and under grain cropping from 2000 to 2009 with N, P, plus S fertilization (NPS, purple diamonds). Overlapping error bars indicate absence of significant differences between scenarios by soil depths combinations (Tukey’s HSD, p &lt; 0.05).",
    "crumbs": [
      "Lessons",
      "16-Models V"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html",
    "href": "coding/week_11/models_07.html",
    "title": "Classification models in R",
    "section": "",
    "text": "This document demonstrates how to apply Logistic Regression James et al. (2013) and tree-based models Breiman (2001) using the tidyverse approach in R. We use the Dry Bean Dataset from the UCI Machine Learning Repository.\nPackages required for today",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#logistic-regression",
    "href": "coding/week_11/models_07.html#logistic-regression",
    "title": "Classification models in R",
    "section": "2.1 Logistic regression",
    "text": "2.1 Logistic regression\nWe will start with a simple example using the Iris dataset, where we will create a binary classification problem.\n\n2.1.1 logistic_reg() from tidymodels\nWe will filter the dataset to only include two species (setosa and versicolor) and train a logistic regression model.\n\n# Load iris dataset\niris_binary &lt;- iris %&gt;%\n  filter(Species != \"versicolor\") %&gt;%  # Keep only two classes\n  mutate(Species = as.factor(Species))\n\n# Split the data\nset.seed(42)\niris_split &lt;- initial_split(iris_binary, prop = 0.8, strata = Species)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\n# Train logistic regression using tidymodels\niris_log_model &lt;- \n  logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\niris_log_wf &lt;- workflow() %&gt;%\n  add_model(iris_log_model) %&gt;%\n  add_formula(Species ~ .)\n\niris_log_fit &lt;- iris_log_wf %&gt;% fit(data = iris_train)\n\n# Evaluate model\niris_predictions &lt;- predict(iris_log_fit, iris_test, type = \"class\") %&gt;%\n  bind_cols(iris_test)\n\niris_metrics &lt;- iris_predictions %&gt;%\n  metrics(truth = Species, estimate = .pred_class)\n\niris_metrics\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.5  \n2 kap      multiclass     0.333\n\n\n\n\n2.1.2 glm() from base R\nAlternatively, we can use the more classic approach with glm().\n\n# Train logistic regression using glm\niris_glm &lt;- glm(Species ~ ., data = iris_train, family = binomial)\n\n# Predict on test data\niris_glm_probs &lt;- predict(iris_glm, iris_test, type = \"response\")\n\n# Ensure the response variable is a factor with two levels\niris_test &lt;- iris_test %&gt;% \n  mutate(Species = factor(Species, levels = c(\"setosa\", \"virginica\")))\n\n# Plot ROC Curve\nroc_curve_glm &lt;- roc(response = iris_test$Species, predictor = iris_glm_probs, levels = rev(levels(iris_test$Species)))\nplot(roc_curve_glm, col = \"blue\", main = \"ROC Curve for GLM Logistic Regression\")\n\n\n\n\n\n\n\nauc(roc_curve_glm)\n\nArea under the curve: 1",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#confusion-matrix",
    "href": "coding/week_11/models_07.html#confusion-matrix",
    "title": "Classification models in R",
    "section": "3.1 Confusion Matrix",
    "text": "3.1 Confusion Matrix\nIn machine learning and statistics, a confusion matrix is used to evaluate the performance of a classification model. It compares predicted labels against actual ground truth labels.\nBelow is a conceptual confusion matrix explaining True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN):\n\n\n\n\n\n\n\n\nActual / Predicted\nPositive Prediction\nNegative Prediction\n\n\n\n\nActual Positive\nTrue Positive (TP): Correctly predicted positive\nFalse Negative (FN): Incorrectly predicted as negative\n\n\nActual Negative\nFalse Positive (FP): Incorrectly predicted as positive\nTrue Negative (TN): Correctly predicted negative",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#explanation-of-terms",
    "href": "coding/week_11/models_07.html#explanation-of-terms",
    "title": "Classification models in R",
    "section": "3.2 Explanation of Terms:",
    "text": "3.2 Explanation of Terms:\n\nTrue Positive (TP): Model correctly predicts a positive instance.\nFalse Positive (FP): Model incorrectly predicts a negative instance as positive (Type I error).\nTrue Negative (TN): Model correctly predicts a negative instance.\nFalse Negative (FN): Model incorrectly predicts a positive instance as negative (Type II error).\n\nThis matrix helps in evaluating performance metrics such as accuracy, precision, recall, and F1-score.",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#classification-metrics",
    "href": "coding/week_11/models_07.html#classification-metrics",
    "title": "Classification models in R",
    "section": "3.3 Classification Metrics",
    "text": "3.3 Classification Metrics\n\n3.3.1 Accuracy\nAccuracy is one of the most popular metrics that measures the proportion of correctly classified cases:\n\\[\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\] - A value of 0.50 suggests the model is predicting correctly only half the time, which is just slightly better than random guessing. - Higher accuracy (closer to 1.00) is desirable, but accuracy alone doesn’t always reflect the model’s performance, especially in imbalanced datasets.\n\n\n3.3.2 Kappa Statistic\nKappa (Cohen’s Kappa) measures how well the model performs relative to random chance:\n\\[\\kappa = \\frac{p_o - p_e}{1 - p_e}\\]\nWhere: \\(p_o\\) is the observed accuracy, \\(p_e\\) is the expected accuracy due to chance.\nInterpretation of Kappa Values: • Kappa = 1.00 → Perfect agreement. • Kappa &gt; 0.80 → Strong agreement. • Kappa between 0.40 and 0.60 → Moderate agreement. • Kappa &lt; 0.40 → Poor agreement. • Kappa = 0 → The model is no better than random chance.\nIn our Iris example, a Kappa of 0.333 suggests that while the model performs better than random, it still struggles to distinguish between the classes.",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#roc-curve-auc-area-under-the-curve",
    "href": "coding/week_11/models_07.html#roc-curve-auc-area-under-the-curve",
    "title": "Classification models in R",
    "section": "3.4 ROC Curve & AUC (Area Under the Curve)",
    "text": "3.4 ROC Curve & AUC (Area Under the Curve)\nThe Receiver Operating Characteristic (ROC) curve shows the trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity).\nInterpretation of AUC Values: - AUC = 1.00 → Perfect classification. - AUC &gt; 0.90 → Excellent discrimination. - AUC between 0.80 - 0.90 → Good classification. - AUC between 0.70 - 0.80 → Fair classification. - AUC = 0.50 → Random guessing.\nIn our Iris example, an AUC of 1.00 means that the model separates the two species perfectly. However, this should be interpreted alongside accuracy and kappa to ensure it’s not due to overfitting.\nIf accuracy is low (e.g., 50%) but AUC is high (1.00), this could indicate a problem in threshold selection or that the dataset is too simple for the model.\nThis example shows both the modern tidymodels approach and the classic glm() approach for binary classification.",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#dry-beans-dataset",
    "href": "coding/week_11/models_07.html#dry-beans-dataset",
    "title": "Classification models in R",
    "section": "4.1 Dry beans dataset",
    "text": "4.1 Dry beans dataset\n\n#url &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip\"\n#download.file(url, \"DryBeanDataset.zip\")\n#unzip(\"DryBeanDataset.zip\", exdir = \"./DryBeanDataset\")\n\ndf &lt;- readxl::read_xlsx(path=\"data/Dry_Bean_Dataset.xlsx\")\n\nLets take a look at the structure and summary of the dataset.\n\nglimpse(df)\n\nRows: 13,611\nColumns: 17\n$ Area            &lt;dbl&gt; 28395, 28734, 29380, 30008, 30140, 30279, 30477, 30519…\n$ Perimeter       &lt;dbl&gt; 610.291, 638.018, 624.110, 645.884, 620.134, 634.927, …\n$ MajorAxisLength &lt;dbl&gt; 208.1781, 200.5248, 212.8261, 210.5580, 201.8479, 212.…\n$ MinorAxisLength &lt;dbl&gt; 173.8887, 182.7344, 175.9311, 182.5165, 190.2793, 181.…\n$ AspectRation    &lt;dbl&gt; 1.197191, 1.097356, 1.209713, 1.153638, 1.060798, 1.17…\n$ Eccentricity    &lt;dbl&gt; 0.5498122, 0.4117853, 0.5627273, 0.4986160, 0.3336797,…\n$ ConvexArea      &lt;dbl&gt; 28715, 29172, 29690, 30724, 30417, 30600, 30970, 30847…\n$ EquivDiameter   &lt;dbl&gt; 190.1411, 191.2728, 193.4109, 195.4671, 195.8965, 196.…\n$ Extent          &lt;dbl&gt; 0.7639225, 0.7839681, 0.7781132, 0.7826813, 0.7730980,…\n$ Solidity        &lt;dbl&gt; 0.9888560, 0.9849856, 0.9895588, 0.9766957, 0.9908933,…\n$ roundness       &lt;dbl&gt; 0.9580271, 0.8870336, 0.9478495, 0.9039364, 0.9848771,…\n$ Compactness     &lt;dbl&gt; 0.9133578, 0.9538608, 0.9087742, 0.9283288, 0.9705155,…\n$ ShapeFactor1    &lt;dbl&gt; 0.007331506, 0.006978659, 0.007243912, 0.007016729, 0.…\n$ ShapeFactor2    &lt;dbl&gt; 0.003147289, 0.003563624, 0.003047733, 0.003214562, 0.…\n$ ShapeFactor3    &lt;dbl&gt; 0.8342224, 0.9098505, 0.8258706, 0.8617944, 0.9419004,…\n$ ShapeFactor4    &lt;dbl&gt; 0.9987239, 0.9984303, 0.9990661, 0.9941988, 0.9991661,…\n$ Class           &lt;chr&gt; \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", \"SEKER\", …\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n13611\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nClass\n0\n1\n4\n8\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nArea\n0\n1\n53048.28\n29324.10\n20420.00\n36328.00\n44652.00\n61332.00\n254616.00\n▇▂▁▁▁\n\n\nPerimeter\n0\n1\n855.28\n214.29\n524.74\n703.52\n794.94\n977.21\n1985.37\n▇▆▁▁▁\n\n\nMajorAxisLength\n0\n1\n320.14\n85.69\n183.60\n253.30\n296.88\n376.50\n738.86\n▇▆▂▁▁\n\n\nMinorAxisLength\n0\n1\n202.27\n44.97\n122.51\n175.85\n192.43\n217.03\n460.20\n▇▇▁▁▁\n\n\nAspectRation\n0\n1\n1.58\n0.25\n1.02\n1.43\n1.55\n1.71\n2.43\n▂▇▅▂▁\n\n\nEccentricity\n0\n1\n0.75\n0.09\n0.22\n0.72\n0.76\n0.81\n0.91\n▁▁▂▇▇\n\n\nConvexArea\n0\n1\n53768.20\n29774.92\n20684.00\n36714.50\n45178.00\n62294.00\n263261.00\n▇▂▁▁▁\n\n\nEquivDiameter\n0\n1\n253.06\n59.18\n161.24\n215.07\n238.44\n279.45\n569.37\n▇▆▁▁▁\n\n\nExtent\n0\n1\n0.75\n0.05\n0.56\n0.72\n0.76\n0.79\n0.87\n▁▁▅▇▂\n\n\nSolidity\n0\n1\n0.99\n0.00\n0.92\n0.99\n0.99\n0.99\n0.99\n▁▁▁▁▇\n\n\nroundness\n0\n1\n0.87\n0.06\n0.49\n0.83\n0.88\n0.92\n0.99\n▁▁▂▇▇\n\n\nCompactness\n0\n1\n0.80\n0.06\n0.64\n0.76\n0.80\n0.83\n0.99\n▂▅▇▂▁\n\n\nShapeFactor1\n0\n1\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.01\n▁▃▇▃▁\n\n\nShapeFactor2\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▇▇▇▃▁\n\n\nShapeFactor3\n0\n1\n0.64\n0.10\n0.41\n0.58\n0.64\n0.70\n0.97\n▂▇▇▃▁\n\n\nShapeFactor4\n0\n1\n1.00\n0.00\n0.95\n0.99\n1.00\n1.00\n1.00\n▁▁▁▁▇",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#data-preprocessing",
    "href": "coding/week_11/models_07.html#data-preprocessing",
    "title": "Classification models in R",
    "section": "4.2 Data Preprocessing",
    "text": "4.2 Data Preprocessing\nWe handle class imbalance using SMOTE and ensure the target variable is a factor.\n\ndf &lt;- df %&gt;% \n  mutate(Class = as.factor(Class)) %&gt;%\n  recipe(Class ~ .) %&gt;%\n  step_smote(Class) %&gt;%\n  prep() %&gt;%\n  juice()\n\n\n4.2.1 Training and Testing Split\n\nset.seed(42)\ndata_split &lt;- initial_split(df, prop = 0.8, strata = Class)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#logistic-regression-model",
    "href": "coding/week_11/models_07.html#logistic-regression-model",
    "title": "Classification models in R",
    "section": "4.3 Logistic Regression Model",
    "text": "4.3 Logistic Regression Model\n\n# First, fit a standard logistic regression model\nlog_model_simple &lt;- logistic_reg() %&gt;%  # Logistic regression (binary only)\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nlog_wf_simple &lt;- workflow() %&gt;%\n  add_model(log_model_simple) %&gt;%\n  add_formula(Class ~ .)\n\nlog_fit_simple &lt;- log_wf_simple %&gt;% fit(data = train_data)\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning: Be cautious because logistic regression is only suitable for binary classification. Since we have 7 classes, a multinomial regression is more appropriate.",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#multinomial-logistic-regression-model",
    "href": "coding/week_11/models_07.html#multinomial-logistic-regression-model",
    "title": "Classification models in R",
    "section": "4.4 Multinomial Logistic Regression Model",
    "text": "4.4 Multinomial Logistic Regression Model\n\nlog_model_multinom &lt;- multinom_reg() %&gt;%  # Multinomial logistic regression\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\nlog_wf_multinom &lt;- workflow() %&gt;%\n  add_model(log_model_multinom) %&gt;%\n  add_formula(Class ~ .)\n\nlog_fit_multinom &lt;- log_wf_multinom %&gt;% fit(data = train_data)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#classification-tree-model",
    "href": "coding/week_11/models_07.html#classification-tree-model",
    "title": "Classification models in R",
    "section": "4.5 Classification Tree Model",
    "text": "4.5 Classification Tree Model\nA classification tree is a single decision tree that splits the data based on feature values to classify observations. It follows these steps:\n\nSplits data at each node based on a feature that minimizes impurity (e.g., Gini index, entropy).\nForms a tree structure where each leaf represents a predicted class.\nProne to overfitting, as it may capture noise in the training data.\n\n\n\n\n\n\n\nTip\n\n\n\nTip: use CART when you need interpretability, and quick decisions and you don’t care about overfitting.\n\n\n\ntree_model &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_wf &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_formula(Class ~ .)\n\ntree_fit &lt;- tree_wf %&gt;% fit(data = train_data)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#random-forest-model",
    "href": "coding/week_11/models_07.html#random-forest-model",
    "title": "Classification models in R",
    "section": "4.6 Random Forest Model",
    "text": "4.6 Random Forest Model\nA random forest is an ensemble method that builds multiple decision trees and combines their outputs. It works as follows:\n\nRandomly samples data (with replacement) to create different training sets (bootstrap sampling).\nConstructs multiple decision trees using subsets of features at each split.\nAggregates the predictions from all trees (majority vote for classification, average for regression).\nReduces overfitting by averaging across trees, improving generalization.\n\n\n\n\n\n\n\nTip\n\n\n\nTip: use random forest when you need higher accuracy and robustness to overfitting.\n\n\n\n# Train a Random Forest model\nrf_model &lt;- rand_forest(mode = \"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_formula(Class ~ .)\n\nrf_fit &lt;- rf_wf %&gt;% fit(data = train_data)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#model-evaluation-1",
    "href": "coding/week_11/models_07.html#model-evaluation-1",
    "title": "Classification models in R",
    "section": "4.7 Model Evaluation",
    "text": "4.7 Model Evaluation\nWe evaluate the models using accuracy, ROC curves, and other metrics.\n\n4.7.1 Understanding ROC Curves\nThe Receiver Operating Characteristic (ROC) curve is a graphical representation of a classification model’s performance. It shows the trade-off between sensitivity (True Positive Rate) and 1 - specificity (False Positive Rate) across different threshold values. A higher area under the ROC curve (AUC) indicates better model performance.\nIn this case, since we have multiple classes, we use a One-vs-All (OvA) approach, where we compute an ROC curve for each class, treating it as a binary classification problem.\nWe evaluate the models using accuracy, ROC curves, and other metrics.\nWe evaluate the models using accuracy and other metrics.\n\n# Logistic regression\nlog_results &lt;- predict(log_fit_simple, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  metrics(truth = Class, estimate = .pred_class)\n\n# Multinomial logistic\nmultlog_results &lt;- predict(log_fit_multinom, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  metrics(truth = Class, estimate = .pred_class)\n\ntree_results &lt;- predict(tree_fit, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  metrics(truth = Class, estimate = .pred_class)\n# Random forest\nrf_results &lt;- predict(rf_fit, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  metrics(truth = Class, estimate = .pred_class)\n\n\n\n4.7.2 Logistic Regression Results\n\nlog_results\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.278\n2 kap      multiclass     0.158\n\n# Generate ROC Curve for multinomial regression using One-vs-All approach\n\n# Convert prediction probabilities to wide format\ndata_roc &lt;- predict(log_fit_simple, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data)\n\n# Compute ROC curves for each class\n# roc_data &lt;- data_roc %&gt;%\n#   roc_curve(truth = Class, \n#             !!!syms(names(select(data_roc, starts_with(\".pred_\")))))\n\n# Plot ROC curves\n# autoplot(roc_data)\n\n\n\n4.7.3 Logistic Regression Results\n\nmultlog_results\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.948\n2 kap      multiclass     0.940\n\n# Generate ROC Curve for multinomial regression using One-vs-All approach\n\n# Convert prediction probabilities to wide format\ndata_roc &lt;- predict(log_fit_multinom, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data)\n\n# Compute ROC curves for each class\nroc_data &lt;- data_roc %&gt;%\n  roc_curve(truth = Class, \n            !!!syms(names(select(data_roc, starts_with(\".pred_\")))))\n\n# Plot ROC curves\nautoplot(roc_data)\n\n\n\n\n\n\n\n\n\n\n4.7.4 Decision Tree Results\n\ntree_results\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.896\n2 kap      multiclass     0.878\n\n\n\n\n4.7.5 Random Forest Results\n\nrf_results\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.956\n2 kap      multiclass     0.949\n\n# Compute ROC Curve for Random Forest\nrf_roc &lt;- predict(rf_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_curve(truth = Class, !!!syms(names(select(., starts_with(\".pred_\")))))\n\nautoplot(rf_roc)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#visualizing-decision-tree",
    "href": "coding/week_11/models_07.html#visualizing-decision-tree",
    "title": "Classification models in R",
    "section": "4.8 7. Visualizing Decision Tree",
    "text": "4.8 7. Visualizing Decision Tree\n\n# Extract the trained model from the workflow\ntree_model_extracted &lt;- extract_fit_parsnip(tree_fit)\n\n# Plot the decision tree\nrpart.plot(tree_model_extracted$fit)",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "coding/week_11/models_07.html#conclusion",
    "href": "coding/week_11/models_07.html#conclusion",
    "title": "Classification models in R",
    "section": "4.9 Conclusion",
    "text": "4.9 Conclusion\nThis tutorial demonstrates how to apply logistic regression and decision tree classification using the tidyverse approach in R. We also handled class imbalance with SMOTE and evaluated models based on accuracy.",
    "crumbs": [
      "Lessons",
      "18-Models VII"
    ]
  },
  {
    "objectID": "outline/outline.html",
    "href": "outline/outline.html",
    "title": "PLNT6800|01 - Course Outline",
    "section": "",
    "text": "Course Code: PLNT6800\nCourse Title: Reproducible Ag Data Science with R\nTerm: Winter\nCredits: 0.50\n\n\n\n\nDays: Wednesdays and Fridays\nTime: 1:00 pm - 2:20 pm\nLocation: CRSC 202\n\n\n\n\n\nDr. Adrian Correndo\nEmail: acorrend@uoguelph.ca",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#course-information",
    "href": "outline/outline.html#course-information",
    "title": "PLNT6800|01 - Course Outline",
    "section": "",
    "text": "Course Code: PLNT6800\nCourse Title: Reproducible Ag Data Science with R\nTerm: Winter\nCredits: 0.50\n\n\n\n\nDays: Wednesdays and Fridays\nTime: 1:00 pm - 2:20 pm\nLocation: CRSC 202\n\n\n\n\n\nDr. Adrian Correndo\nEmail: acorrend@uoguelph.ca",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#course-description",
    "href": "outline/outline.html#course-description",
    "title": "PLNT6800|01 - Course Outline",
    "section": "2. Course Description",
    "text": "2. Course Description\nReproducible Ag Data Science with R is designed for graduate students in crop and soil sciences to develop key skills in data science using R. This course emphasizes reproducibility in data analysis, ensuring that results can be consistently replicated. Students will learn essential data science concepts, and how to use functions, packages, and version control to effectively manage their data and collaborate with peers. Following tidy principles, the course promotes best coding practices for data wrangling, effective visualization, and clean deployment of statistical models common in agriculture. By the end of the course, students will be equipped to handle a variety of agricultural datasets and produce reliable, reproducible research outcomes.\n\nPrerequisite(s)\nA basic understanding of R or any programming language is recommended but not required. Basic statistical theory is also recommended.\n\n\nTextbooks and Resources\nRecommended:\n\nR for Data Science by Hadley Wickham & Garrett Grolemund.\nOnline resources and package documentation will be provided throughout the course.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#course-learning-outcomes",
    "href": "outline/outline.html#course-learning-outcomes",
    "title": "PLNT6800|01 - Course Outline",
    "section": "3. Course Learning Outcomes",
    "text": "3. Course Learning Outcomes\nBy the end of this course, students will be able to:\n\nUnderstand and apply the principles of reproducible research in data science.\nUse version control tools like GitHub for managing code and collaborative projects.\nDevelop proficiency in R, including data wrangling, data visualization, and the use of relevant packages for agricultural datasets.\nApply statistical models to agricultural data and interpret the results.\nProduce professional reports using RMarkdown and Quarto, ensuring reproducibility and clarity.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#calendar",
    "href": "outline/outline.html#calendar",
    "title": "PLNT6800|01 - Course Outline",
    "section": "4. Calendar",
    "text": "4. Calendar\nSee http://adriancorrendo.github.io/plnt6800/calendar.html\n\nLast Day to Drop Course\nTBD",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#assessment-breakdown",
    "href": "outline/outline.html#assessment-breakdown",
    "title": "PLNT6800|01 - Course Outline",
    "section": "5. Assessment Breakdown",
    "text": "5. Assessment Breakdown\n\n\n\n\n\n\n\n\nComponent\nWeight (%)\nDetails\n\n\n\n\nWeekly Exercises\n30%\nHands-on exercises to practice skills covered in each week’s topic.\n\n\nSemester Project\n50%\nComplete data analysis project, report, and presentation.\n\n\nFinal Exam\n20%\nCumulative assessment covering all topics from the course.\n\n\n\n\nFinal Exam\n\nDate: Apr 11, 11.59 pm.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#course-grading-policies",
    "href": "outline/outline.html#course-grading-policies",
    "title": "PLNT6800|01 - Course Outline",
    "section": "6. Course Grading Policies",
    "text": "6. Course Grading Policies\n\na. Late Submissions\nAssignments submitted late will be penalized 5% per day, up to six days. Extensions granted only for valid reasons.\n\n\nb. Use of Devices\nElectronic recording of classes is forbidden without prior permission from the instructor.\n\n\nc. Academic honesty\nPlease adhere to the following guidelines when working on assignments for this course:\n\nIndividual and Team Assignments: You are welcome to discuss individual homework and lab assignments with other students; however, direct sharing or copying of code or written work is not permitted. For team assignments, collaboration is allowed freely within your team. Sharing or copying code or written content between teams is prohibited. Any unauthorized sharing or copying will be treated as a violation for all parties involved.\nExams: Collaboration or discussion with others during exams is strictly prohibited. Unauthorized collaboration or use of unapproved materials will be considered a violation for all students involved.\nReusing Code: Unless specified otherwise, you may refer to online resources (e.g., StackOverflow) for coding examples in assignments. If you use code from an external source directly or take inspiration from it, you must clearly cite the source. The use of AI to complete tasks is not prohibited but it must be disclosed. Failure to cite reused code will be considered plagiarism.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#course-statements",
    "href": "outline/outline.html#course-statements",
    "title": "PLNT6800|01 - Course Outline",
    "section": "7. Course Statements",
    "text": "7. Course Statements\n\nA. Communication with instructor\nDuring the course, your instructor will interact with you on various course matters on the course website using the following ways of communication:\n\nAnnouncements: The instructor will use Announcements on the Course Home page to provide you with course reminders and updates. Pleasecheck this section frequently for course updates from your instructor.\nEmail: If you have a conflict that prevents you from completing course requirements, or have a question concerning a personal matter, you cansend your instructor a private message by email. The instructor will attempt to respond to your email within 24 hours.\nVideo Call: If you have a complex question you would like to discuss with your instructor, you may book a video meeting on Zoom or Teams. Video meetings depend on the availability and are booked on a first come first served basis.\n\n\n\nB. Course Technology Requirements\nThis course will use a variety of technologies and resources. To successfully participate in and complete this course, students will need access to the following\n\n1. Communication tools:\nCourseLink. This platform will be used as the main Course-Home Page. If you need any assistance with the software tools or the CourseLink website, contact CourseLink Support. Email: courselink@uoguelph.ca Tel: 519-824-4120 ext. 56939 Toll-Free (CAN/USA): 1-866- 275-1478. Support Hours (Eastern Time): Monday thru Friday: 8:30 am–8:30 pm; Saturday: 10:00 am–4:00 pm; Sunday: 12:00 pm–6:00 pm\nZoom. This course will use Zoom for lectures when in-person class is not possible. Check your system requirements to ensure you will be able to participate (https://opened.uoguelph.ca/student-resources/system-and-software-requirements/). A Zoom link for the class will be provided before the first day of class. Please, check Home-Page and announcements on CourseLink, and emails from the instructor (acorrend@uoguelph.ca).\n\n\n2. Software & Tools:\n\nR (latest stable version, available at CRAN).\nRStudio/Posit IDE (desktop or cloud-based version for writing and running R code).\nCourse-Specific Libraries and Packages: Students will be required to install R packages. Detailed instructions will be provided in class.\nVersion Control and Collaboration Tools: Git (for version control) and a free GitHub account for collaborative project work and sharing code.\n\n\n\n3. Computing Requirements:\nA laptop or desktop computer capable of running R and RStudio (Windows, MacOS, or Linux). Minimum specifications include:\n\nProcessor: At least a dual-core processor.\nRAM: 8 GB or more (16 GB recommended for handling larger datasets).\nStorage: 10 GB of free space for software installation, course files, and datasets.\n\n\n\n4. Internet Access:\nReliable high-speed internet for accessing online sessions, resources, downloading software, and using cloud-based platforms (e.g., Posit Cloud, GitHub).\n\n\n\n\nC. Data Usage Policy for the Semester Project\nStudents are encouraged to use data from their own research projects for the semester project. However, it is essential to ensure the integrity and privacy of the data, as well as compliance with the policies of their research lab or institution. To safeguard data privacy and integrity: \n\nData Sharing Restrictions: Students are NOT allowed to upload raw research data directly to the instructor, peers, GitHub repositories, Posit Cloud, or any other external platform.\nDe-Identification and Transformation: Before using or sharing the data for the semester project, students must de-identify and transform the data as necessary. This process should ensure that sensitive information or identifying details are removed or anonymized. All data preparation must be performed locally on the student’s machine before incorporating it into the project. \nDocumentation Requirement: Students must include a clear description of the steps taken to de-identify and transform the data in their project report or presentation. This demonstrates adherence to ethical data handling practices.\n\nBy following these guidelines, students can apply their learning to real-world datasets while respecting ethical and institutional standards. The instructor is not responsible for students’ violations to the integrity and privacy of their research data. Non-compliance with this policy may result in disqualification of the project or additional academic consequences.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#accessibility",
    "href": "outline/outline.html#accessibility",
    "title": "PLNT6800|01 - Course Outline",
    "section": "8. Accessibility",
    "text": "8. Accessibility\nStudents requiring accommodations must register with Student Accessibility Services. Contact the instructor early in the semester to arrange accommodations.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "outline/outline.html#land-acknowledgement",
    "href": "outline/outline.html#land-acknowledgement",
    "title": "PLNT6800|01 - Course Outline",
    "section": "9. Land Acknowledgement",
    "text": "9. Land Acknowledgement\nThe University of Guelph resides on the ancestral lands of the Attawandaron people and the treaty lands and territory of the Mississaugas of the Credit. We recognize the significance of the Dish with One Spoon Covenant to this land and offer respect to our Anishinaabe, Haudenosaunee, and Métis neighbours. Today, this gathering place is home to many First Nations, Inuit, and Métis peoples, and acknowledging them reminds us of our important connection to this land where we work and learn.",
    "crumbs": [
      "Course Outline",
      "Outline"
    ]
  },
  {
    "objectID": "software/software.html",
    "href": "software/software.html",
    "title": "Required Software & Accounts",
    "section": "",
    "text": "R & RStudio IDEGitGitHubGitHub DesktopPosit Cloud\n\n\nBefore starting with the course, it is important that you install R and R Studio in your computer. Please, follow these simple steps:\nDownload the latest version of R from CRAN, here Download the latest version of RStudio from posit, here Run and install in the downloaded files in the same order. There are plenty of useful tutorials on the internet such as:\n\nhttps://rstudio-education.github.io/hopr/starting.html\nhttps://www.youtube.com/watch?v=Tb3R4GLJ45U\n\n\n\nWhat is Git?\nGit is a powerful version control system that helps you track changes in your code, collaborate with others, and manage your projects effectively.\nHow to Install Git:\nFollow these steps to install Git on your computer:\n\nVisit the official Git website (https://git-scm.com/);to download the installer for your operating system.\nFor detailed guidance, read the official tutorial to install git (https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\nYou may use these step-by-step videos for installation:\n\n• Installing Git on Windows (https://www.youtube.com/watch?v=iYkLrXobBbA)\n• Installing Git on Mac (https://www.youtube.com/watch?v=B4qsvQ5IqWk)\nOnce installed, verify by typing git –version in your terminal to ensure Git is ready to use!\n\n\nWhat is GitHub?\nGitHub is a web-based interface (nowadays own by Microsoft) that facilitates the use of Git, an open source version control software. This lets multiple people make separate changes to projects and have control over them. GitHub is to Git the equivalent of what RStudio is to R.\nFor this course, it is required that you create your GitHub account. Preferably, using your University email (##@uoguelph.ca). Please, follow the steps detailed on this official link from GitHub:\nhttps://docs.github.com/en/get-started/start-your-journey/creating-an-account-on-github\nYou may also find multiple useful YouTube tutorials like this one:\nhttps://www.youtube.com/watch?v=h5cKAd94QNo\n\n\nWhat is GitHub Desktop?\nGitHub Desktop is a user-friendly application that helps you interact with Git and GitHub through a simple graphical interface, making it easier to manage your repositories.\nHow to Install GitHub Desktop:\nFollow these steps to install GitHub Desktop on your computer:\n\nVisit the GitHub Desktop website (https://desktop.github.com); and download the application for your operating system.\nRun the installer and follow the on-screen instructions to complete the installation.\nAfter installation, sign in to your GitHub account or create one if you don’t have it.\n\nFor additional help, check out these videos:\n• Installing GitHub Desktop on Windows (https://www.youtube.com/watch?v=3JdDAJ2YPeU)\n• Installing GitHub Desktop on Mac (https://www.youtube.com/watch?v=C0n6O4d0ccw)\nOnce installed, you can easily clone repositories, commit changes, and sync your work with GitHub!\n\n\nWhat is Posit Cloud?\nPosit Cloud is an online platform that allows you to run RStudio “online”. It helps to write and execute R code directly in your browser, making it easy to learn and work on data science projects without installing software locally.\nHow to Create a Free Posit Cloud Account:\nFollow these steps to set up your free account:\n\nVisit Posit Cloud website at https://posit.cloud.\nClick on Sign Up and choose to create an account using your UoG email. THIS IS IMPORTANT, USE YOUR UOG EMAIL!\nComplete the registration form and verify your email if required.\nOnce registered, log in to your Posit Cloud account and start exploring R projects directly in your browser.\nMore instructions about our course workspace will be provided during the semester.\n\nFor a hint of what PositCloud does, you may watch with this video:\nhttps://www.youtube.com/watch?v=-fzwm4ZhVQQ With Posit Cloud, you’ll have a convenient environment to learn and work on your R programming assignments!",
    "crumbs": [
      "Software",
      "Instructions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLNT6800|01 - Special Topics in Plant Science",
    "section": "",
    "text": "Reproducible Ag Data Science with R is designed for graduate students in crop and soil sciences to develop key skills in data science using R. This course emphasizes reproducibility in data analysis, ensuring that results can be consistently replicated. Students will learn essential data science concepts, and how to use functions, packages, and version control to effectively manage their data and collaborate with peers. Following tidy principles, the course promotes best coding practices for data wrangling, effective visualization, and clean deployment of statistical models common in agriculture. By the end of the course, students will be equipped to handle a variety of agricultural datasets and produce reliable, reproducible research outcomes.\n\n\nBy the end of this course, students will be able to:\n\nUnderstand and apply the principles of reproducible research in data science.\nUse version control tools like GitHub for managing code and collaborative projects.\nDevelop proficiency in R, including data wrangling, data visualization, and the use of relevant packages for agricultural datasets.\nApply statistical models to agricultural data and interpret the results.\nProduce professional reports using RMarkdown and Quarto, ensuring reproducibility and clarity.\n\n\n\n\nThis course will use a variety of technologies and resources. To successfully participate in and complete this course, students will need access to the following\n\n\nCourseLink. This platform will be used as the main Course-Home Page. If you need any assistance with the software tools or the CourseLink website, contact CourseLink Support. Email: courselink@uoguelph.ca Tel: 519-824-4120 ext. 56939 Toll-Free (CAN/USA): 1-866- 275-1478. Support Hours (Eastern Time): Monday thru Friday: 8:30 am–8:30 pm; Saturday: 10:00 am–4:00 pm; Sunday: 12:00 pm–6:00 pm\nZoom. This course will use Zoom for lectures when in-person class is not possible. Check your system requirements to ensure you will be able to participate (https://opened.uoguelph.ca/student-resources/system-and-software-requirements/). A Zoom link for the class will be provided before the first day of class. Please, check Home-Page and announcements on CourseLink, and emails from the instructor (acorrend@uoguelph.ca).\n\n\n\n\nR (latest stable version, available at CRAN).\nRStudio/Posit IDE (desktop or cloud-based version for writing and running R code).\nCourse-Specific Libraries and Packages: Students will be required to install R packages. Detailed instructions will be provided in class.\nVersion Control and Collaboration Tools: Git (for version control) and a free GitHub account for collaborative project work and sharing code.\n\n\n\n\nA laptop or desktop computer capable of running R and RStudio (Windows, MacOS, or Linux). Minimum specifications include:\n\nProcessor: At least a dual-core processor.\nRAM: 8 GB or more (16 GB recommended for handling larger datasets).\nStorage: 10 GB of free space for software installation, course files, and datasets.\n\n\n\n\nReliable high-speed internet for accessing online sessions, resources, downloading software, and using cloud-based platforms (e.g., Posit Cloud, GitHub).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "PLNT6800|01 - Special Topics in Plant Science",
    "section": "",
    "text": "Reproducible Ag Data Science with R is designed for graduate students in crop and soil sciences to develop key skills in data science using R. This course emphasizes reproducibility in data analysis, ensuring that results can be consistently replicated. Students will learn essential data science concepts, and how to use functions, packages, and version control to effectively manage their data and collaborate with peers. Following tidy principles, the course promotes best coding practices for data wrangling, effective visualization, and clean deployment of statistical models common in agriculture. By the end of the course, students will be equipped to handle a variety of agricultural datasets and produce reliable, reproducible research outcomes.\n\n\nBy the end of this course, students will be able to:\n\nUnderstand and apply the principles of reproducible research in data science.\nUse version control tools like GitHub for managing code and collaborative projects.\nDevelop proficiency in R, including data wrangling, data visualization, and the use of relevant packages for agricultural datasets.\nApply statistical models to agricultural data and interpret the results.\nProduce professional reports using RMarkdown and Quarto, ensuring reproducibility and clarity.\n\n\n\n\nThis course will use a variety of technologies and resources. To successfully participate in and complete this course, students will need access to the following\n\n\nCourseLink. This platform will be used as the main Course-Home Page. If you need any assistance with the software tools or the CourseLink website, contact CourseLink Support. Email: courselink@uoguelph.ca Tel: 519-824-4120 ext. 56939 Toll-Free (CAN/USA): 1-866- 275-1478. Support Hours (Eastern Time): Monday thru Friday: 8:30 am–8:30 pm; Saturday: 10:00 am–4:00 pm; Sunday: 12:00 pm–6:00 pm\nZoom. This course will use Zoom for lectures when in-person class is not possible. Check your system requirements to ensure you will be able to participate (https://opened.uoguelph.ca/student-resources/system-and-software-requirements/). A Zoom link for the class will be provided before the first day of class. Please, check Home-Page and announcements on CourseLink, and emails from the instructor (acorrend@uoguelph.ca).\n\n\n\n\nR (latest stable version, available at CRAN).\nRStudio/Posit IDE (desktop or cloud-based version for writing and running R code).\nCourse-Specific Libraries and Packages: Students will be required to install R packages. Detailed instructions will be provided in class.\nVersion Control and Collaboration Tools: Git (for version control) and a free GitHub account for collaborative project work and sharing code.\n\n\n\n\nA laptop or desktop computer capable of running R and RStudio (Windows, MacOS, or Linux). Minimum specifications include:\n\nProcessor: At least a dual-core processor.\nRAM: 8 GB or more (16 GB recommended for handling larger datasets).\nStorage: 10 GB of free space for software installation, course files, and datasets.\n\n\n\n\nReliable high-speed internet for accessing online sessions, resources, downloading software, and using cloud-based platforms (e.g., Posit Cloud, GitHub).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "slides/day1.html#welcome",
    "href": "slides/day1.html#welcome",
    "title": "Reproducible Ag Data Science with R",
    "section": "Welcome 👋",
    "text": "Welcome 👋\n\nGoal: Gain foundational knowledge and understand how data science can improve agricultural practices.\nLet’s dive into it with an emphasis on reproducibility and data literacy.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember: Questions and discussions are encouraged! 💬",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#objectives-for-today",
    "href": "slides/day1.html#objectives-for-today",
    "title": "Reproducible Ag Data Science with R",
    "section": "Objectives for Today 📌",
    "text": "Objectives for Today 📌\n\nDefine core concepts:\n\nData Science,\nData Literacy,\nReproducibility.\n\nUnderstand the role of reproducible data science in agriculture.\nExplore challenges and opportunities.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#what-is-data-science-in-agriculture",
    "href": "slides/day1.html#what-is-data-science-in-agriculture",
    "title": "Reproducible Ag Data Science with R",
    "section": "What is Data Science in Agriculture? 🌱",
    "text": "What is Data Science in Agriculture? 🌱\n\nApplying data engineering, analysis, statistics, and machine learning to solve agricultural problems.\nExamples: Precision agriculture, yield forecasting, environmental monitoring.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#key-definitions",
    "href": "slides/day1.html#key-definitions",
    "title": "Reproducible Ag Data Science with R",
    "section": "Key Definitions 📖",
    "text": "Key Definitions 📖\n\n\n\nData Science: Extracting insights from data using algorithms and statistical methods. \nData Literacy: Skills to read, interpret, and analyze data. \nReproducibility: Ensuring analyses can be recreated by others.\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhy does reproducibility matter?\n\nTrustworthy results,\ntransparency, &\ncollaboration in research.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#challenges-in-data-literacy",
    "href": "slides/day1.html#challenges-in-data-literacy",
    "title": "Reproducible Ag Data Science with R",
    "section": "Challenges in Data Literacy 🌐",
    "text": "Challenges in Data Literacy 🌐\n\nDiverse data sources (weather, soil, crop data)\nStandardization issues across datasets\nData skills gap among ag professionals",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#why-does-it-matter",
    "href": "slides/day1.html#why-does-it-matter",
    "title": "Reproducible Ag Data Science with R",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\n\nIt is the #1 skill-gap in the job market: \n\nAcademia,\nIndustry,\nGovernment, NGOs, etc.\n\n\n\n\n\nIs there a REPRODUCIBILITY CRISIS in science?\nYES\nA Nature survey with ~1,600 researchers found that\n\n+70% failure rate to reproduce another scientist’s experiments\n+50% have failed to reproduce their own experiments\nMain causes: selective reporting, weak stats, code/data unavailability, etc.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#good-news-is",
    "href": "slides/day1.html#good-news-is",
    "title": "Reproducible Ag Data Science with R",
    "section": "GOOD NEWS IS…",
    "text": "GOOD NEWS IS…",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#why-reproducibility-in-agriculture",
    "href": "slides/day1.html#why-reproducibility-in-agriculture",
    "title": "Reproducible Ag Data Science with R",
    "section": "Why Reproducibility in Agriculture?",
    "text": "Why Reproducibility in Agriculture?\n\nAgriculture research relies heavily on environmental data, often variable and complex.\nWe have complex challenges 🗒️\n\nVariability due to environmental factors, soil types, and weather patterns.\nComplex datasets involving long-term studies, geographical variability.\n\nOpportunities ✅\n\nReproducibility helps stakeholders make reliable, data-driven decisions.\nEnsures scientific findings are reliable and valid.\nFacilitates collaboration, accountability, and efficiency among researchers and practitioners.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#challenges-in-ag-research",
    "href": "slides/day1.html#challenges-in-ag-research",
    "title": "Reproducible Ag Data Science with R",
    "section": "Challenges in Ag-research",
    "text": "Challenges in Ag-research\n\nREPRODUCIBILITY 💻\n\nLimited capability to reproduce analyses & results\nDATA are rarely shared, CODES even less\n\n\n\nACCESSIBILITY 📲\n\nYet we are not translating enough science into flexible, and transparent decision tools.\n\n\n\n“But it all starts with …”\n\n\nEDUCATION 🎓\n\nLimited curriculum in applied data science",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#discussion-prompt",
    "href": "slides/day1.html#discussion-prompt",
    "title": "Reproducible Ag Data Science with R",
    "section": "Discussion Prompt 💬",
    "text": "Discussion Prompt 💬\n\n\ni. Where do you think improved data literacy & reproducibility could impact agriculture the most?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nConsider areas like resource management, market predictions, and farm management.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#what-is-r",
    "href": "slides/day1.html#what-is-r",
    "title": "Reproducible Ag Data Science with R",
    "section": "What is R? 🧮",
    "text": "What is R? 🧮\n\n\nR is a programming language and environment primarily for statistical analysis, data visualization, and data science.\nKnown for its extensive statistical libraries, data manipulation capabilities, and graphics.\nWidely used in fields like data science, bioinformatics, agriculture, and social sciences.\n\n\n\nBrief History 📜\n\nOrigin: Developed in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand.\nInspiration: R is an implementation of the S language, designed at Bell Laboratories for data analysis.\nOpen Source: Released as free, open-source software, leading to a large community of users and contributors.\nPopularity: Today, R is one of the top programming languages for statistical analysis and data science.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#r-vs.-excel-for-data-wrangling",
    "href": "slides/day1.html#r-vs.-excel-for-data-wrangling",
    "title": "Reproducible Ag Data Science with R",
    "section": "R vs. Excel for Data Wrangling 📊",
    "text": "R vs. Excel for Data Wrangling 📊\n\n\n\nExcel: Known for ease of use, popular among business and finance professionals.\n\nPros: Intuitive, good for small datasets and quick analysis.\nCons: Limited in handling large datasets, lacks reproducibility.\n\nR: Provides powerful data manipulation packages (e.g., dplyr, tidyr).\n\nPros: Handles large datasets efficiently, supports complex transformations, fully reproducible.\nCons: Requires programming knowledge, steeper learning curve than Excel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip: R is highly scalable and is ideal for projects requiring automation, reproducibility, and handling large datasets.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#r-vs.-sas-for-statistical-analysis",
    "href": "slides/day1.html#r-vs.-sas-for-statistical-analysis",
    "title": "Reproducible Ag Data Science with R",
    "section": "R vs. SAS for Statistical Analysis 📉",
    "text": "R vs. SAS for Statistical Analysis 📉\n\n\n\nSAS: A powerful statistical software suite used widely in industries such as healthcare and finance.\n\nPros: Robust for regulatory environments, highly standardized.\nCons: Proprietary and costly, limited community contributions.\n\nR: Offers a vast array of statistical packages and flexibility in method implementation.\n\nPros: Free and open-source, customizable, strong community support.\nCons: Requires more coding and configuration for regulatory standards.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nComparison: R is often chosen for research and academia due to its flexibility and customization, while SAS remains strong in industries needing strict compliance and control.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#r-vs-python",
    "href": "slides/day1.html#r-vs-python",
    "title": "Reproducible Ag Data Science with R",
    "section": "R vs Python 🔍",
    "text": "R vs Python 🔍\n\nR, & Python are popular languages in data science and research.\nEach language has unique strengths, ideal use cases, and licensing considerations.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#r-strengths-and-use-cases",
    "href": "slides/day1.html#r-strengths-and-use-cases",
    "title": "Reproducible Ag Data Science with R",
    "section": "R: Strengths and Use Cases 🧮",
    "text": "R: Strengths and Use Cases 🧮\n\n\n\nDesigned for Statistics: R is optimized for statistical analysis, making it ideal for research and academia.\nVisualization: Excellent data visualization libraries like ggplot2.\nLicensing: Licensed under GPL; many packages are also GPL, with some using MIT or BSD.\n\n\nIdeal Use Cases:\n\nData analysis, visualization, and complex statistical modeling.\nResearch and academia where open-source, reproducible code is needed.\nLicensing in Production: GPL may restrict proprietary use; check package licenses carefully.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#python-strengths-and-use-cases",
    "href": "slides/day1.html#python-strengths-and-use-cases",
    "title": "Reproducible Ag Data Science with R",
    "section": "Python: Strengths and Use Cases 🐍",
    "text": "Python: Strengths and Use Cases 🐍\n\n\n\nGeneral-Purpose Language: Python is popular for both data science and software development.\nMachine Learning & AI: Extensive libraries for ML and AI, such as scikit-learn, TensorFlow.\nLicensing: PSFL (Python Software Foundation License), highly permissive for proprietary use.\n\n\nIdeal Use Cases:\n\nEnd-to-end development, from data wrangling to ML and web development.\nProduction-ready ML and AI applications.\nLicensing in Production: Permissive licenses allow closed-source use, making Python production-friendly.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#comparison-summary",
    "href": "slides/day1.html#comparison-summary",
    "title": "Reproducible Ag Data Science with R",
    "section": "Comparison Summary 📊",
    "text": "Comparison Summary 📊\n\n\n\n\n\n\n\nNote\n\n\n\nR: Best for statistical analysis and visualization, but GPL license may restrict use in proprietary products.\nExcel: User-friendly, ideal for simple tasks, but limited for complex data wrangling.\nSAS: Industry-standard for statistical analysis with regulatory requirements, but costly and less flexible than R.\nPython: Strong in ML and AI with highly permissive licensing, making it ideal for production.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nR\nPython\n\n\n\n\nPrimary Strength\nStatistics & Visualization\nGeneral-purpose, ML, AI\n\n\nPerformance\nModerate\nModerate\n\n\nLicensing\nGPL (core), MIT, BSD (some)\nPSFL, highly permissive\n\n\nProduction Use\nLimited by GPL\nVery friendly for proprietary\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nChoosing the right tool depends on your project’s requirements, team skills, and licensing needs for research vs. production.",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "slides/day1.html#thank-you",
    "href": "slides/day1.html#thank-you",
    "title": "Reproducible Ag Data Science with R",
    "section": "THANK YOU!",
    "text": "THANK YOU!\nacorrend@uoguelph.ca\n\nAdrian A. Correndo\nAssistant Professor\nSustainable Cropping Systems\nDepartment of Plant Agriculture\nUniversity of Guelph\n\n\nRm 226, Crop Science Bldg | Department of Plant Agriculture\nOntario Agricultural College | University of Guelph | 50 Stone Rd E, Guelph, ON-N1G 2W1, Canada.\n\nContact me",
    "crumbs": [
      "Slides",
      "Reproducibility"
    ]
  },
  {
    "objectID": "outline/calendar.html",
    "href": "outline/calendar.html",
    "title": "PLNT6800|01 - Calendar",
    "section": "",
    "text": "Days: Wednesdays and Fridays\nTime: 1:00 pm - 2:20 pm\nLocation: CRSC 202",
    "crumbs": [
      "Calendar",
      "Schedule"
    ]
  },
  {
    "objectID": "outline/calendar.html#calendar",
    "href": "outline/calendar.html#calendar",
    "title": "PLNT6800|01 - Calendar",
    "section": "Calendar",
    "text": "Calendar\nThe course runs on Wednesdays and Fridays from 1:00 PM to 2:20 PM. Below is the tentative schedule. Please, note the schedule may vary depending on the progress of the class.\n\n\n\nWeek\nDate\nTopic\nCode\nLesson\n\n\n\n\n1\nJan 8\nIntroductions, Reproducibility\n-\n-\n\n\n\nJan 10\nEssentials of RStudio, R coding\n🌐\n🎥\n\n\n2\nJan 15\nFundamentals of R packages\n🌐\n🎥\n\n\n\nJan 17\nBasics of version control & GitHub\n🌐\n🎥\n\n\n3\nJan 22\nData Wrangling I\n🌐\n🎥\n\n\n\nJan 24\nData Wrangling II\n🌐\n🎥\n\n\n4\nJan 29\nData Wrangling III\n🌐\n🎥\n\n\n\nJan 31\nData Viz I: ggplot2 basics\n🌐\n🎥\n\n\n5\nFeb 5\nData Viz II: Multiple plots\n🌐\n🎥\n\n\n\nFeb 7\nData Viz III: Advanced plots, maps\n🌐\n🎥\n\n\n6\nFeb 12\nIteration: Loops & Mapping\n🌐\n🎥\n\n\n\nFeb 14\nWeather Data: Retrieving & Processing\n🌐\n🎥\n\n\n\n\nWinter Break begins after the end of class\n\n\n\n\n7\nFeb 26\nModels I: Key concepts\n🌐\n🎥\n\n\n\nFeb 28\nModels II: Explanatory vs. Predictive\n🌐\n🎥\n\n\n8\nMar 5\nModels III: Linear Models\n🌐\n🎥\n\n\n\nMar 7\nModels IV: Fixed, Random, Mixed Effects\n🌐\n🎥\n\n\n9\nMar 12\nModels V: Regression I\n🌐\n🎥\n\n\n\nMar 14\nModels VI: Regression II\n🌐\n🎥\n\n\n10\nMar 19\nModels VII: Review LMs\n🌐\n🎥\n\n\n\nMar 21\nModels VIII: Principal Components\n🌐\n🎥\n\n\n11\nMar 26\nQuarto / Rmarkdown tricks\n🌐\n🎥\n\n\n\nMar 28\nGeneral Review class\n🌐\n🎥\n\n\n12\nApr 2\nSemester Projects Presentations I\n🌐\n\n\n\n\nApr 4\nSemester Projects Presentations II\n🌐\n\n\n\n13\nApr 11\nFinal Exam Due (11.59 pm)\n🌐",
    "crumbs": [
      "Calendar",
      "Schedule"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html",
    "href": "coding/week_10/models_06.html",
    "title": "Non-Linear Regression with R",
    "section": "",
    "text": "Non-linear regression is a statistical technique used to model relationships that cannot be well-represented by a straight line. Unlike linear regression, which assumes a constant rate of change, non-linear models accommodate curves and complex relationships in data.\nThis tutorial will:\n\nIntroduce the concept of non-linear regression. \nShow how to fit non-linear models in R using nls() and nlme(). \nTry the minpack.lm package for starting values. \nConduct model selection using AIC and AICc. \nApply these concepts to the agridat::lasrosas.corn dataset. \nSee an example of specific package to help with non-linear regression\n\n\n\nMany real-world relationships are inherently non-linear. Examples include:\n\nGrowth models (e.g., exponential or power functions).\nYield response to fertilizers.\nEnzyme kinetics in biological systems.\n\nRequired packages for today\n\n# Load necessary libraries\nlibrary(pacman)\np_load(dplyr, tidyr) # data wrangling\np_load(ggplot2) #plots\np_load(agridat) # dataset\np_load(nls, nlme) # non-linear models\np_load(minpack.lm) # convergence help for nl models\np_load(AICcmodavg) # corrected AIC performance\np_load(soiltestcorr) # nl models for soil fertility",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#why-non-linear-regression",
    "href": "coding/week_10/models_06.html#why-non-linear-regression",
    "title": "Non-Linear Regression with R",
    "section": "",
    "text": "Many real-world relationships are inherently non-linear. Examples include:\n\nGrowth models (e.g., exponential or power functions).\nYield response to fertilizers.\nEnzyme kinetics in biological systems.\n\nRequired packages for today\n\n# Load necessary libraries\nlibrary(pacman)\np_load(dplyr, tidyr) # data wrangling\np_load(ggplot2) #plots\np_load(agridat) # dataset\np_load(nls, nlme) # non-linear models\np_load(minpack.lm) # convergence help for nl models\np_load(AICcmodavg) # corrected AIC performance\np_load(soiltestcorr) # nl models for soil fertility",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#visualizing-the-data",
    "href": "coding/week_10/models_06.html#visualizing-the-data",
    "title": "Non-Linear Regression with R",
    "section": "2.1 Visualizing the Data",
    "text": "2.1 Visualizing the Data\nTo determine if a non-linear model is needed, we first visualize the data:\n\n# Global scatter plot\nggplot(data_corn, aes(x = nitro, y = yield)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y~poly(x, 2))+\n  labs(title = \"Corn Yield vs Nitrogen\", x = \"Nitrogen (kg/ha)\", y = \"Yield (tons/ha)\")\n\n\n\n\n\n\n\n# Grouped scatter plot\nggplot(data_corn, aes(x = nitro, y = yield)) +\n  geom_point(aes(color = topo, shape = year)) +\n  geom_smooth(aes(color = topo, linetype = year), se=F)+\n  labs(title = \"Corn Yield vs Nitrogen\", x = \"Nitrogen (kg/ha)\", y = \"Yield (tons/ha)\")",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#only-topo",
    "href": "coding/week_10/models_06.html#only-topo",
    "title": "Non-Linear Regression with R",
    "section": "4.1 only topo",
    "text": "4.1 only topo\n\nnlme_model_topo &lt;- nlme(yield ~ a * nitro^b,\n                         data = data_corn,\n                         fixed = (a+b) ~ topo,\n                         random = (a + b) ~ 1 | rep,\n                         start = c(a_Intercept = 1, b_Intercept = 0.5, \n                                   a_HT = 0, a_LO = 0, a_W = 0,\n                                   b_HT = 0, b_LO = 0, b_W = 0\n                                   )      )\nsummary(nlme_model_topo)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ a * nitro^b \n  Data: data_corn \n       AIC      BIC    logLik\n  33377.94 33451.67 -16676.97\n\nRandom effects:\n Formula: list(a ~ 1, b ~ 1)\n Level: rep\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \na.(Intercept) 4.358860e-01 a.(In)\nb.(Intercept) 5.329321e-09 0.475 \nResidual      3.071279e+01       \n\nFixed effects:  (a + b) ~ topo \n                  Value Std.Error   DF   t-value p-value\na.(Intercept)  65.83564  9.195307 3433  7.159700  0.0000\na.topoHT      -25.49439 12.649071 3433 -2.015515  0.0439\na.topoLO        1.40690 12.133727 3433  0.115949  0.9077\na.topoW       -12.09268 11.787969 3433 -1.025849  0.3050\nb.(Intercept)   0.04413  0.032635 3433  1.352101  0.1764\nb.topoHT        0.00672  0.060009 3433  0.112046  0.9108\nb.topoLO        0.01297  0.042594 3433  0.304426  0.7608\nb.topoW         0.01019  0.045655 3433  0.223244  0.8234\n Correlation: \n              a.(In) a.tpHT a.tpLO a.topW b.(In) b.tpHT b.tpLO\na.topoHT      -0.726                                          \na.topoLO      -0.757  0.550                                   \na.topoW       -0.779  0.567  0.591                            \nb.(Intercept) -0.993  0.722  0.753  0.775                     \nb.topoHT       0.540 -0.966 -0.409 -0.421 -0.544              \nb.topoLO       0.761 -0.553 -0.994 -0.594 -0.766  0.417       \nb.topoW        0.710 -0.516 -0.538 -0.989 -0.715  0.389  0.548\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.5734100 -0.3300691  0.0845198  0.6716186  3.5438008 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n\ncoef(nlme_model_topo)\n\n   a.(Intercept)  a.topoHT a.topoLO   a.topoW b.(Intercept)    b.topoHT\nR1      65.65141 -25.49439 1.406899 -12.09268    0.04412586 0.006723821\nR2      65.72720 -25.49439 1.406899 -12.09268    0.04412586 0.006723821\nR3      66.12830 -25.49439 1.406899 -12.09268    0.04412586 0.006723821\n     b.topoLO    b.topoW\nR1 0.01296678 0.01019221\nR2 0.01296678 0.01019221\nR3 0.01296678 0.01019221\n\n# Some components of broom mixed doesn't work yet for non-linear models\nbroom.mixed::tidy(nlme_model_topo)\n\n# A tibble: 8 × 7\n  effect term           estimate std.error    df statistic  p.value\n  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 fixed  a.(Intercept)  65.8        9.20    3433     7.16  9.84e-13\n2 fixed  a.topoHT      -25.5       12.6     3433    -2.02  4.39e- 2\n3 fixed  a.topoLO        1.41      12.1     3433     0.116 9.08e- 1\n4 fixed  a.topoW       -12.1       11.8     3433    -1.03  3.05e- 1\n5 fixed  b.(Intercept)   0.0441     0.0326  3433     1.35  1.76e- 1\n6 fixed  b.topoHT        0.00672    0.0600  3433     0.112 9.11e- 1\n7 fixed  b.topoLO        0.0130     0.0426  3433     0.304 7.61e- 1\n8 fixed  b.topoW         0.0102     0.0457  3433     0.223 8.23e- 1\n\nbroom.mixed::tidy(nls_model)\n\n# A tibble: 2 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 a      55.7       4.33       12.9  5.01e-37\n2 b       0.0564    0.0181      3.12 1.85e- 3",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#only-year",
    "href": "coding/week_10/models_06.html#only-year",
    "title": "Non-Linear Regression with R",
    "section": "4.2 only year",
    "text": "4.2 only year\n\nnlme_model_year &lt;- nlme(yield ~ a * nitro^b,\n                         data = data_corn,\n                         fixed = a + b ~ year,\n                         random = a + b ~ 1 | rep,\n                         start = c(a_Intercept = 1, b_Intercept = 0.5, \n                                   a_2001 = 0, b_2001 = 0))\n\nsummary(nlme_model_year)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ a * nitro^b \n  Data: data_corn \n       AIC      BIC    logLik\n  33791.26 33840.41 -16887.63\n\nRandom effects:\n Formula: list(a ~ 1, b ~ 1)\n Level: rep\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \na.(Intercept) 3.886561e-02 a.(In)\nb.(Intercept) 1.927724e-09 0.054 \nResidual      3.265434e+01       \n\nFixed effects:  a + b ~ year \n                 Value Std.Error   DF   t-value p-value\na.(Intercept) 50.55627  5.361897 3437  9.428802  0.0000\na.year2001    15.29395  9.280588 3437  1.647950  0.0995\nb.(Intercept)  0.06190  0.024771 3437  2.498676  0.0125\nb.year2001    -0.02825  0.036459 3437 -0.774741  0.4385\n Correlation: \n              a.(In) a.2001 b.(In)\na.year2001    -0.578              \nb.(Intercept) -0.992  0.573       \nb.year2001     0.674 -0.985 -0.679\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.7378975 -0.2882427  0.1438697  0.7748094  3.3330942 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n\ncoef(nlme_model_year)\n\n   a.(Intercept) a.year2001 b.(Intercept)  b.year2001\nR1      50.55581   15.29395    0.06189593 -0.02824663\nR2      50.55517   15.29395    0.06189593 -0.02824663\nR3      50.55783   15.29395    0.06189593 -0.02824663\n\n# Some components of broom mixed doesn't work yet for non-linear models\nbroom.mixed::tidy(nlme_model_year)\n\n# A tibble: 4 × 7\n  effect term          estimate std.error    df statistic  p.value\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 fixed  a.(Intercept)  50.6       5.36    3437     9.43  7.39e-21\n2 fixed  a.year2001     15.3       9.28    3437     1.65  9.95e- 2\n3 fixed  b.(Intercept)   0.0619    0.0248  3437     2.50  1.25e- 2\n4 fixed  b.year2001     -0.0282    0.0365  3437    -0.775 4.39e- 1",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#topo-year",
    "href": "coding/week_10/models_06.html#topo-year",
    "title": "Non-Linear Regression with R",
    "section": "4.3 topo + year",
    "text": "4.3 topo + year\n\nnlme_model_topoyear &lt;- nlme(yield ~ a * nitro^b,\n                         data = data_corn,\n                         fixed = a + b ~ topo + year,\n                         random = a + b ~ 1 | rep,\n                         start = c(a_Intercept = 1, b_Intercept = 0.5, \n                                       a_HT = 0, a_LO = 0, a_W = 0, a_2001 = 0, \n                                       b_HT = 0, b_LO = 0, b_W = 0, b_2001 = 0))",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#minpack.lm",
    "href": "coding/week_10/models_06.html#minpack.lm",
    "title": "Non-Linear Regression with R",
    "section": "4.4 minpack.lm",
    "text": "4.4 minpack.lm\n\n# Fit using nlsLM() instead of nls()\nnls_fit &lt;- nlsLM(yield ~ a * nitro^b, \n                 data = data_corn, \n                 start = list(a = 10, b = 0.8)) \n\n# Extract better initial estimates\ncoef(nls_fit)\n\nnlme_model_topoyear &lt;- nlme(yield ~ a * nitro^b, \n                             data = data_corn, \n                             fixed = a + b ~ topo + year, \n                             random = a + b ~ 1 | rep, \n                             start = c(a_Intercept = coef(nls_fit)[1], \n                                       b_Intercept = coef(nls_fit)[2], \n                                       a_HT = 0.5, a_LO = -0.5, a_W = 0.8, a_2001 = 0.3, \n                                       b_HT = 0.1, b_LO = -0.1, b_W = 0.2, b_2001 = 0.05))",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#topoyear",
    "href": "coding/week_10/models_06.html#topoyear",
    "title": "Non-Linear Regression with R",
    "section": "4.5 topo*year",
    "text": "4.5 topo*year\n\nnlme_model_interaction &lt;- nlme(yield ~ a * nitro^b,\n                                data = data_corn,\n                                fixed = a + b ~ topo * year, \n                                random = a + b ~ 1 | rep,\n                                start = c(a_Intercept = 1, b_Intercept = 0.5, \n                                          a_HT = 0, a_LO = 0, a_W = 0, a_2001 = 0, \n                                          a_HT_2001 = 0, a_LO_2001 = 0, a_W_2001 = 0,\n                                          b_HT = 0, b_LO = 0, b_W = 0, b_2001 = 0, \n                                          b_HT_2001 = 0, b_LO_2001 = 0, b_W_2001 = 0))\nsummary(nlme_model_interaction)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ a * nitro^b \n  Data: data_corn \n       AIC      BIC    logLik\n  33089.36 33212.24 -16524.68\n\nRandom effects:\n Formula: list(a ~ 1, b ~ 1)\n Level: rep\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \na.(Intercept) 4.218854e-01 a.(In)\nb.(Intercept) 5.341360e-09 0.423 \nResidual      2.938401e+01       \n\nFixed effects:  a + b ~ topo * year \n                      Value Std.Error   DF   t-value p-value\na.(Intercept)      55.24845 11.382004 3425  4.854018  0.0000\na.topoHT          -18.91198 14.663476 3425 -1.289734  0.1972\na.topoLO            5.95162 15.329161 3425  0.388255  0.6979\na.topoW            -4.92420 14.134876 3425 -0.348373  0.7276\na.year2001         34.48361 19.939592 3425  1.729404  0.0838\na.topoHT:year2001 -25.47476 26.755473 3425 -0.952133  0.3411\na.topoLO:year2001 -13.67997 25.911191 3425 -0.527956  0.5976\na.topoW:year2001  -24.38799 25.267053 3425 -0.965209  0.3345\nb.(Intercept)       0.04102  0.048357 3425  0.848201  0.3964\nb.topoHT            0.05674  0.076614 3425  0.740538  0.4590\nb.topoLO           -0.00215  0.062256 3425 -0.034577  0.9724\nb.topoW             0.02704  0.061999 3425  0.436191  0.6627\nb.year2001         -0.03186  0.064449 3425 -0.494372  0.6211\nb.topoHT:year2001  -0.06188  0.117790 3425 -0.525302  0.5994\nb.topoLO:year2001   0.04040  0.083879 3425  0.481585  0.6301\nb.topoW:year2001   -0.00726  0.090420 3425 -0.080283  0.9360\n Correlation: \n                  a.(In) a.tpHT a.tpLO a.topW a.2001 a.HT:2 a.LO:2 a.W:20\na.topoHT          -0.776                                                 \na.topoLO          -0.742  0.576                                          \na.topoW           -0.805  0.625  0.598                                   \na.year2001        -0.571  0.443  0.424  0.459                            \na.topoHT:year2001  0.425 -0.548 -0.316 -0.342 -0.745                     \na.topoLO:year2001  0.439 -0.341 -0.592 -0.354 -0.770  0.573              \na.topoW:year2001   0.450 -0.349 -0.334 -0.559 -0.789  0.588  0.607       \nb.(Intercept)     -0.992  0.770  0.737  0.799  0.566 -0.422 -0.436 -0.447\nb.topoHT           0.626 -0.971 -0.465 -0.504 -0.357  0.532  0.275  0.282\nb.topoLO           0.771 -0.598 -0.991 -0.620 -0.440  0.328  0.586  0.347\nb.topoW            0.774 -0.601 -0.574 -0.991 -0.442  0.329  0.340  0.555\nb.year2001         0.744 -0.578 -0.553 -0.599 -0.965  0.719  0.743  0.762\nb.topoHT:year2001 -0.407  0.632  0.302  0.328  0.528 -0.945 -0.406 -0.417\nb.topoLO:year2001 -0.572  0.444  0.735  0.461  0.741 -0.553 -0.972 -0.585\nb.topoW:year2001  -0.531  0.412  0.394  0.680  0.688 -0.513 -0.529 -0.969\n                  b.(In) b.tpHT b.tpLO b.topW b.2001 b.HT:2 b.LO:2\na.topoHT                                                          \na.topoLO                                                          \na.topoW                                                           \na.year2001                                                        \na.topoHT:year2001                                                 \na.topoLO:year2001                                                 \na.topoW:year2001                                                  \nb.(Intercept)                                                     \nb.topoHT          -0.631                                          \nb.topoLO          -0.777  0.490                                   \nb.topoW           -0.780  0.492  0.606                            \nb.year2001        -0.750  0.474  0.583  0.585                     \nb.topoHT:year2001  0.411 -0.650 -0.319 -0.320 -0.547              \nb.topoLO:year2001  0.577 -0.364 -0.742 -0.450 -0.768  0.420       \nb.topoW:year2001   0.535 -0.338 -0.415 -0.686 -0.713  0.390  0.548\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.66304432 -0.12672025  0.07029809  0.35912890  3.70405544 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n\ndf_int &lt;- broom.mixed::tidy(nlme_model_interaction)\n\ndf_int %&gt;% \n  mutate(estimate = as.numeric(estimate),\n         `std.error` = as.numeric(`std.error`)) %&gt;% \n  separate(term, into = c(\"coef\", \"topo_year\"), sep = \".\", remove = F) %&gt;% \n  ggplot(aes(x = term, y = estimate))+\n  geom_point(aes(color = term))+\n  geom_errorbar(aes(ymin = estimate - `std.error`, ymax = estimate + `std.error`))+\n  theme_classic()",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#aic",
    "href": "coding/week_10/models_06.html#aic",
    "title": "Non-Linear Regression with R",
    "section": "5.1 AIC:",
    "text": "5.1 AIC:\nWe can use the Akaike Information Criterion (AIC).\n\n# Get \nAIC(nlme_model_ab, nlme_model_topo, nlme_model_year, nlme_model_interaction)\n\n                       df      AIC\nnlme_model_ab           6 33856.62\nnlme_model_topo        12 33377.94\nnlme_model_year         8 33791.26\nnlme_model_interaction 20 33089.36",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#aicc",
    "href": "coding/week_10/models_06.html#aicc",
    "title": "Non-Linear Regression with R",
    "section": "5.2 AICc:",
    "text": "5.2 AICc:\nWhen comparing different nonlinear models, it is essential to use an objective model selection criterion. One of the most commonly used criteria is the Akaike Information Criterion (AIC) and its corrected version AICc.\nWhy Use AICc Instead of AIC?\nAIC is widely used for model comparison, but it has a known bias when applied to small sample sizes or when the number of parameters (K) is large relative to the sample size (n). AICc corrects for this bias by adding a small-sample penalty:\n\\[\nAICc = AIC + \\frac{2K(K+1)}{n-K-1}\n\\]\nWhere:\nAIC is the standard Akaike Information Criterion: \\(\\(AIC = -2 \\log L + 2K\\)\\)  K is the number of estimated parameters  n is the sample size \nThus, when n is large, AICc ≈ AIC, but for small datasets, AICc penalizes overfitting more effectively.",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#delta-aicc-and-aicc-weights",
    "href": "coding/week_10/models_06.html#delta-aicc-and-aicc-weights",
    "title": "Non-Linear Regression with R",
    "section": "5.3 Delta AICc and AICc Weights",
    "text": "5.3 Delta AICc and AICc Weights\nTo compare models, we use Delta AICc (ΔAICc) and AICc weights:\n\nΔAICc: The difference between each model’s AICc and the lowest AICc value.\nA model with ΔAICc = 0 is the best model.\nModels with ΔAICc &lt; 2 have substantial support.\nΔAICc &gt; 10 means the model has little support.\nAICc weight (wAICc): Measures the relative likelihood of each model given the data.\nA higher weight means the model is more likely to be the best model.\nThe weights sum to 1, allowing for direct comparison of model likelihoods.",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#using-aiccmodavg-in-r",
    "href": "coding/week_10/models_06.html#using-aiccmodavg-in-r",
    "title": "Non-Linear Regression with R",
    "section": "5.4 Using AICcmodavg in R",
    "text": "5.4 Using AICcmodavg in R\n\n# Run them separatedly\nAICcmodavg::AICc(nlme_model_ab)\n\n[1] 33856.64\n\nAICcmodavg::AICc(nlme_model_topo)\n\n[1] 33378.03\n\nAICcmodavg::AICc(nlme_model_year)\n\n[1] 33791.3\n\nAICcmodavg::AICc(nlme_model_interaction)\n\n[1] 33089.6\n\n\nThe AICcmodavg package provides functions for model selection: ### Compare Models Using aictab()\n\n# Create a list of candidate models\nmodel_list &lt;- list(\n  simple = nlme_model_ab,\n  topo = nlme_model_topo,\n  year = nlme_model_year,\n  interaction = nlme_model_interaction\n)\n\n# Model selection table\nmodel_selection &lt;- AICcmodavg::aictab(cand.set = model_list, modnames = names(model_list))\n\nmodel_selection\n\n\nModel selection based on AICc:\n\n             K     AICc Delta_AICc AICcWt Cum.Wt        LL\ninteraction 20 33089.60       0.00      1      1 -16524.68\ntopo        12 33378.03     288.43      0      1 -16676.97\nyear         8 33791.30     701.70      0      1 -16887.63\nsimple       6 33856.64     767.04      0      1 -16922.31\n\n\nThis will output a table with:\n\nAICc values for each model\nΔAICc (relative difference)\nAICc weights (relative model support)",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_10/models_06.html#conclusion",
    "href": "coding/week_10/models_06.html#conclusion",
    "title": "Non-Linear Regression with R",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\n\nAICc is preferred over AIC when sample sizes are small.\nΔAICc helps identify the best model and evaluate model differences.\nAICc weights allow comparison of model likelihoods.\nThe AICcmodavg package makes it easy to apply AICc-based model selection in R.\n\nBy using AICc, we can objectively choose the best model while avoiding overfitting. 🚀",
    "crumbs": [
      "Lessons",
      "17-Models VI"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html",
    "href": "coding/week_04/07_ggplot2_intro.html",
    "title": "Data Viz I",
    "section": "",
    "text": "Description\nIn this class, we will explore advanced visualization techniques using ggplot2. You’ll learn how to fine-tune plots by modifying shapes, lines, legends, and adding custom titles and annotations. We will continue working with the agridat::lasrosas.corn dataset.",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html#define-custom-palettes",
    "href": "coding/week_04/07_ggplot2_intro.html#define-custom-palettes",
    "title": "Data Viz I",
    "section": "3.1 Define custom palettes",
    "text": "3.1 Define custom palettes\n\nmy_colors &lt;- c(\"#1e6091\", \"#f9c74f\", \"#9b2226\",  \"#599999\",  \"#8e5572\")",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html#histogram-with-adjusted-transparency-and-custom-bins",
    "href": "coding/week_04/07_ggplot2_intro.html#histogram-with-adjusted-transparency-and-custom-bins",
    "title": "Data Viz I",
    "section": "3.2 Histogram with Adjusted Transparency and Custom Bins",
    "text": "3.2 Histogram with Adjusted Transparency and Custom Bins\n\n# Density\ndensity_01 &lt;- \ncorn_data %&gt;% \nggplot(aes(x = yield)) +\n  geom_density(aes(fill = Year), color = \"grey15\", alpha = 0.5)+\n  labs(title = \"Yield Distribution Across Years\", x = \"Yield (qq/ha)\", y = \"Count\") +\n  scale_fill_manual(values = my_colors)+\n  theme_classic()\n\n# Histogram\nhisto_01 &lt;-\nggplot(corn_data, aes(x = yield)) +\n  geom_histogram(aes(fill = Year), bins = 20, alpha = 0.6, color = \"black\") +\n  labs(title = \"Yield Distribution Across Years\", x = \"Yield (qq/ha)\", y = \"Count\") +\n  scale_fill_manual(values = my_colors)+\n  geom_rug(aes(color = Year), alpha = 0.5)+\n  scale_color_manual(values = my_colors)+\n  theme_classic()\n\nhisto_01\n\n\n\n\n\n\n\n# Faceting by Year\nhisto_02 &lt;-\ncorn_data %&gt;% \nggplot(aes(x = yield)) +\n  geom_histogram(aes(fill = Year), bins = 30, alpha = 0.6, color = \"black\") +\n  labs(title = \"Yield Distribution Across Years\", x = \"Yield (qq/ha)\", y = \"Count\") +\n  scale_fill_manual(values = my_colors)+\n  geom_rug(aes(color = topo), alpha = 0.5)+\n  scale_color_manual(values = my_colors)+\n  facet_wrap(~topo)+\n  theme_classic()+\n  theme(legend.position = \"none\")\nhisto_02\n\n\n\n\n\n\n\ncorn_data %&gt;% \nggplot(aes(x = yield)) +\n  geom_histogram(aes(fill = topo), bins = 20, alpha = 0.6, color = \"black\") +\n  labs(title = \"Yield Distribution Across Years\", x = \"Yield (qq/ha)\", y = \"Count\") +\n  scale_fill_manual(values = my_colors)+\n  geom_rug(aes(color = topo), alpha = 0.5)+\n  scale_color_manual(values = my_colors)+\n  facet_wrap(~topo)+\n  theme_classic()+\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html#columnbar-plot",
    "href": "coding/week_04/07_ggplot2_intro.html#columnbar-plot",
    "title": "Data Viz I",
    "section": "3.3 Column/Bar plot",
    "text": "3.3 Column/Bar plot\n\n# Prepare summarized data frame\naggregated_corn &lt;- corn_data %&gt;% group_by(Year, year, nitro, nf) %&gt;% \n  summarize(yield_mean = mean(yield),\n            sd_yield = sd(yield))\n\n`summarise()` has grouped output by 'Year', 'year', 'nitro'. You can override\nusing the `.groups` argument.\n\n# Column plot\ncolplot_01 &lt;- \n  aggregated_corn %&gt;% \nggplot() +\n  geom_col(aes(x = nf, y = yield_mean, fill = Year), \n           color = \"grey25\") + # Triangle shape\n  scale_fill_manual(values = my_colors)+\n  labs(title = \"Yield vs. Nitrogen Levels\", x = \"Nitrogen (kg/ha)\", y = \"Yield (qq/ha)\") +\n  facet_wrap(~Year)+\n  theme_classic()\n\ncolplot_01\n\n\n\n\n\n\n\n# Add SD bars\ncolplot_02 &lt;-\ncolplot_01 +\n  geom_errorbar(data = aggregated_corn, \n                aes(ymin = yield_mean - sd_yield,\n                    ymax = yield_mean + sd_yield, \n                    x = nf),\n                width = .25)\n  \n\ncolplot_02\n\n\n\n\n\n\n\n# facet by topography\naggregated_topo &lt;- corn_data %&gt;% group_by(Year, year, nitro, nf, topo) %&gt;% \n  summarize(yield_mean = mean(yield),\n            sd_yield = sd(yield))\n\n`summarise()` has grouped output by 'Year', 'year', 'nitro', 'nf'. You can\noverride using the `.groups` argument.\n\n# dacet by topography and Year\ncolplot_03 &lt;- \naggregated_topo %&gt;% \nggplot() +\n  geom_col(aes(x = nf, y = yield_mean, fill = topo), \n           color = \"grey25\", alpha = 0.5) + # Triangle shape\n  geom_errorbar(aes(ymin = yield_mean - sd_yield,\n                    ymax = yield_mean + sd_yield, x = nf),\n                width = .25)+\n  labs(title = \"Yield vs. Nitrogen Levels\", x = \"Nitrogen (kg/ha)\", y = \"Yield (qq/ha)\") +\n  facet_grid(topo~Year)+\n  scale_fill_manual(values = my_colors)+\n  theme_classic()\n\ncolplot_03",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html#scatter-plot-with-custom-shapes-and-line-types",
    "href": "coding/week_04/07_ggplot2_intro.html#scatter-plot-with-custom-shapes-and-line-types",
    "title": "Data Viz I",
    "section": "3.4 Scatter Plot with Custom Shapes and Line Types",
    "text": "3.4 Scatter Plot with Custom Shapes and Line Types\n\ncorn_data %&gt;%\nggplot(aes(x = nitro, y = yield, color = factor(year))) +\n  geom_point(size = 3, shape = 17) + # Triangle shape\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\") +\n  labs(title = \"Yield vs. Nitrogen Levels\", x = \"Nitrogen (kg/ha)\", y = \"Yield (qq/ha)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_04/07_ggplot2_intro.html#boxplot-with-custom-shapes-and-line-types",
    "href": "coding/week_04/07_ggplot2_intro.html#boxplot-with-custom-shapes-and-line-types",
    "title": "Data Viz I",
    "section": "3.5 BoxPlot with Custom Shapes and Line Types",
    "text": "3.5 BoxPlot with Custom Shapes and Line Types\n\ncorn_data %&gt;% \nggplot(aes(x = nf, y = yield)) +\n  geom_boxplot(aes(fill = Year), color = \"grey15\", size = 0.5) +\n  geom_jitter(aes(x = nf, y = yield, color=Year), size = 0.1)+\n  scale_fill_brewer(palette=2, type = \"qual\")+\n  scale_color_brewer(palette=2, type = \"qual\")+\n  labs(title = \"Yield vs. Nitrogen Levels\", \n       x = \"Nitrogen (kg/ha)\", y = \"Yield (qq/ha)\") +\n  facet_wrap(~Year)+\n  theme_classic()",
    "crumbs": [
      "Lessons",
      "07-Data viz I"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html",
    "href": "coding/week_03/05_datawrangling_02.html",
    "title": "Transforming Ag data in R II",
    "section": "",
    "text": "This lesson builds on our previous session by introducing more advanced data wrangling techniques using tidyr, stringr, and forcats. We will explore how to manipulate and transform data for efficient analysis. Additionally, we introduce lubridate for handling dates effectively.\n\n\n\nlibrary(pacman)\np_load(agridat, dplyr, tidyr, stringr, forcats, skimr, lubridate)",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#required-packages-for-today",
    "href": "coding/week_03/05_datawrangling_02.html#required-packages-for-today",
    "title": "Transforming Ag data in R II",
    "section": "",
    "text": "library(pacman)\np_load(agridat, dplyr, tidyr, stringr, forcats, skimr, lubridate)",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#aggregation-with-group_by-and-summarize",
    "href": "coding/week_03/05_datawrangling_02.html#aggregation-with-group_by-and-summarize",
    "title": "Transforming Ag data in R II",
    "section": "2.1 Aggregation with group_by() and summarize()",
    "text": "2.1 Aggregation with group_by() and summarize()\n\ndata_corn &lt;- agridat::lasrosas.corn\nsummary_data &lt;- data_corn %&gt;% \n  group_by(topo, year) %&gt;% \n  summarize(mean_yield = mean(yield, na.rm = TRUE), .groups = \"drop\")\nsummary_data\n\n# A tibble: 8 × 3\n  topo   year mean_yield\n  &lt;fct&gt; &lt;int&gt;      &lt;dbl&gt;\n1 E      1999       64.8\n2 E      2001       92.7\n3 HT     1999       53.4\n4 HT     2001       44.7\n5 LO     1999       71.2\n6 LO     2001       99.9\n7 W      1999       66.0\n8 W      2001       67.7",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#applying-functions-to-multiple-columns-using-across",
    "href": "coding/week_03/05_datawrangling_02.html#applying-functions-to-multiple-columns-using-across",
    "title": "Transforming Ag data in R II",
    "section": "2.2 Applying Functions to Multiple Columns using across()",
    "text": "2.2 Applying Functions to Multiple Columns using across()\n\ndata_across &lt;- data_corn %&gt;% \n  mutate(across(c(lat, long), ~ round(.x, digits=1), .names = \"rounded_{.col}\")) # Rounding values to 1 decimal place for better readability\nhead(data_across)\n\n  year       lat      long yield nitro topo     bv rep nf rounded_lat\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5       -33.1\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5       -33.1\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5       -33.1\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5       -33.1\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5       -33.1\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5       -33.1\n  rounded_long\n1        -63.8\n2        -63.8\n3        -63.8\n4        -63.8\n5        -63.8\n6        -63.8",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#creating-conditional-columns-with-case_when",
    "href": "coding/week_03/05_datawrangling_02.html#creating-conditional-columns-with-case_when",
    "title": "Transforming Ag data in R II",
    "section": "2.3 Creating Conditional Columns with case_when()",
    "text": "2.3 Creating Conditional Columns with case_when()\n\ndata_casewhen &lt;- data_corn %&gt;% \n  mutate(yield_category = case_when(\n    yield &gt; 10 ~ \"High\",\n    yield &gt; 5 ~ \"Medium\",\n    TRUE ~ \"Low\"\n  ))\nhead(data_casewhen)\n\n  year       lat      long yield nitro topo     bv rep nf yield_category\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5           High\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5           High\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5           High\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5           High\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5           High\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5           High",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#slicing-data-with-slice",
    "href": "coding/week_03/05_datawrangling_02.html#slicing-data-with-slice",
    "title": "Transforming Ag data in R II",
    "section": "2.4 Slicing data with slice():",
    "text": "2.4 Slicing data with slice():\n\n# Selecting the first 3 rows\nfirst_rows &lt;- data_corn %&gt;% slice(1:3)\nhead(first_rows)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n\n# Selecting the last 3 rows\nlast_rows &lt;- data_corn %&gt;% slice_tail(n = 3)\nhead(last_rows)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 2001 -33.05110 -63.84189 92.33    39   LO 166.75  R3 N1\n2 2001 -33.05112 -63.84182 88.98    39   LO 163.59  R3 N1\n3 2001 -33.05115 -63.84175 85.74    39   LO 163.48  R3 N1\n\n# Selecting 3 random rows\nrandom_rows &lt;- data_corn %&gt;% slice_sample(n = 3)\nhead(random_rows)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 2001 -33.05006 -63.84804 90.68  75.4    W 193.82  R1 N3\n2 1999 -33.05079 -63.84613 58.94  66.0   HT 181.90  R3 N3\n3 1999 -33.05155 -63.84387 66.83  66.0    E 171.07  R2 N3\n\n# Selecting every 2nd row\nevery_second_row &lt;- data_corn %&gt;% slice(seq(1, n(), by = 2))\nhead(every_second_row)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n3 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n4 1999 -33.05121 -63.84844 76.17 131.5    W 172.94  R1 N5\n5 1999 -33.05123 -63.84830 69.77 131.5    W 171.88  R1 N5\n6 1999 -33.05126 -63.84816 71.05 131.5    W 173.02  R1 N5",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#working-with-time-series-data-lead-and-lag",
    "href": "coding/week_03/05_datawrangling_02.html#working-with-time-series-data-lead-and-lag",
    "title": "Transforming Ag data in R II",
    "section": "2.5 Working with Time-Series Data: lead() and lag()",
    "text": "2.5 Working with Time-Series Data: lead() and lag()\n\ndata_lag &lt;- data_corn %&gt;% \n  arrange(year, topo) %&gt;% \n  mutate(yield_change = yield - lag(yield))\nhead(data_lag)\n\n  year       lat      long yield nitro topo     bv rep nf yield_change\n1 1999 -33.05174 -63.84532 59.94 131.5    E 182.12  R1 N5           NA\n2 1999 -33.05175 -63.84525 58.96 131.5    E 182.57  R1 N5        -0.98\n3 1999 -33.05176 -63.84518 61.77 131.5    E 178.07  R1 N5         2.81\n4 1999 -33.05178 -63.84511 66.41 131.5    E 177.83  R1 N5         4.64\n5 1999 -33.05179 -63.84504 66.06 131.5    E 176.17  R1 N5        -0.35\n6 1999 -33.05180 -63.84497 62.13 131.5    E 176.56  R1 N5        -3.93",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#gathering-and-spreading-data",
    "href": "coding/week_03/05_datawrangling_02.html#gathering-and-spreading-data",
    "title": "Transforming Ag data in R II",
    "section": "3.1 Gathering and Spreading Data",
    "text": "3.1 Gathering and Spreading Data\n\n# Convert from wide to long format using pivot_longer\nlong_data &lt;- data_corn %&gt;% \n  pivot_longer(cols = c(yield, nitro), \n               names_to = \"measurement\", # name of the column with description\n               values_to = \"value\") # name of the column with values\nhead(long_data)\n\n# A tibble: 6 × 9\n   year   lat  long topo     bv rep   nf    measurement value\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;       &lt;dbl&gt;\n1  1999 -33.1 -63.8 W      163. R1    N5    yield        72.1\n2  1999 -33.1 -63.8 W      163. R1    N5    nitro       132. \n3  1999 -33.1 -63.8 W      170. R1    N5    yield        73.8\n4  1999 -33.1 -63.8 W      170. R1    N5    nitro       132. \n5  1999 -33.1 -63.8 W      168. R1    N5    yield        77.2\n6  1999 -33.1 -63.8 W      168. R1    N5    nitro       132. \n\n# Convert back from long to wide format using pivot_wider\nwide_data &lt;- long_data %&gt;% \n  pivot_wider(names_from = measurement, \n              values_from = value)\nhead(wide_data)\n\n# A tibble: 6 × 9\n   year   lat  long topo     bv rep   nf    yield nitro\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1999 -33.1 -63.8 W      163. R1    N5     72.1  132.\n2  1999 -33.1 -63.8 W      170. R1    N5     73.8  132.\n3  1999 -33.1 -63.8 W      168. R1    N5     77.2  132.\n4  1999 -33.1 -63.8 W      177. R1    N5     76.4  132.\n5  1999 -33.1 -63.8 W      171. R1    N5     75.6  132.\n6  1999 -33.1 -63.8 W      171. R1    N5     70.2  132.",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#separating-and-uniting-columns",
    "href": "coding/week_03/05_datawrangling_02.html#separating-and-uniting-columns",
    "title": "Transforming Ag data in R II",
    "section": "3.2 Separating and Uniting Columns",
    "text": "3.2 Separating and Uniting Columns\n\n# Example dataset with a combined column\nexample_data &lt;- data_corn %&gt;% \n  mutate(topo_year = paste(topo, year, sep = \"_\"))\n\n# Splitting 'topo_year' into two columns\nseparated_data &lt;- example_data %&gt;% \n  separate(topo_year, into = c(\"topo\", \"year\"), sep = \"_\")\nhead(separated_data)\n\n        lat      long yield nitro     bv rep nf topo year\n1 -33.05113 -63.84886 72.14 131.5 162.60  R1 N5    W 1999\n2 -33.05115 -63.84879 73.79 131.5 170.49  R1 N5    W 1999\n3 -33.05116 -63.84872 77.25 131.5 168.39  R1 N5    W 1999\n4 -33.05117 -63.84865 76.35 131.5 176.68  R1 N5    W 1999\n5 -33.05118 -63.84858 75.55 131.5 171.46  R1 N5    W 1999\n6 -33.05120 -63.84851 70.24 131.5 170.56  R1 N5    W 1999\n\n# Combining 'topo' and 'year' back into a single column\nunited_data &lt;- separated_data %&gt;% \n  unite(\"topo_year\", topo, year, sep = \"-\")\nhead(united_data)\n\n        lat      long yield nitro     bv rep nf topo_year\n1 -33.05113 -63.84886 72.14 131.5 162.60  R1 N5    W-1999\n2 -33.05115 -63.84879 73.79 131.5 170.49  R1 N5    W-1999\n3 -33.05116 -63.84872 77.25 131.5 168.39  R1 N5    W-1999\n4 -33.05117 -63.84865 76.35 131.5 176.68  R1 N5    W-1999\n5 -33.05118 -63.84858 75.55 131.5 171.46  R1 N5    W-1999\n6 -33.05120 -63.84851 70.24 131.5 170.56  R1 N5    W-1999",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#nesting-and-unnesting-data",
    "href": "coding/week_03/05_datawrangling_02.html#nesting-and-unnesting-data",
    "title": "Transforming Ag data in R II",
    "section": "3.3 Nesting and Unnesting Data",
    "text": "3.3 Nesting and Unnesting Data\n\nnested_data &lt;- data_corn %&gt;% \n  group_by(topo) %&gt;% \n  nest()\nhead(nested_data)\n\n# A tibble: 4 × 2\n# Groups:   topo [4]\n  topo  data                \n  &lt;fct&gt; &lt;list&gt;              \n1 W     &lt;tibble [1,043 × 8]&gt;\n2 HT    &lt;tibble [785 × 8]&gt;  \n3 E     &lt;tibble [730 × 8]&gt;  \n4 LO    &lt;tibble [885 × 8]&gt;  \n\nunnested_data &lt;- nested_data %&gt;% \n  unnest(cols = c(data))\nhead(unnested_data)\n\n# A tibble: 6 × 9\n# Groups:   topo [1]\n  topo   year   lat  long yield nitro    bv rep   nf   \n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 W      1999 -33.1 -63.8  72.1  132.  163. R1    N5   \n2 W      1999 -33.1 -63.8  73.8  132.  170. R1    N5   \n3 W      1999 -33.1 -63.8  77.2  132.  168. R1    N5   \n4 W      1999 -33.1 -63.8  76.4  132.  177. R1    N5   \n5 W      1999 -33.1 -63.8  75.6  132.  171. R1    N5   \n6 W      1999 -33.1 -63.8  70.2  132.  171. R1    N5",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#detecting-and-extracting-strings",
    "href": "coding/week_03/05_datawrangling_02.html#detecting-and-extracting-strings",
    "title": "Transforming Ag data in R II",
    "section": "4.1 Detecting and Extracting Strings",
    "text": "4.1 Detecting and Extracting Strings\n\nnames &lt;- c(\"Wheat Field\", \"Corn Field\", \"Soybean Farm\")\nstr_detect(names, \"Field\") # Check if 'Field' is present\n\n[1]  TRUE  TRUE FALSE\n\nstr_subset(names, \"Corn\") # Extract values containing 'Corn'\n\n[1] \"Corn Field\"",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#modifying-strings",
    "href": "coding/week_03/05_datawrangling_02.html#modifying-strings",
    "title": "Transforming Ag data in R II",
    "section": "4.2 Modifying Strings",
    "text": "4.2 Modifying Strings\n\nnames &lt;- str_replace(names, \"Field\", \"Plot\")\nnames\n\n[1] \"Wheat Plot\"   \"Corn Plot\"    \"Soybean Farm\"\n\ncapitalized_names &lt;- str_to_title(names)\nhead(capitalized_names)\n\n[1] \"Wheat Plot\"   \"Corn Plot\"    \"Soybean Farm\"\n\ndata_clean &lt;- data_corn %&gt;% \n  mutate(topo_clean = str_replace_all(topo, \"[^a-zA-Z0-9]\", \"_\"))\n\nhead(data_clean)\n\n  year       lat      long yield nitro topo     bv rep nf topo_clean\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5          W\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5          W\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5          W\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5          W\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5          W\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5          W",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#splitting-strings",
    "href": "coding/week_03/05_datawrangling_02.html#splitting-strings",
    "title": "Transforming Ag data in R II",
    "section": "4.3 Splitting Strings",
    "text": "4.3 Splitting Strings\n\nwords &lt;- \"Wheat,Corn,Soybean\"\nsplit_words &lt;- str_split(words, \",\")\nhead(split_words)\n\n[[1]]\n[1] \"Wheat\"   \"Corn\"    \"Soybean\"",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#reordering-factors",
    "href": "coding/week_03/05_datawrangling_02.html#reordering-factors",
    "title": "Transforming Ag data in R II",
    "section": "5.1 Reordering Factors",
    "text": "5.1 Reordering Factors\n\nlibrary(forcats)\ncrops &lt;- factor(c(\"soybean\", \"corn\", \"wheat\"), levels = c(\"wheat\", \"corn\", \"soybean\"))\ncrops &lt;- fct_relevel(crops, \"corn\") # Moves 'corn' to first position\nhead(crops)\n\n[1] soybean corn    wheat  \nLevels: corn wheat soybean",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#lump-rare-categories-together",
    "href": "coding/week_03/05_datawrangling_02.html#lump-rare-categories-together",
    "title": "Transforming Ag data in R II",
    "section": "5.2 Lump Rare Categories Together",
    "text": "5.2 Lump Rare Categories Together\n\nset.seed(123)\ndata &lt;- data.frame(crop = sample(c(\"corn\", \"soybean\", \"wheat\", \"barley\", \"oats\"), 20, replace = TRUE))\n\n# Using mutate() to lump rare categories together\nfactor_data &lt;- data %&gt;%\n  mutate(crop_lumped = fct_lump_n(crop, n = 3)) # Keep top 3 categories, lump others into 'Other'\nfactor_data\n\n      crop crop_lumped\n1    wheat       wheat\n2    wheat       wheat\n3  soybean     soybean\n4  soybean     soybean\n5    wheat       wheat\n6     oats       Other\n7   barley       Other\n8     corn        corn\n9  soybean     soybean\n10   wheat       wheat\n11    oats       Other\n12   wheat       wheat\n13   wheat       wheat\n14    corn        corn\n15  barley       Other\n16    corn        corn\n17    corn        corn\n18    oats       Other\n19   wheat       wheat\n20 soybean     soybean",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#extracting-date-components",
    "href": "coding/week_03/05_datawrangling_02.html#extracting-date-components",
    "title": "Transforming Ag data in R II",
    "section": "6.1 Extracting Date Components",
    "text": "6.1 Extracting Date Components\n\nyear(parsed_dates)\n\n[1] 2023 2024 2025\n\nmonth(parsed_dates)\n\n[1] 6 1 7\n\nday(parsed_dates)\n\n[1] 15 20  4",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_03/05_datawrangling_02.html#parsing-and-extracting-date-components",
    "href": "coding/week_03/05_datawrangling_02.html#parsing-and-extracting-date-components",
    "title": "Transforming Ag data in R II",
    "section": "6.2 Parsing and Extracting Date Components",
    "text": "6.2 Parsing and Extracting Date Components\n\ndates_corn &lt;- data_corn %&gt;% \n  mutate(date = ymd(paste(year, \"01\", \"01\", sep = \"-\")))\nhead(dates_corn)\n\n  year       lat      long yield nitro topo     bv rep nf       date\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5 1999-01-01\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5 1999-01-01\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5 1999-01-01\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5 1999-01-01\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5 1999-01-01\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5 1999-01-01",
    "crumbs": [
      "Lessons",
      "05-Data Wrangling II"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html",
    "href": "coding/week_02/03_versioncontrol.html",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "",
    "text": "Have you ever struggled with:\n\nMessy file versions? 📂 Naming files like code.R, code_final.R, code_final_final.R, codefinal3.R, or code_seethis.R? 🤯\nLost in email threads? 📧 Collaborating with colleagues by sending multiple file versions via email, leading to confusion and mistakes?\nUntracked changes? 🔄 Making edits but forgetting what changed and why?\n\nIf so, you’re not alone! These are common challenges in agriculture data science and research projects. Fortunately, Git and GitHub provide an efficient way to manage and collaborate on code without the chaos.",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html#install-git",
    "href": "coding/week_02/03_versioncontrol.html#install-git",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "5.1 1️⃣ Install Git",
    "text": "5.1 1️⃣ Install Git\nGit is a version control system that runs locally on your computer. To install it:\n\n📥 Download Git from git-scm.com\nFollow the installation instructions for your OS (Windows, macOS, Linux)\nOpen RStudio, go to the “Terminal” tab,\nVerify the installation by running this line:\n\ngit --version",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html#create-a-github-account",
    "href": "coding/week_02/03_versioncontrol.html#create-a-github-account",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "5.2 2️⃣ Create a GitHub Account",
    "text": "5.2 2️⃣ Create a GitHub Account\nGitHub is a cloud-based platform that stores your Git repositories online.\n\n🌍 Go to GitHub and sign up.\nSet up your GitHub profile and SSH key (optional but recommended for authentication).",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html#creating-a-github-repository",
    "href": "coding/week_02/03_versioncontrol.html#creating-a-github-repository",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "9.1 ✅ Creating a GitHub Repository",
    "text": "9.1 ✅ Creating a GitHub Repository\n\nOpen GitHub and click New Repository ➕.\nName the repository (e.g., my-first-repo) and choose Public or Private.\nClick Create Repository 🚀.",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html#creating-an-r-script-and-committing-changes",
    "href": "coding/week_02/03_versioncontrol.html#creating-an-r-script-and-committing-changes",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "9.2 📂 Creating an R Script and Committing Changes",
    "text": "9.2 📂 Creating an R Script and Committing Changes\n\nOpen GitHub Desktop and click Clone a Repository.\nSelect your repository and choose a local directory.\nOpen RStudio and create a new file named hello.R.\nAdd the following content:\nprint(\"Hello R world!\")\nSave the file and return to GitHub Desktop.\nYou should see the file listed under Changes.\nWrite a commit message (e.g., “Added hello.R”) and click Commit to main.\nClick Push Origin to upload the changes to GitHub.",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_02/03_versioncontrol.html#cloning-or-pulling-changes",
    "href": "coding/week_02/03_versioncontrol.html#cloning-or-pulling-changes",
    "title": "🌱 Getting Started with Git and GitHub for Ag Data Science",
    "section": "9.3 🔄 Cloning or Pulling Changes",
    "text": "9.3 🔄 Cloning or Pulling Changes\n\nIf you want to work on an existing repository, click Clone Repository in GitHub Desktop.\nSelect a repository and choose a local directory.\nIf changes have been made remotely, click Fetch Origin and then Pull Origin to update your local version.",
    "crumbs": [
      "Lessons",
      "03-Git & GitHub"
    ]
  },
  {
    "objectID": "coding/week_05/08_ggplot2_adv.html",
    "href": "coding/week_05/08_ggplot2_adv.html",
    "title": "Data Viz II",
    "section": "",
    "text": "Description\nIn this session, we’ll explore advanced visualization techniques using ggplot2, focusing on secondary axes, polygons for data segmentation, and mathematical annotations. These skills will help you create more dynamic and insightful visual representations.",
    "crumbs": [
      "Lessons",
      "08-Data viz II"
    ]
  },
  {
    "objectID": "coding/week_05/08_ggplot2_adv.html#explanation",
    "href": "coding/week_05/08_ggplot2_adv.html#explanation",
    "title": "Data Viz II",
    "section": "2.1 Explanation:",
    "text": "2.1 Explanation:\n\nPrimary Y-axis: Displays precipitation in mm.\nSecondary Y-axis: Converts temperature to °C using a conversion factor.\nAnnotations: Temperature and precipitation values are labeled for clarity.",
    "crumbs": [
      "Lessons",
      "08-Data viz II"
    ]
  },
  {
    "objectID": "coding/week_05/08_ggplot2_adv.html#explanation-1",
    "href": "coding/week_05/08_ggplot2_adv.html#explanation-1",
    "title": "Data Viz II",
    "section": "3.1 Explanation:",
    "text": "3.1 Explanation:\n\nannotate(\"rect\"): Creates colored polygons for each quadrant.\nLabels: Descriptive labels are added to clarify each region.",
    "crumbs": [
      "Lessons",
      "08-Data viz II"
    ]
  },
  {
    "objectID": "coding/week_05/08_ggplot2_adv.html#explanation-2",
    "href": "coding/week_05/08_ggplot2_adv.html#explanation-2",
    "title": "Data Viz II",
    "section": "4.1 Explanation:",
    "text": "4.1 Explanation:\n\nlm() fits a linear model to the data.\nannotate() displays the regression equation on the plot.",
    "crumbs": [
      "Lessons",
      "08-Data viz II"
    ]
  },
  {
    "objectID": "coding/week_12/quarto.html",
    "href": "coding/week_12/quarto.html",
    "title": "Quarto Tips & Tricks",
    "section": "",
    "text": "Quarto is a powerful tool for creating dynamic documents, and it supports multiple output formats, including HTML and PDF. This article presents essential tips and tricks to help you customize and optimize your Quarto documents effectively.",
    "crumbs": [
      "Lessons",
      "19-Quarto tips"
    ]
  },
  {
    "objectID": "coding/week_12/quarto.html#rmarkdown-syntax",
    "href": "coding/week_12/quarto.html#rmarkdown-syntax",
    "title": "Quarto Tips & Tricks",
    "section": "\n7.1 Rmarkdown syntax",
    "text": "7.1 Rmarkdown syntax\n‘{r mtcars-plot1, fig.width=6, fig.height=4, warning=FALSE, message=FALSE}’\n\nShow codelibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "19-Quarto tips"
    ]
  },
  {
    "objectID": "coding/week_12/quarto.html#new-quarto-syntax",
    "href": "coding/week_12/quarto.html#new-quarto-syntax",
    "title": "Quarto Tips & Tricks",
    "section": "\n7.2 New quarto syntax",
    "text": "7.2 New quarto syntax\nWithin the chunk after closing the braces ‘{}’\n\n#| echo: true\n#| warning: false\n#| message: false\n#| fig-width: 6\n#| fig-height: 4\n\n\nShow codeggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  theme_classic()",
    "crumbs": [
      "Lessons",
      "19-Quarto tips"
    ]
  },
  {
    "objectID": "coding/week_12/quarto.html#from-file",
    "href": "coding/week_12/quarto.html#from-file",
    "title": "Quarto Tips & Tricks",
    "section": "\n11.1 From file",
    "text": "11.1 From file",
    "crumbs": [
      "Lessons",
      "19-Quarto tips"
    ]
  },
  {
    "objectID": "coding/week_12/quarto.html#from-url",
    "href": "coding/week_12/quarto.html#from-url",
    "title": "Quarto Tips & Tricks",
    "section": "\n11.2 From URL",
    "text": "11.2 From URL",
    "crumbs": [
      "Lessons",
      "19-Quarto tips"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html",
    "href": "coding/week_13/bayes_02.html",
    "title": "Bayesian Statistics in R",
    "section": "",
    "text": "📌 Today’s Topics\n\n\n\nWe’ll learn how to compute posterior distributions, step-by-step:\n\n🎯 Acceptance/Rejection Sampling (AR Sampling)\n🔁 Markov Chain Monte Carlo (MCMC) — more efficient than AR!\n\nAnd introduce powerful R packages for Bayesian modeling:\n\n📦 brms — beginner-friendly interface to Stan\n🔬 rstan — write your own Stan models\n🧪 rjags — Gibbs sampling with BUGS syntax",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#packages-to-use-today",
    "href": "coding/week_13/bayes_02.html#packages-to-use-today",
    "title": "Bayesian Statistics in R",
    "section": "1 Packages to use today",
    "text": "1 Packages to use today\n\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#computing-posterior-distributions",
    "href": "coding/week_13/bayes_02.html#computing-posterior-distributions",
    "title": "Bayesian Statistics in R",
    "section": "2 🎲 Computing Posterior Distributions",
    "text": "2 🎲 Computing Posterior Distributions\n\n2.1 1️⃣ Acceptance/Rejection Sampling — Basics\nHere’s how AR sampling works:\n\nPropose values for parameters\nSimulate data based on those values\nMeasure how well it fits the observed data\nAccept if close enough (✔️), reject otherwise (❌)\n\n\n2.1.1 🌽 Simulating Corn Yield vs. Plant Density\nWe simulate yield using a parabolic function:\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2 \\]\nThen compare simulated data to the real observed values. If the “fit” is good enough, we keep those parameter values.\n👀 We’ll visualize which parameter sets are accepted and which aren’t!\n\n\n\n\n\n\n\n\n\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2\\]\n\nGenerate proposal parameter values using the prior ditributions:\n\n\\[\\beta_0 \\sim uniform(4, 6)\\]\n\\[\\beta_1 \\sim uniform(1, 3)\\]\n\\[\\beta_2 \\sim uniform(0.1, 2)\\]\n\\[\\sigma \\sim Gamma(2, 2)\\]\n\nset.seed(6767)\nb0_try &lt;- runif(1, 4, 6)  # Parameter model for intercept (Uniform)\nb1_try &lt;- runif(1, 1, 3)  # Parameter model for slope (Uniform)\nb2_try &lt;- rgamma(1, 0.1, 2) # Parameter model for quadratic term (Gamma)\n# Mathematical equation for process model\nmu_try &lt;- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try &lt;- rgamma(1, 2, 2)\n\n\nGenerate data with those parameters\n\n\n\nset.seed(567)\ny_try &lt;- rnorm(n, mu_try, sigma_try)  # Process model\n\n\nCompare the simulated data with the observed data = “difference”\n\n\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] &lt;- sum(abs(y - y_try))\n\n\n“Accept” (gold) that combination of parameters if the difference &lt; predifined acceptable error. “Reject” (red) if the difference &gt; predifined acceptable error.\n\n\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Corn Yield (Mg/ha)\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] &lt; error, \"gold\", \"tomato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, what if whe change the priors:\n\n\n\n\n\n\n\n\n\nNow, do many tries\n\nfor (k in 1:K_tries) {\n    \n    b0_try &lt;- runif(1, 2, 10)  # Parameter model for intercept as uniform\n    b1_try &lt;- rnorm(1, 2.2, .5)  # Parameter model for slope as normal or gaussian\n    b2_try &lt;- rgamma(1, .25, 2) # Parameter model for quad term as gamma\n    # Mathematical equation for process model\n    mu_try &lt;- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try &lt;- rgamma(1, 2, 2)\n\n    y_try &lt;- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] &lt;- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] &lt;- y_try\n    \n    posterior_samp_parameters[k, ] &lt;- c(b0_try, b1_try, b2_try, sigma_try)\n}\n\nAcceptance rate\n\nlength(which(diff &lt; error))/K_tries\n\n[1] 0.169531\n\n\nPriors versus posteriors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 📊 Plot predictions\n\n# Prepare data\nfiltered_yhat &lt;- y_hat[which(diff &lt; error), 50]\ndf_yhat &lt;- data.frame(pred = filtered_yhat)\n\n# Plot\nggplot(df_yhat, aes(x = pred)) +\n  # Posterior\n  geom_histogram(aes(y = ..density..), fill = \"grey\", color = \"black\", bins = 30) +\n  geom_vline(xintercept = y[50], color = \"gold\", linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    x = expression(hat(y)[50]),\n    y = \"Density\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s get started",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#markov-chain-monte-carlo",
    "href": "coding/week_13/bayes_02.html#markov-chain-monte-carlo",
    "title": "Bayesian Statistics in R",
    "section": "3 Markov Chain Monte Carlo",
    "text": "3 Markov Chain Monte Carlo\n\nMarkov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics (Brooks et al., 2011).\nIn a nutshell, MCMC represents a family of algorithms that facilitate the generation of random samples from probability distributions that are difficult (e.g. high-dimensional) to sample directly. They are “chains” because the random samples are produced in consecutive-dependent steps (i.e. step 2 comes from step 1, step 3 comes from step 2, ….). This details is a game-changer to more efficiently use and integrate Monte Carlos simulations.\nSources on MCMC:\nhttps://www.mcmchandbook.net/\nhttps://cran.r-project.org/package=MCMCpack\nhttps://cran.r-project.org/package=mcmc\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#brms-bayesian-regression-models-using-stan",
    "href": "coding/week_13/bayes_02.html#brms-bayesian-regression-models-using-stan",
    "title": "Bayesian Statistics in R",
    "section": "4 brms: Bayesian Regression Models using “Stan”",
    "text": "4 brms: Bayesian Regression Models using “Stan”\n\nDocumentation: https://paul-buerkner.github.io/brms/\nBug-reports: https://github.com/paul-buerkner/brms/issues\nbrms is a very handy R-package that facilitates running Bayesian models using a relatively simple syntax. It is basically and interface that runs “Stan” behind the scenes. It uses a syntax quite similar to the lme4 package.\nIt allows to use several different type of distributions and link functions for models that are linear, counts, survival, response, ordinal, zero-inflated, etc.\nDue to its relatively simple syntax, today, we are going to start our Bayesian coding with brms.\nMore about brms at https://www.jstatsoft.org/article/view/v080i01\n\n\n4.1 Fit brms\nLet’s fit the example using the brms package.\n\n\n4.2 brms pars\n\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n4.3 Model\n\n#| eval: false\n#| echo: true\n\n# 01. Run models\n\nbayes_model &lt;- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\n#bayes_model &lt;- readRDS(file = \"bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n\n\n\n\n\n\n\n# Visualize model results\nbayes_model\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 61) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     5.42      0.27     4.89     5.95 1.00     2664     2753\nB1_Intercept     2.04      0.10     1.82     2.24 1.00     2719     2502\nB2_Intercept     0.11      0.01     0.10     0.13 1.00     2764     2641\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.73      0.07     0.61     0.88 1.00     2997     2715\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nling)\nChain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 4: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 3: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 3: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.389 seconds (Warm-up)\nChain 3:                1.471 seconds (Sampling)\nChain 3:                1.86 seconds (Total)\nChain 3: \nChain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.524 seconds (Warm-up)\nChain 2:                1.613 seconds (Sampling)\nChain 2:                2.137 seconds (Total)\nChain 2: \nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.39 seconds (Warm-up)\nChain 1:                1.826 seconds (Sampling)\nChain 1:                2.216 seconds (Total)\nChain 1: \nChain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.434 seconds (Warm-up)\nChain 4:                2.192 seconds (Sampling)\nChain 4:                2.626 seconds (Total)\nChain 4: \n\n\n\n4.3.1 Compare vs traditional linear model (lm)\n\ndata_frame_q &lt;- data_frame %&gt;% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n      5.415        2.038       -0.114  \n\n\n\n\n\n4.4 Using posterior distributions\n\n4.4.1 Prepare summary\n\n# Create predictions\nm1 &lt;- data_frame %&gt;% \n  ungroup() %&gt;% \n  dplyr::select(x) %&gt;% \n  group_by(x) %&gt;% filter(x == max(x)) %&gt;% \n  ungroup() %&gt;% unique() %&gt;% rename(max = x) %&gt;% \n  # Generate a sequence of x values\n  mutate(data = max %&gt;% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %&gt;% \n  unnest() %&gt;% dplyr::select(-max) %&gt;%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %&gt;% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %&gt;% ungroup()\n\n# Summarize\nm1_quantiles &lt;- m1 %&gt;% \n  group_by(x) %&gt;% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n\n\n4.4.2 Plot posterior\n\n# Plot\nm1_plot &lt;- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk1\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"gold3\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T)+\n  scale_color_viridis_d()+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_bw()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rstan-r-interface-to-stan",
    "href": "coding/week_13/bayes_02.html#rstan-r-interface-to-stan",
    "title": "Bayesian Statistics in R",
    "section": "5 rstan: R interface to “Stan”",
    "text": "5 rstan: R interface to “Stan”\n\nDocumentation: https://mc-stan.org/rstan/\nBug reports: https://github.com/stan-dev/rstan/issues/\nstan is a stand-alone open-source software platform designed for statistical modeling using high-performance statistical computation applying its own language. When selecting the Bayesian computational approach (i.e. rejection sampling criteria) there are several alternatives to choose. Stan produces Bayesian statistical inference following Hamiltonian Monte Carlo (HMC), and No-U-Turn Samples (NUTS). Besides R, stan has interfaces with other popular languages such as Python, MATLAB, Julia.\nIn contrast to brms, stan’s syntax is more complicated for begginers, but the positive side is that requires us to write the statistical model.\nWe will not fit a model directly with stan today, but brms brings a function that allows users to obtain the code to run the analysis by ourselves using rstan. Let’s see…",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rjags-r-interface-to-just-another-gibbs-sampler",
    "href": "coding/week_13/bayes_02.html#rjags-r-interface-to-just-another-gibbs-sampler",
    "title": "Bayesian Statistics in R",
    "section": "6 rjags: R interface to “Just Another Gibbs Sampler”",
    "text": "6 rjags: R interface to “Just Another Gibbs Sampler”\n\nDocumentation: https://mcmc-jags.sourceforge.io/\nBug reports: https://sourceforge.net/projects/mcmc-jags/\nrjags is another popular option for Bayesian statistical inference following MCMC using R. Rjags produces Bayesian statistical inference following BUGS language (WinBUGS). Similar to stan, rjags it is probably not for beginner, since it requires us to write out the statistical model (although it is always ideal). To extract the posteriors, it also requires coda, which is especially designed for summarizing and plotting MCMC simulations.",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html",
    "href": "coding/week_09/models_04.html",
    "title": "Models IV: Mixed Models I",
    "section": "",
    "text": "After planning the experimental design, identifying dependent variable, independent variable(s), conducting the experiment, and collecting the data…the expected path would be as follows:\n\nflowchart LR\n  A[Dig the data] --&gt; B{Model Selection}\n  B --&gt; C[Significant\\nEffects?]\n  subgraph Inference\n  C --&gt;|Yes| D[Comparisons]\n  end\n\n\n\n\nflowchart LR\n  A[Dig the data] --&gt; B{Model Selection}\n  B --&gt; C[Significant\\nEffects?]\n  subgraph Inference\n  C --&gt;|Yes| D[Comparisons]\n  end",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#corn-data",
    "href": "coding/week_09/models_04.html#corn-data",
    "title": "Models IV: Mixed Models I",
    "section": "4.1 Corn Data",
    "text": "4.1 Corn Data\n\ndata_corn &lt;- agridat::lasrosas.corn\n\nFirst, the most important step is ALWAYS to write down the model.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#block-as-fixed",
    "href": "coding/week_09/models_04.html#block-as-fixed",
    "title": "Models IV: Mixed Models I",
    "section": "4.2 Block as Fixed",
    "text": "4.2 Block as Fixed\nIn a traditional approach blocks are defined as fixed, affecting the mean of the expected value. Yet there is no consensus about treating blocks as fixed or as random. For more information, read Dixon (2016).\nLet’s define the model. For simplification (and avoid writing interaction terms), here we are going to consider that \\(\\tau_i\\) is the “treatment”.\n\\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\]\n\\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] where \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\n\n# SIMPLEST MODEL\nblock_fixed &lt;- lm(\n  # Response variable\n  yield ~ \n    # Fixed\n    nf + rep,\n  # Data\n  data = data_corn)\n\ncar::Anova(block_fixed, type=3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 1800690    1 4654.7170 &lt; 2.2e-16 ***\nnf            23987    5   12.4012 6.009e-12 ***\nrep            1271    2    1.6429    0.1936    \nResiduals   1328839 3435                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#block-as-random",
    "href": "coding/week_09/models_04.html#block-as-random",
    "title": "Models IV: Mixed Models I",
    "section": "4.3 Block as Random",
    "text": "4.3 Block as Random\nAn alternative approach is considering a MIXED MODEL, where blocks are considered “random”. Basically, we add a term to the model that it is expected to show a “null” overall effect over the mean of the variable of interest but introduces “noise”. By convention, a random effect is expected to have an expected value equal to zero but a positive variance as follows: \\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\] \\[ \\beta_j \\sim N(0, \\sigma^2_{b} )\\] \\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] Similar than before, \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the “random” effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\nSo what’s the difference? Simply specifying this component: \\[ \\beta_j \\sim N(0, \\sigma^2_b) \\], which serves to model the variance.\nHow do we write that?\n\n4.3.1 nlme\n\n# RANDOM BLOCK\nblock_random_nlme &lt;- \n  nlme::lme(\n    # Fixed\n    yield ~ nf,\n    # Random\n    random = ~1|rep,\n    # Data\n    data = data_corn)\n\n# ANOVA\ncar::Anova(block_random_nlme, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n               Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5648.401  1  &lt; 2.2e-16 ***\nnf            62.005  5  4.677e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.3.2 lme4\n\n# RANDOM BLOCK\nblock_random_lme4 &lt;- \n  lme4::lmer(# Response variable\n                   yield ~ \n                   # Fixed (Removing intercept? Why?)\n                   nf +\n                   # Random\n                   (1|rep), \n                   # Data\n                   data=data_corn)\n\n# ANOVA\ncar::Anova(block_random_lme4, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n               Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5648.406  1  &lt; 2.2e-16 ***\nnf            62.005  5  4.677e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#create-a-split-plot-design",
    "href": "coding/week_09/models_04.html#create-a-split-plot-design",
    "title": "Models IV: Mixed Models I",
    "section": "5.1 Create a split-plot design",
    "text": "5.1 Create a split-plot design\nThe agricolae package (Mendiburu 2021) brings a set of very useful functions to generate different experimental designs, among them, the split-plot. Let’s see an example:\n\n# Example with agricolae\n\n# Define plots\nmainplot &lt;- c(0,50,80,110,140)\nsubplot &lt;- c(\"Var_1\", \"Var_2\", \"Var_3\")\n\n# Produce\nsp_design &lt;- agricolae::design.split(\n                        trt1 = mainplot, trt2 = subplot, \n                        design = \"rcbd\", r = 3, \n                        randomization = TRUE, seed = 4561)\n\n#View(sp_design)\n\n#View(sp_design$book)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#split-plot-as-a-mixed-model",
    "href": "coding/week_09/models_04.html#split-plot-as-a-mixed-model",
    "title": "Models IV: Mixed Models I",
    "section": "5.2 Split-Plot as a Mixed Model",
    "text": "5.2 Split-Plot as a Mixed Model\nWhen moving to a Split-Plot design, an additional source of error is introduced: Main plot error (^2_m), which accounts for variation among the whole plots within each block.\n\\[y_{ijk} = \\mu + \\alpha_i + \\tau_k + (\\alpha\\tau){ik} + \\beta_j + \\gamma{ij} + \\epsilon_{ijk}\\]\nwhere: \\(\\alpha_i\\) = Fixed effect of the main-plot factor (e.g., tillage), \\(\\tau_k\\) = Fixed effect of the subplot factor (e.g., nitrogen rate), \\((\\alpha\\tau)_{ik}\\) = Fixed interaction effect between main and subplot factors, \\(\\beta_j \\sim N(0, \\sigma^2_b)\\) = Random effect of block, \\(\\gamma_{ij} \\sim N(0, \\sigma^2_m)\\) = Random effect of main plot within block (main-plot error), \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_e)\\) = Random error of experimental unit.\nThus, three sources of variability are modeled: 1. Block-to-block variation → \\(\\sigma^2_b\\) 2. Main-plot variation (whole plot error) → \\(\\sigma^2_m\\) 3. Residual variation (subplot error) → \\(\\sigma^2_e\\)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#data",
    "href": "coding/week_09/models_04.html#data",
    "title": "Models IV: Mixed Models I",
    "section": "5.3 Data",
    "text": "5.3 Data\nThe following is just a fake data set where we have a split-splot arrangement within an RCBD (BLOCK), where at the main plot corresponds to nitrogen rate (NRATE, with 5 levels), and the subplot to wheat variety (VARIETY, with 3 levels).\n\n# Read csv\n#split_plot_data_0 &lt;- read.csv(file = \"../data/01_split_plot_data.csv\", header = TRUE)\n\n# File online? Try this...(remove \"#\")\nurl_split &lt;- \"https://raw.githubusercontent.com/adriancorrendo/tidymixedmodelsweb/master/data/01_split_plot_data.csv\"\n\nsplit_plot_data_0 &lt;- read.csv(url_split)\n\n\n# Data hierarchy\nsplit_plot_data &lt;- \n  split_plot_data_0 %&gt;% \n  mutate(NRATE = factor(NRATE)) %&gt;% \n  # Identify Main Plot\n  #mutate(main = factor(BLOCK:NRATE)) %&gt;% \n  dplyr::select(BLOCK, NRATE, VARIETY, YIELD)\n\n#View(split_plot_data)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#explore-the-data",
    "href": "coding/week_09/models_04.html#explore-the-data",
    "title": "Models IV: Mixed Models I",
    "section": "5.4 Explore the data",
    "text": "5.4 Explore the data\nNow, let’s use several functions to explore the data.\n\n5.4.1 glimpse()\nFor example, the glimpse() function from the dplyr package (Wickham et al. 2022) allows to take a quick look to the columns in our data frame (it’s like a transposed version of print())\n\n# Glimpse from dplyr\ndplyr::glimpse(split_plot_data)\n\nRows: 45\nColumns: 4\n$ BLOCK   &lt;chr&gt; \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B…\n$ NRATE   &lt;fct&gt; 0, 0, 0, 50, 50, 50, 80, 80, 80, 110, 110, 110, 140, 140, 140,…\n$ VARIETY &lt;chr&gt; \"Var_1\", \"Var_2\", \"Var_3\", \"Var_1\", \"Var_2\", \"Var_3\", \"Var_1\",…\n$ YIELD   &lt;int&gt; 1938, 3946, 4628, 2038, 4346, 5949, 3837, 4287, 6765, 3466, 49…\n\n\n\n\n5.4.2 skim()\nThen, the skim() function from the skimr package (Waring et al. 2022) allows to take a deeper look to all the variables (columns), creating a quick summary that reports the presence of missing values, etc., etc.\n\n# Skim from skimr\nskimr::skim(split_plot_data)\n\n\nData summary\n\n\nName\nsplit_plot_data\n\n\nNumber of rows\n45\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBLOCK\n0\n1\n2\n2\n0\n3\n0\n\n\nVARIETY\n0\n1\n5\n5\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nNRATE\n0\n1\nFALSE\n5\n0: 9, 50: 9, 80: 9, 110: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYIELD\n0\n1\n4444.02\n1347.72\n1938\n3389\n4458\n5440\n6950\n▅▆▇▆▅\n\n\n\n\n\n\n\n5.4.3 ggplot()\nOf course, we shouldn’t miss to use ggplot2 for a better look\n\n# Boxplot\nsplit_plot_data %&gt;% \n  # Plot\nggplot() + \n  # Boxplots\n  geom_boxplot(aes(x = NRATE, y = YIELD, fill = VARIETY))+\n  geom_jitter(aes(x = NRATE, y = YIELD, fill = VARIETY))+\n  # Plot by site\n  facet_wrap(~VARIETY)+\n  scale_y_continuous(limits = c(0,10000), breaks = seq(0,10000, by=2000))+\n  # Change theme\n  theme_bw()",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#models",
    "href": "coding/week_09/models_04.html#models",
    "title": "Models IV: Mixed Models I",
    "section": "5.5 Models",
    "text": "5.5 Models\nFor the analysis of split-plot designs we basically need to specify an error term that otherwise the model will not see: the MAIN PLOT ERROR TERM (see Venables and Ripley (2002), pg. 283). By default, the random term that the computer will identify is the one happening at the lowest level in the hierarchy (replication). However, we need to specify that the main plot serves as a kind of block to the design.\n\n5.5.1 nlme::lme\n\n# Model without split component\nno_split &lt;- nlme::lme(# Response variable\n                 YIELD ~\n                   # Fixed\n                   0 + NRATE*VARIETY,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|BLOCK, \n                   # Data\n                   data = split_plot_data,\n                   # Method\n                   method = \"REML\")\n\n# Model with split component\nsplit_nlme &lt;- nlme::lme(# Response variable\n                 YIELD ~\n                   # Fixed (Removing intercept? Why?)\n                   0 + NRATE*VARIETY,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|BLOCK/NRATE, \n                   # Data\n                   data = split_plot_data,\n                   # Method\n                   method = \"REML\")\n\n# Type 3 (when interaction is present)\ncar::Anova(split_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: YIELD\n                Chisq Df Pr(&gt;Chisq)    \nNRATE         559.646  5  &lt; 2.2e-16 ***\nVARIETY        22.128  2  1.567e-05 ***\nNRATE:VARIETY  18.117  8    0.02036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Let's see the difference between models in terms of DFs\nsummary(no_split)\n\nLinear mixed-effects model fit by REML\n  Data: split_plot_data \n       AIC     BIC    logLik\n  511.0067 534.827 -238.5033\n\nRandom effects:\n Formula: ~1 | BLOCK\n        (Intercept) Residual\nStdDev:  0.02585478  521.401\n\nFixed effects:  YIELD ~ 0 + NRATE * VARIETY \n                         Value Std.Error DF   t-value p-value\nNRATE0                2536.000  301.0310 28  8.424381  0.0000\nNRATE50               2786.667  301.0310 28  9.257074  0.0000\nNRATE80               3857.667  301.0310 28 12.814847  0.0000\nNRATE110              3467.333  301.0310 28 11.518192  0.0000\nNRATE140              3100.667  301.0310 28 10.300156  0.0000\nVARIETYVar_2           649.667  425.7222 28  1.526034  0.1382\nVARIETYVar_3          1965.333  425.7222 28  4.616469  0.0001\nNRATE50:VARIETYVar_2   603.000  602.0621 28  1.001558  0.3251\nNRATE80:VARIETYVar_2   104.000  602.0621 28  0.172740  0.8641\nNRATE110:VARIETYVar_2  830.667  602.0621 28  1.379703  0.1786\nNRATE140:VARIETYVar_2 1561.000  602.0621 28  2.592756  0.0150\nNRATE50:VARIETYVar_3  1152.333  602.0621 28  1.913978  0.0659\nNRATE80:VARIETYVar_3   763.667  602.0621 28  1.268419  0.2151\nNRATE110:VARIETYVar_3 1033.000  602.0621 28  1.715770  0.0973\nNRATE140:VARIETYVar_3  292.667  602.0621 28  0.486107  0.6307\n Correlation: \n                      NRATE0 NRATE50 NRATE80 NRATE110 NRATE140 VARIETYV_2\nNRATE50                0.000                                             \nNRATE80                0.000  0.000                                      \nNRATE110               0.000  0.000   0.000                              \nNRATE140               0.000  0.000   0.000   0.000                      \nVARIETYVar_2          -0.707  0.000   0.000   0.000    0.000             \nVARIETYVar_3          -0.707  0.000   0.000   0.000    0.000    0.500    \nNRATE50:VARIETYVar_2   0.500 -0.500   0.000   0.000    0.000   -0.707    \nNRATE80:VARIETYVar_2   0.500  0.000  -0.500   0.000    0.000   -0.707    \nNRATE110:VARIETYVar_2  0.500  0.000   0.000  -0.500    0.000   -0.707    \nNRATE140:VARIETYVar_2  0.500  0.000   0.000   0.000   -0.500   -0.707    \nNRATE50:VARIETYVar_3   0.500 -0.500   0.000   0.000    0.000   -0.354    \nNRATE80:VARIETYVar_3   0.500  0.000  -0.500   0.000    0.000   -0.354    \nNRATE110:VARIETYVar_3  0.500  0.000   0.000  -0.500    0.000   -0.354    \nNRATE140:VARIETYVar_3  0.500  0.000   0.000   0.000   -0.500   -0.354    \n                      VARIETYV_3 NRATE50:VARIETYV_2 NRATE80:VARIETYV_2\nNRATE50                                                               \nNRATE80                                                               \nNRATE110                                                              \nNRATE140                                                              \nVARIETYVar_2                                                          \nVARIETYVar_3                                                          \nNRATE50:VARIETYVar_2  -0.354                                          \nNRATE80:VARIETYVar_2  -0.354      0.500                               \nNRATE110:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE140:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE50:VARIETYVar_3  -0.707      0.500              0.250            \nNRATE80:VARIETYVar_3  -0.707      0.250              0.500            \nNRATE110:VARIETYVar_3 -0.707      0.250              0.250            \nNRATE140:VARIETYVar_3 -0.707      0.250              0.250            \n                      NRATE110:VARIETYV_2 NRATE140:VARIETYV_2\nNRATE50                                                      \nNRATE80                                                      \nNRATE110                                                     \nNRATE140                                                     \nVARIETYVar_2                                                 \nVARIETYVar_3                                                 \nNRATE50:VARIETYVar_2                                         \nNRATE80:VARIETYVar_2                                         \nNRATE110:VARIETYVar_2                                        \nNRATE140:VARIETYVar_2  0.500                                 \nNRATE50:VARIETYVar_3   0.250               0.250             \nNRATE80:VARIETYVar_3   0.250               0.250             \nNRATE110:VARIETYVar_3  0.500               0.250             \nNRATE140:VARIETYVar_3  0.250               0.500             \n                      NRATE50:VARIETYV_3 NRATE80:VARIETYV_3 NRATE110:VARIETYV_3\nNRATE50                                                                        \nNRATE80                                                                        \nNRATE110                                                                       \nNRATE140                                                                       \nVARIETYVar_2                                                                   \nVARIETYVar_3                                                                   \nNRATE50:VARIETYVar_2                                                           \nNRATE80:VARIETYVar_2                                                           \nNRATE110:VARIETYVar_2                                                          \nNRATE140:VARIETYVar_2                                                          \nNRATE50:VARIETYVar_3                                                           \nNRATE80:VARIETYVar_3   0.500                                                   \nNRATE110:VARIETYVar_3  0.500              0.500                                \nNRATE140:VARIETYVar_3  0.500              0.500              0.500             \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.9300562 -0.6188455  0.0428333  0.5702584  1.6941534 \n\nNumber of Observations: 45\nNumber of Groups: 3 \n\nsummary(split_nlme)\n\nLinear mixed-effects model fit by REML\n  Data: split_plot_data \n       AIC      BIC    logLik\n  513.0067 538.2282 -238.5033\n\nRandom effects:\n Formula: ~1 | BLOCK\n        (Intercept)\nStdDev:  0.03201185\n\n Formula: ~1 | NRATE %in% BLOCK\n        (Intercept) Residual\nStdDev: 0.006112608  521.401\n\nFixed effects:  YIELD ~ 0 + NRATE * VARIETY \n                         Value Std.Error DF   t-value p-value\nNRATE0                2536.000  301.0310  8  8.424381  0.0000\nNRATE50               2786.667  301.0310  8  9.257074  0.0000\nNRATE80               3857.667  301.0310  8 12.814847  0.0000\nNRATE110              3467.333  301.0310  8 11.518192  0.0000\nNRATE140              3100.667  301.0310  8 10.300156  0.0000\nVARIETYVar_2           649.667  425.7222 21  1.526034  0.1419\nVARIETYVar_3          1965.333  425.7222 21  4.616469  0.0001\nNRATE50:VARIETYVar_2   603.000  602.0621 21  1.001558  0.3280\nNRATE80:VARIETYVar_2   104.000  602.0621 21  0.172740  0.8645\nNRATE110:VARIETYVar_2  830.667  602.0621 21  1.379703  0.1822\nNRATE140:VARIETYVar_2 1561.000  602.0621 21  2.592756  0.0170\nNRATE50:VARIETYVar_3  1152.333  602.0621 21  1.913978  0.0694\nNRATE80:VARIETYVar_3   763.667  602.0621 21  1.268419  0.2185\nNRATE110:VARIETYVar_3 1033.000  602.0621 21  1.715770  0.1009\nNRATE140:VARIETYVar_3  292.667  602.0621 21  0.486107  0.6319\n Correlation: \n                      NRATE0 NRATE50 NRATE80 NRATE110 NRATE140 VARIETYV_2\nNRATE50                0.000                                             \nNRATE80                0.000  0.000                                      \nNRATE110               0.000  0.000   0.000                              \nNRATE140               0.000  0.000   0.000   0.000                      \nVARIETYVar_2          -0.707  0.000   0.000   0.000    0.000             \nVARIETYVar_3          -0.707  0.000   0.000   0.000    0.000    0.500    \nNRATE50:VARIETYVar_2   0.500 -0.500   0.000   0.000    0.000   -0.707    \nNRATE80:VARIETYVar_2   0.500  0.000  -0.500   0.000    0.000   -0.707    \nNRATE110:VARIETYVar_2  0.500  0.000   0.000  -0.500    0.000   -0.707    \nNRATE140:VARIETYVar_2  0.500  0.000   0.000   0.000   -0.500   -0.707    \nNRATE50:VARIETYVar_3   0.500 -0.500   0.000   0.000    0.000   -0.354    \nNRATE80:VARIETYVar_3   0.500  0.000  -0.500   0.000    0.000   -0.354    \nNRATE110:VARIETYVar_3  0.500  0.000   0.000  -0.500    0.000   -0.354    \nNRATE140:VARIETYVar_3  0.500  0.000   0.000   0.000   -0.500   -0.354    \n                      VARIETYV_3 NRATE50:VARIETYV_2 NRATE80:VARIETYV_2\nNRATE50                                                               \nNRATE80                                                               \nNRATE110                                                              \nNRATE140                                                              \nVARIETYVar_2                                                          \nVARIETYVar_3                                                          \nNRATE50:VARIETYVar_2  -0.354                                          \nNRATE80:VARIETYVar_2  -0.354      0.500                               \nNRATE110:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE140:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE50:VARIETYVar_3  -0.707      0.500              0.250            \nNRATE80:VARIETYVar_3  -0.707      0.250              0.500            \nNRATE110:VARIETYVar_3 -0.707      0.250              0.250            \nNRATE140:VARIETYVar_3 -0.707      0.250              0.250            \n                      NRATE110:VARIETYV_2 NRATE140:VARIETYV_2\nNRATE50                                                      \nNRATE80                                                      \nNRATE110                                                     \nNRATE140                                                     \nVARIETYVar_2                                                 \nVARIETYVar_3                                                 \nNRATE50:VARIETYVar_2                                         \nNRATE80:VARIETYVar_2                                         \nNRATE110:VARIETYVar_2                                        \nNRATE140:VARIETYVar_2  0.500                                 \nNRATE50:VARIETYVar_3   0.250               0.250             \nNRATE80:VARIETYVar_3   0.250               0.250             \nNRATE110:VARIETYVar_3  0.500               0.250             \nNRATE140:VARIETYVar_3  0.250               0.500             \n                      NRATE50:VARIETYV_3 NRATE80:VARIETYV_3 NRATE110:VARIETYV_3\nNRATE50                                                                        \nNRATE80                                                                        \nNRATE110                                                                       \nNRATE140                                                                       \nVARIETYVar_2                                                                   \nVARIETYVar_3                                                                   \nNRATE50:VARIETYVar_2                                                           \nNRATE80:VARIETYVar_2                                                           \nNRATE110:VARIETYVar_2                                                          \nNRATE140:VARIETYVar_2                                                          \nNRATE50:VARIETYVar_3                                                           \nNRATE80:VARIETYVar_3   0.500                                                   \nNRATE110:VARIETYVar_3  0.500              0.500                                \nNRATE140:VARIETYVar_3  0.500              0.500              0.500             \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.9300562 -0.6188455  0.0428333  0.5702584  1.6941534 \n\nNumber of Observations: 45\nNumber of Groups: \n           BLOCK NRATE %in% BLOCK \n               3               15 \n\n\n\n\n5.5.2 lmer code (lme4)\n\nsplit_lme4 &lt;- lme4::lmer(# Response variable\n                   YIELD ~ \n                   # Fixed (Removing intercept? Why?)\n                   0+NRATE*VARIETY +\n                   # Random\n                   (1|BLOCK/NRATE), \n                   # Data\n                   data=split_plot_data,\n                   contrasts = list(NRATE = \"contr.sum\", VARIETY = \"contr.sum\"\n                   )\n)\n\n# Alternative (being more explicit with the syntax)\n# split_lme4 &lt;- lme4::lmer(YIELD ~ NRATE * VARIETY + (1|BLOCK) + (1|BLOCK:NRATE), data=split_plot_data)\n\n# Type 3\ncar::Anova(split_lme4, type = 3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: YIELD\n                 Chisq Df Pr(&gt;Chisq)    \nNRATE         3326.392  5    &lt; 2e-16 ***\nVARIETY        188.511  2    &lt; 2e-16 ***\nNRATE:VARIETY   18.117  8    0.02036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Adjusting options for constrasts\noptions(contrasts = c(\"contr.sum\", \"contr.poly\")) # Ensure correct contrasts for Type 3\n\ncar::Anova(split_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: YIELD\n               Chisq Df Pr(&gt;Chisq)    \nNRATE         559.65  5  &lt; 2.2e-16 ***\nVARIETY               0               \nNRATE:VARIETY         0               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#check-assumptions",
    "href": "coding/week_09/models_04.html#check-assumptions",
    "title": "Models IV: Mixed Models I",
    "section": "5.6 Check assumptions",
    "text": "5.6 Check assumptions\n\n# Single tests\nperformance::check_normality(split_lme4)\n\nOK: residuals appear as normally distributed (p = 0.910).\n\nperformance::check_homogeneity(split_lme4)\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.396).\n\nperformance::check_autocorrelation(split_lme4)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.410).\n\nperformance::check_outliers(split_lme4)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\nperformance::check_collinearity(split_lme4)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n  Increased SE  VIF      VIF  CI SE_factor Tolerance\n         NRATE 1.00 [1.00, 1.00]      1.00      1.00\n       VARIETY 1.00 [1.00, 1.00]      1.00      1.00\n NRATE:VARIETY 1.00 [1.00, 1.00]      1.00      1.00\n\n# Check Plots\nperformance::check_model(split_lme4)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#comparisons",
    "href": "coding/week_09/models_04.html#comparisons",
    "title": "Models IV: Mixed Models I",
    "section": "5.7 Comparisons",
    "text": "5.7 Comparisons\n\n# Estimate the comparisons (pairwise)\nsplit_lm4_comparisons &lt;-  \n  split_lme4 %&gt;% \n  # ~ specifies the level of comparison (marginal or interaction)\n  # Since interaction was significant we specify ~ Interaction (Factor1*Factor2)\n  emmeans(., ~ NRATE*VARIETY)\n\n# Add letters\nsplit_lm4_comparisons %&gt;% \n  # Compact Letters Display (cld)\n  cld(., \n      # Specify grouped comparisons by...\n      #by = \"NRATE\", \n      # Order\n      decreasing = TRUE, details=FALSE, reversed=TRUE, \n      # Specs\n      alpha=0.05,  adjust = \"tukey\", Letters=LETTERS)\n\n NRATE VARIETY emmean  SE df lower.CL upper.CL .group  \n 80    Var_3     6587 301 30     5630     7544  A      \n 110   Var_3     6466 301 30     5509     7423  AB     \n 50    Var_3     5904 301 30     4947     6861  ABC    \n 140   Var_3     5359 301 30     4402     6316  ABCD   \n 140   Var_2     5311 301 30     4354     6268  ABCD   \n 110   Var_2     4948 301 30     3991     5905   BCDE  \n 80    Var_2     4611 301 30     3654     5568    CDEF \n 0     Var_3     4501 301 30     3544     5458    CDEF \n 50    Var_2     4039 301 30     3082     4996     DEFG\n 80    Var_1     3858 301 30     2901     4815     DEFG\n 110   Var_1     3467 301 30     2510     4424      EFG\n 0     Var_2     3186 301 30     2229     4143       FG\n 140   Var_1     3101 301 30     2144     4058       FG\n 50    Var_1     2787 301 30     1830     3744        G\n 0     Var_1     2536 301 30     1579     3493        G\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 15 estimates \nP value adjustment: tukey method for comparing a family of 15 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#data-1",
    "href": "coding/week_09/models_04.html#data-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.1 Data",
    "text": "6.1 Data\n\n6.1.1 Details\nAn interesting split-split plot experiment in which the sub-plot treatments have a 2*5 factorial structure.\nAn experiment was conducted in 1932 on the experimental field of the Dominion Rust Research Laboratory. The study was designed to determine the effect on the incidence of root rot, of variety of wheat, kinds of dust for seed treatment, method of application of the dust, and efficacy of soil inoculation with the root-rot organism.\nThe field had 4 blocks.\nEach block has 2 whole plots for the genotypes.\nEach whole-plot had 10 sub-plots for the 5 different kinds of dust and 2 methods of application.\nEach sub-plot had 2 sub-sub-plots, one for inoculated soil and the other one for uninoculated soil.\nC. H. Goulden, (1939). Methods of statistical analysis, 1st ed. Page 18. https://archive.org/stream/methodsofstatist031744mbp\n\nsplitsplit_data &lt;- agridat::goulden.splitsplit\n\nglimpse(splitsplit_data)\n\nRows: 160\nColumns: 9\n$ row   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2…\n$ col   &lt;int&gt; 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 3, 4, 5, 6, 7,…\n$ yield &lt;int&gt; 64, 69, 67, 66, 71, 64, 64, 75, 70, 66, 67, 68, 65, 68, 71, 62, …\n$ inoc  &lt;fct&gt; Uninoc, Uninoc, Uninoc, Inoc, Inoc, Uninoc, Uninoc, Inoc, Inoc, …\n$ trt   &lt;int&gt; 5, 7, 1, 1, 9, 9, 10, 10, 2, 2, 6, 5, 6, 4, 4, 3, 3, 8, 8, 7, 6,…\n$ gen   &lt;fct&gt; Marquis, Marquis, Marquis, Marquis, Marquis, Marquis, Marquis, M…\n$ dry   &lt;fct&gt; Dry, Dry, Dry, Dry, Dry, Dry, Wet, Wet, Wet, Wet, Wet, Dry, Wet,…\n$ dust  &lt;fct&gt; DuBay, Check, Ceresan, Ceresan, CaCO3, CaCO3, CaCO3, CaCO3, Cere…\n$ block &lt;fct&gt; B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, …\n\n# Make sure all factors are not numbers or integers.\nsplitsplit_data &lt;- splitsplit_data %&gt;% \n  mutate(trt = as.factor(trt))",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#models-1",
    "href": "coding/week_09/models_04.html#models-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.2 Models",
    "text": "6.2 Models\n\n6.2.1 nlme\n\n# Model with split component\nsplitsplit_nlme &lt;- nlme::lme(# Response variable\n                 yield ~\n                   # Fixed (Removing intercept? Why?)\n                   0 + gen*trt*inoc,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|block/gen/trt, \n                   # Data\n                   data = splitsplit_data,\n                   # Method\n                   method = \"REML\")\n\n# Type 3 (when interaction is present)\ncar::Anova(splitsplit_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n                 Chisq Df Pr(&gt;Chisq)    \ngen          2202.1234  2  &lt; 2.2e-16 ***\ntrt            58.6809  9  2.405e-09 ***\ninoc           62.7247  1  2.377e-15 ***\ngen:trt        19.6914  9  0.0199156 *  \ngen:inoc        0.0199  1  0.8878172    \ntrt:inoc       30.8330  9  0.0003162 ***\ngen:trt:inoc    8.5127  9  0.4834183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.2.2 lme4\n\nsplitsplit_lme4 &lt;- lme4::lmer(# Response variable\n                   yield ~ \n                   # Fixed (Removing intercept? Why?)\n                   0+gen*trt*inoc +\n                   # Random\n                   (1|block/gen/trt), \n                   # Data\n                   data=splitsplit_data)\n\n# Type 3 (when interaction is present)\ncar::Anova(splitsplit_lme4, type = 3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n                 Chisq Df Pr(&gt;Chisq)    \ngen          2202.1225  2  &lt; 2.2e-16 ***\ntrt            58.6809  9  2.405e-09 ***\ninoc           62.7247  1  2.377e-15 ***\ngen:trt        19.6914  9  0.0199156 *  \ngen:inoc        0.0199  1  0.8878172    \ntrt:inoc       30.8330  9  0.0003162 ***\ngen:trt:inoc    8.5127  9  0.4834183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#comparisons-1",
    "href": "coding/week_09/models_04.html#comparisons-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.3 Comparisons",
    "text": "6.3 Comparisons\n\n# Estimate the comparisons (pairwise)\nsplitsplit_lm4_comparisons &lt;-  \n  splitsplit_lme4 %&gt;% \n  # ~ specifies the level of comparison (marginal or interaction)\n  # Since interaction was significant we specify ~ Interaction (Factor1*Factor2)\n  emmeans(., ~ trt:inoc)\n\n# Add letters\nsplitsplit_lm4_comparisons %&gt;% \n  # Compact Letters Display (cld)\n  cld(., \n      # Specify grouped comparisons by...\n      by = \"trt\", \n      # Order\n      decreasing = TRUE, details=FALSE, reversed=TRUE, \n      # Specs\n      alpha=0.05,  adjust = \"tukey\", Letters=LETTERS)\n\ntrt = 1:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.4 2.03 11.7     58.6     74.1  A    \n Uninoc   64.1 2.03 11.7     56.4     71.9  A    \n\ntrt = 2:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     67.2 2.03 11.7     59.5     75.0  A    \n Uninoc   62.6 2.03 11.7     54.9     70.4   B   \n\ntrt = 3:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.9 2.03 11.7     59.1     74.6  A    \n Uninoc   65.5 2.03 11.7     57.7     73.3  A    \n\ntrt = 4:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.9 2.03 11.7     59.1     74.6  A    \n Uninoc   63.8 2.03 11.7     56.0     71.5  A    \n\ntrt = 5:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.6 2.03 11.7     58.9     74.4  A    \n Uninoc   66.1 2.03 11.7     58.4     73.9  A    \n\ntrt = 6:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     65.1 2.03 11.7     57.4     72.9  A    \n Uninoc   61.1 2.03 11.7     53.4     68.9   B   \n\ntrt = 7:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     77.5 2.03 11.7     69.7     85.3  A    \n Uninoc   67.4 2.03 11.7     59.6     75.1   B   \n\ntrt = 8:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     75.1 2.03 11.7     67.4     82.9  A    \n Uninoc   64.6 2.03 11.7     56.9     72.4   B   \n\ntrt = 9:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     71.5 2.03 11.7     63.7     79.3  A    \n Uninoc   67.8 2.03 11.7     60.0     75.5  A    \n\ntrt = 10:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     72.5 2.03 11.7     64.7     80.3  A    \n Uninoc   63.6 2.03 11.7     55.9     71.4   B   \n\nResults are averaged over the levels of: gen \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 20 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html",
    "href": "coding/week_08/models_02.html",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "",
    "text": "Statistical models in agriculture serve two primary purposes: explanatory and predictive modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.\nThis article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.\nRequired packages:\n\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(emmeans, multcomp, multcompView)\np_load(randomForest, caret, metrica)",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#introduction",
    "href": "coding/week_08/models_02.html#introduction",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "",
    "text": "Statistical models in agriculture serve two primary purposes: explanatory and predictive modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.\nThis article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.\nRequired packages:\n\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(emmeans, multcomp, multcompView)\np_load(randomForest, caret, metrica)",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#explanatory-models",
    "href": "coding/week_08/models_02.html#explanatory-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "2 Explanatory Models",
    "text": "2 Explanatory Models\nExplanatory models are designed to understand how different factors influence a response variable. These models help answer questions such as: What are the main drivers of yield variation? How do nitrogen application and rainfall affect crop performance?\n\n2.1 Example: Linear Regression for Explanation\n\n# Simulated agricultural data\ndata_ag &lt;- data.frame(\n  nitrogen = c(50, 100, 150, 200, 250, 300),\n  rainfall = c(800, 850, 900, 950, 1000, 1050),\n  yield = c(2.5, 3.1, 3.8, 4.2, 4.5, 4.6)\n)\n\n# Fit a linear model\nlm_fit &lt;- lm(yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\n\nResiduals:\nALL 6 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 2.5        NaN     NaN      NaN\nas.factor(nitrogen)100      0.6        NaN     NaN      NaN\nas.factor(nitrogen)150      1.3        NaN     NaN      NaN\nas.factor(nitrogen)200      1.7        NaN     NaN      NaN\nas.factor(nitrogen)250      2.0        NaN     NaN      NaN\nas.factor(nitrogen)300      2.1        NaN     NaN      NaN\nrainfall                     NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 5 and 0 DF,  p-value: NA\n\n\n\n\n2.2 Interpretation\n\nThe coefficients indicate the effect of each predictor on yield.\nThe p-values help determine statistical significance.\nThe R-squared value explains how much variance is accounted for by the model.\n\n\n\n2.3 Means Comparisons\n\n# Perform multiple comparisons\nemmeans_fit &lt;- emmeans(lm_fit, ~ nitrogen)\ncomp &lt;- cld(emmeans_fit, Letters = letters)\ncomp\n\n nitrogen emmean  SE df lower.CL upper.CL .group\n      100    3.1 NaN  0      NaN      NaN       \n      150    3.8 NaN  0      NaN      NaN       \n      200    4.2 NaN  0      NaN      NaN       \n      250    4.5 NaN  0      NaN      NaN       \n       50 nonEst  NA NA       NA       NA       \n      300 nonEst  NA NA       NA       NA       \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 2 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n2.4 Visualization\nWhat are we missing here?\n\n# Create ggplot of estimated means\ncomp_plot &lt;- ggplot(comp, aes(x = as.factor(nitrogen), y = emmean, fill = nitrogen)) +\n  geom_col(color = \"black\") +\n  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.2) +\n  geom_text(aes(label = .group), vjust = -0.5, size = 5) +\n  labs(title = \"Means Comparison for Nitrogen Levels\",\n       x = \"Nitrogen Levels (kg/ha)\",\n       y = \"Estimated Yield (t/ha)\") +\n  theme_minimal()\n\ncomp_plot",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#predictive-models",
    "href": "coding/week_08/models_02.html#predictive-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "3 Predictive Models",
    "text": "3 Predictive Models\nPredictive models aim to forecast future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments. Machine learning models dominate here. A key challenge in predictive modeling is ensuring that the model generalizes well to unseen data, which is why we use techniques like cross-validation. An advantage is that we don’t need repetitions of the data to use these models, but we need to have a good size so the algorithms can “learn” (machine learning).\n\n3.1 Cross-Validation and Generalization Performance\nCross-validation is a resampling technique used to evaluate a model’s ability to generalize to new data. It helps avoid overfitting, where a model performs well on training data but poorly on unseen data. One common method is k-fold cross-validation, where the dataset is split into k subsets, and the model is trained and tested multiple times.\n\nTraining Error: The error the model makes on the data it was trained on.\nGeneralization Performance: The model’s ability to make accurate predictions on unseen data.\nValidation Set Approach: One practical method in agriculture is to leave out data from the latest year as a test set, ensuring the model is evaluated on future-like conditions.\n\n\n\n3.2 Updated Agricultural Dataset with Multiple Years\nTo better illustrate predictive modeling, we expand our dataset to include multiple years, allowing us to simulate a real-world scenario where we leave the latest year out for validation.\n\n# Simulated multi-year agricultural data\ndata_ag &lt;- data.frame(\n  year = rep(2011:2020, each = 50),  # 20 observations per year\n  nitrogen = runif(500, 90, 300),\n  rainfall = runif(500, 700, 1050),\n  psnt = runif(500, 5, 60), # pre-sidedress N test (ppm)\n  yield = 2 + 0.01 * runif(500, 90, 300) + 0.005 * runif(500, 700, 1050) + rnorm(500, 0, 0.2) - 0.002 * runif(500, 5, 60) \n)\n\n# Splitting into training (excluding latest year) and test set (latest year only)\ntrain_data &lt;- data_ag %&gt;% filter(year &lt; 2020)\ntest_data &lt;- data_ag %&gt;% filter(year == 2020)\n\n\n\n3.3 Example: Random Forest with Cross-Validation\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Train model with cross-validation using \"caret\" package\ntrain_control &lt;- caret::trainControl(method = \"cv\", number = 5)\nrf_fit &lt;- caret::train(yield ~ nitrogen + rainfall + psnt, data = train_data, method = \"rf\", trControl = train_control)\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\n# Model Performance\nprint(rf_fit)\n\nRandom Forest \n\n450 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 359, 360, 360, 361, 360 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared    MAE      \n  2     0.8276507  0.01846057  0.6760507\n  3     0.8312802  0.01758722  0.6789385\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n# Predict on the test set\npredictions &lt;- predict(rf_fit, test_data)\n\n# Evaluate Generalization Performance\nsqrt(mean((predictions - test_data$yield)^2))  # Root Mean Squared Error on test data\n\n[1] 0.8748979\n\n# Using the metrica package\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n\n$RMSE\n[1] 0.8748979\n\n# Plot predicted vs observed scatter\nmetrica::scatter_plot(pred = predictions, obs = test_data$yield)\n\n\n\n\n\n\n\n# With tidyverse syntax will be...\ntest_preds &lt;- test_data %&gt;% mutate(predictions = predict(rf_fit, test_data))\n\nmetrica::scatter_plot(data = test_preds, \n                      pred = predictions, obs = yield)\n\n\n\n\n\n\n\n# Root mean square error\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n\n$RMSE\n[1] 0.8748979\n\n# Relative mean square error (as a proportion)\nmetrica::RRMSE(data = test_preds, pred = predictions, obs = yield)\n\n$RRMSE\n[1] 0.1065797\n\n# Estimate more prediction error metrics\nmetrica::metrics_summary(data = test_preds, pred = predictions, obs = yield,\n                         type = \"regression\")\n\n   Metric         Score\n1      B0  4.558434e+00\n2      B1  4.458834e-01\n3       r -3.703233e-01\n4      R2  1.371394e-01\n5      Xa  7.437572e-01\n6     CCC -2.754306e-01\n7     MAE  6.859013e-01\n8    RMAE  8.355618e-02\n9    MAPE  8.577196e+00\n10  SMAPE  8.371905e+00\n11    RAE  1.204755e+00\n12    RSE  1.529245e+00\n13    MBE -9.765955e-03\n14    PBE -1.189684e-01\n15    PAB  1.245990e-02\n16    PPB  2.007823e+01\n17    MSE  7.654463e-01\n18   RMSE  8.748979e-01\n19  RRMSE  1.065797e-01\n20    RSR  1.747912e+00\n21 iqRMSE  9.206810e-01\n22    MLA  1.537834e-01\n23    MLP  6.116629e-01\n24   RMLA  1.537834e-01\n25   RMLP  6.116629e-01\n26     SB  9.537387e-05\n27   SDSD  1.536881e-01\n28    LCS  6.116629e-01\n29    PLA  2.009069e+01\n30    PLP  7.990931e+01\n31     Ue  7.990931e+01\n32     Uc  2.007823e+01\n33     Ub  1.245990e-02\n34    NSE -5.292446e-01\n35     E1 -2.047553e-01\n36   Erel -7.542440e-01\n37    KGE -4.783167e-01\n38      d  1.547432e-01\n39     d1  1.530369e-01\n40    d1r  3.976223e-01\n41    RAC  3.622340e-01\n42     AC -3.776125e+00\n43 lambda -2.754306e-01\n44  dcorr  3.659082e-01\n45    MIC  2.995257e-01\n\n\n\n\n3.4 Interpretation\n\nCross-validation ensures the model is not just memorizing the training data but generalizing well (e.g. predicting well on unseen observations).\nTraining vs. Test/Validation Performance: Comparing error metrics between the training set and the unseen test set gives an estimate of real-world predictive ability. If the difference training between training error and testing error is too much, it’s very likely our model is “over-fitted” (e.g. reading really well the training data but too much).\nLeaving the latest year out allows us to test predictions on future-like data, a common technique in agricultural forecasting.\n\nUsing these techniques ensures that predictive models in agriculture provide reliable and actionable insights rather than overfitted results that fail in practice.\nPredictive models aim to forecast future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments.\n\n\n3.5 Example: Final Random Forest for Forecasting\nNow we have our final model, we train one more time with all the available data, then predict new observations\n\n# Fit a random forest model\nset.seed(123)\n\nrf_fit &lt;- randomForest(yield ~ nitrogen + rainfall + psnt, data = data_ag, ntree = 500)\nprint(rf_fit)\n\n\nCall:\n randomForest(formula = yield ~ nitrogen + rainfall + psnt, data = data_ag,      ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.6820117\n                    % Var explained: -7.2\n\n# Predict yield for new nitrogen and rainfall levels\nnew_data &lt;- data.frame(field = c(\"Elora\", \"Waterloo\", \"Ridgetown\", \"Winchester\"),\n                       nitrogen = c(125, 175, 225, 220), \n                       rainfall = c(870, 920, 980, 1000),\n                       psnt = c(35, 30, 45, 60))\n\n# Adding predictions\nnew_preds &lt;- new_data %&gt;% mutate(predictions = predict(rf_fit, new_data))\n\n\n\n3.6 Interpretation\n\nThis model is non-parametric and learns patterns from data.\nIt is robust against outliers and complex interactions.\nPerformance is evaluated using Mean Squared Error (MSE) or R-squared.",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#key-differences-between-explanatory-and-predictive-models",
    "href": "coding/week_08/models_02.html#key-differences-between-explanatory-and-predictive-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "4 Key Differences Between Explanatory and Predictive Models",
    "text": "4 Key Differences Between Explanatory and Predictive Models\n\n\n\n\n\n\n\n\nFeature\nExplanatory Models\nPredictive Models\n\n\n\n\nPurpose\nUnderstanding relationships\nMaking accurate forecasts\n\n\nExample\nLinear regression, ANOVA\nMachine learning (random forests, neural networks)\n\n\nAssumptions\nRequires assumptions about data distribution\nOften non-parametric, flexible\n\n\nOutput\nCoefficients, p-values\nPredictions, accuracy metrics",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#conclusion",
    "href": "coding/week_08/models_02.html#conclusion",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nUnderstanding the distinction between explanatory and predictive models is essential for agricultural research. While explanatory models help us understand why certain patterns exist, predictive models allow us to make data-driven decisions for future planning. A combination of both approaches can maximize insights and improve decision-making in precision agriculture.\nThis article brings simple examples in R using linear regression for explanatory analysis and random forests for prediction. Depending on the research question, both modeling strategies play a crucial role in agricultural data science.",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html",
    "href": "coding/week_06/10_iteration.html",
    "title": "Iteration in R: The Power of purrr",
    "section": "",
    "text": "Description\nThis lesson explores iteration in R, focusing on the power of the purrr package for functional programming. We’ll compare traditional for loops with purrr’s map() functions to illustrate more efficient and readable approaches to iteration.\nRequired packages for today\nlibrary(pacman) # to install and load packages faster\np_load(dplyr, tidyr) # data wrangling\np_load(purrr) # iteration mapping\np_load(ggplot2) # plots\np_load(agridat) # data\np_load(nlme, broom.mixed, car, performance) # mixed models work\np_load(emmeans, multcomp, multcompView) # multiple comparisons",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#understanding-the-difference-between-map-and-map_dbl",
    "href": "coding/week_06/10_iteration.html#understanding-the-difference-between-map-and-map_dbl",
    "title": "Iteration in R: The Power of purrr",
    "section": "4.1 Understanding the Difference Between map() and map_dbl()",
    "text": "4.1 Understanding the Difference Between map() and map_dbl()\n\nmap_dbl() guarantees that the output is a numeric vector.\n\nSimilarly:\n\nmap_chr() returns a character vector.\nmap_lgl() produces a logical vector.\nmap_int() yields an integer vector.\n\n\nmap(), the most general form, returns a list by default.\n\nThese functions are part of an iterative approach where a function is mapped over elements of a list or vector.",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#practical-application",
    "href": "coding/week_06/10_iteration.html#practical-application",
    "title": "Iteration in R: The Power of purrr",
    "section": "5.1 Practical Application",
    "text": "5.1 Practical Application\nA common workflow involves combining group_by() and nest() to create nested data frames for iteration. You can then use mutate() along with map() to apply a function to each nested data frame:\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\n\ndata %&gt;%\n  group_by(group_variable) %&gt;%\n  nest() %&gt;%\n  mutate(results = map(data, your_function))\nThis approach is very powerful for applying custom functions to subsets of data efficiently.\nLet’s see that in practice…",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#load-the-data",
    "href": "coding/week_06/10_iteration.html#load-the-data",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.1 Load the data",
    "text": "6.1 Load the data\n\ndata_corn_00 &lt;- agridat::lasrosas.corn\nhead(data_corn_00)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#prepare-the-data",
    "href": "coding/week_06/10_iteration.html#prepare-the-data",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.2 Prepare the data",
    "text": "6.2 Prepare the data\n\ndata_corn_01 &lt;- data_corn_00 %&gt;% \n  # Select only necessary variables\n  dplyr::select(year, topo, rep, nf, yield) %&gt;% \n  # Group by\n  group_by(year, topo) %&gt;% \n  # Create nested data frames\n  nest(my_data = c(rep, nf, yield))",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#create-functions",
    "href": "coding/week_06/10_iteration.html#create-functions",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.3 Create functions",
    "text": "6.3 Create functions\n\n6.3.1 Rep (block) as fixed\n\n# SIMPLEST MODEL\nfit_block_fixed &lt;- function(x){\n  lm(# Response variable\n     yield ~ \n       # Fixed (treatment)\n       nf + \n       # Block as fixed too\n       rep,\n     # Data\n     data = x)\n}\n\n\n\n6.3.2 Rep (block) as random\n\n# RANDOM BLOCK (mixed model)\nfit_block_random &lt;- function(x){\n  nlme::lme(# Response variable\n    yield ~\n    # Fixed\n    nf,\n    # Random\n    random = ~1|rep,\n    # Data\n    data = x)\n  }",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#fit-models-with-mapping",
    "href": "coding/week_06/10_iteration.html#fit-models-with-mapping",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.4 Fit models with mapping",
    "text": "6.4 Fit models with mapping\n\nmodels &lt;- data_corn_01 %&gt;% \n  # BLOCK as FIXED \n  mutate(model_1 = map(my_data, fit_block_fixed)) %&gt;% \n  # BLOCK as RANDOM\n  mutate(model_2 = map(my_data, fit_block_random)) %&gt;% \n    \n  # Data wrangling\n  pivot_longer(cols = c(model_1:model_2), # show alternative 'contains' model\n               names_to = \"model_id\",\n               values_to = \"model\") %&gt;% \n  # Map over model column\n  mutate(results = map(model, broom.mixed::augment )) %&gt;% \n  # Performance\n  mutate(performance = map(model, broom.mixed::glance )) %&gt;% \n  # Extract AIC\n  mutate(AIC = map(performance, ~.x$AIC)) %&gt;% \n  ungroup()",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#model-selection",
    "href": "coding/week_06/10_iteration.html#model-selection",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.5 Model selection",
    "text": "6.5 Model selection\nCompare models performance\n\n# Visual model selection\nbest_models &lt;- \n  models %&gt;% \n  group_by(year, topo) %&gt;% \n  # Use case_when to identify the best model\n  mutate(best_model = \n           case_when(AIC == min(as.numeric(AIC)) ~ \"Yes\",\n                     TRUE ~ \"No\")) %&gt;% \n  ungroup()\n\n# Plot\nbest_models %&gt;% \n  ggplot()+\n  geom_point(aes(x = model_id, y = as.numeric(AIC), \n                 color = best_model, shape = best_model), \n             size = 3)+\n  facet_wrap(year~topo)+\n  theme_bw()\n\n\n\n\n\n\n\n# Final models\nselected_models &lt;- best_models %&gt;% dplyr::filter(best_model == \"Yes\")",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#run-anova",
    "href": "coding/week_06/10_iteration.html#run-anova",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.6 Run ANOVA",
    "text": "6.6 Run ANOVA\nEstimate the effects of factor under study\n\nmodels_effects &lt;- \n  selected_models %&gt;%\n  # Type 3 Sum of Squares (Partial SS, when interactions are present)\n  mutate(ANOVA = map(model, ~Anova(., type = 3)) )\n\n# Extract ANOVAS\nmodels_effects$ANOVA[[1]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5729.92  1  &lt; 2.2e-16 ***\nnf           164.03  5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[2]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 6089.64  1  &lt; 2.2e-16 ***\nnf           318.03  5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[3]]\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept) 158985   1 7299.604 &lt; 2.2e-16 ***\nnf            1975   5   18.136 4.699e-16 ***\nrep            691   2   15.858 2.509e-07 ***\nResiduals     7841 360                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[8]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 18282.200  1  &lt; 2.2e-16 ***\nnf             72.431  5  3.194e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#means-comparison",
    "href": "coding/week_06/10_iteration.html#means-comparison",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.7 Means comparison",
    "text": "6.7 Means comparison\n\n# MULTCOMPARISON\n# emmeans and cld multcomp\n# We need to specify ourselves the most important interaction to perform the comparisons\nmult_comp &lt;- \n  models_effects %&gt;% \n  # Comparisons estimates (emmeans)\n  mutate(mc_estimates = map(model, ~emmeans(., ~ nf))) %&gt;% \n  # Assign letters and p-value adjustment (multcomp)\n  mutate(mc_letters = \n           map(mc_estimates, \n               ~as.data.frame( \n                 # By specifies a strata or level to assign the letters\n                 cld(., decreasing = TRUE, details=FALSE,\n                     reversed=TRUE, alpha=0.05,  adjust = \"tukey\", Letters=LETTERS))))\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#packages-well-use-today",
    "href": "coding/week_13/bayes_02.html#packages-well-use-today",
    "title": "Bayesian Statistics in R",
    "section": "1 📦 Packages we’ll use today",
    "text": "1 📦 Packages we’ll use today\n\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#markov-chain-monte-carlo-mcmc",
    "href": "coding/week_13/bayes_02.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian Statistics in R",
    "section": "3 🔁 Markov Chain Monte Carlo (MCMC)",
    "text": "3 🔁 Markov Chain Monte Carlo (MCMC)\n\nMCMC methods changed Bayesian stats forever! 🧠🔥\n\nThey let us generate samples from complex distributions\nThey form a chain, where each sample depends on the previous\nUsed in packages like brms, rstan, and rjags\n\n📚 More info: - MCMC Handbook - MCMCpack - mcmc - Paper: Foundations of MCMC",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#brms-bayesian-modeling-made-easy",
    "href": "coding/week_13/bayes_02.html#brms-bayesian-modeling-made-easy",
    "title": "Bayesian Statistics in R",
    "section": "4 brms: Bayesian Modeling Made Easy",
    "text": "4 brms: Bayesian Modeling Made Easy\n\n🔗 Docs: https://paul-buerkner.github.io/brms/\n🐛 Issues: https://github.com/paul-buerkner/brms/issues\nbrms makes it easy to run complex Bayesian models — without writing Stan code manually. It’s inspired by lme4, so syntax feels familiar.\nIt supports a wide range of models: - Linear, GLM, survival, zero-inflated, ordinal, count, and more\n✨ We’ll use brms as our go-to interface in this session!\n📚 More info: - JSS Article on brms\n\n\n4.1 Fit brms\nLet’s fit the example using the brms package.\n\n\n4.2 brms pars\n\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n4.3 Model\n\n# 01. Run model\nbayes_model &lt;- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\n#bayes_model &lt;- readRDS(file = \"bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n\n\n\n\n\n\n\n# Visualize model results\nbayes_model\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 61) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     5.42      0.27     4.89     5.95 1.00     2664     2753\nB1_Intercept     2.04      0.10     1.82     2.24 1.00     2719     2502\nB2_Intercept     0.11      0.01     0.10     0.13 1.00     2764     2641\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.73      0.07     0.61     0.88 1.00     2997     2715\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nmpling)\nChain 4: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 3: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 3: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.393 seconds (Warm-up)\nChain 3:                1.474 seconds (Sampling)\nChain 3:                1.867 seconds (Total)\nChain 3: \nChain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.525 seconds (Warm-up)\nChain 2:                1.631 seconds (Sampling)\nChain 2:                2.156 seconds (Total)\nChain 2: \nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.393 seconds (Warm-up)\nChain 1:                1.853 seconds (Sampling)\nChain 1:                2.246 seconds (Total)\nChain 1: \nChain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.437 seconds (Warm-up)\nChain 4:                2.207 seconds (Sampling)\nChain 4:                2.644 seconds (Total)\nChain 4: \n\n\n\n4.3.1 Compare vs traditional linear model (lm)\n\ndata_frame_q &lt;- data_frame %&gt;% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n      5.415        2.038       -0.114  \n\n\n\n\n\n4.4 Using posterior distributions\n\n4.4.1 Prepare summary\n\n# Create predictions\nm1 &lt;- data_frame %&gt;% \n  ungroup() %&gt;% \n  dplyr::select(x) %&gt;% \n  group_by(x) %&gt;% filter(x == max(x)) %&gt;% \n  ungroup() %&gt;% unique() %&gt;% rename(max = x) %&gt;% \n  # Generate a sequence of x values\n  mutate(data = max %&gt;% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %&gt;% \n  unnest() %&gt;% dplyr::select(-max) %&gt;%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %&gt;% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %&gt;% ungroup()\n\n# Summarize\nm1_quantiles &lt;- m1 %&gt;% \n  group_by(x) %&gt;% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n\n\n4.4.2 Plot posterior\n\n# Plot\nm1_plot &lt;- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk1\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"gold3\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T, \n              linetype = \"dashed\")+\n  scale_color_manual(values=c(\"purple4\", \"tomato\"))+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_classic()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rstan-full-control-with-stan",
    "href": "coding/week_13/bayes_02.html#rstan-full-control-with-stan",
    "title": "Bayesian Statistics in R",
    "section": "5 rstan: Full Control with Stan",
    "text": "5 rstan: Full Control with Stan\n\n🔗 Docs: https://mc-stan.org/rstan/\n🐛 Issues: https://github.com/stan-dev/rstan/issues\nStan is a powerful, high-performance platform for Bayesian modeling, using: - Hamiltonian Monte Carlo (HMC) - No-U-Turn Sampler (NUTS)\nUnlike brms, Stan requires writing the full model — offering full flexibility and speed.\n✨ brms can even show the Stan code it generates under the hood!\nStan also supports Python, Julia, MATLAB, and more.",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rjags-just-another-gibbs-sampler",
    "href": "coding/week_13/bayes_02.html#rjags-just-another-gibbs-sampler",
    "title": "Bayesian Statistics in R",
    "section": "6 rjags: Just Another Gibbs Sampler",
    "text": "6 rjags: Just Another Gibbs Sampler\n\n🔗 Docs: https://mcmc-jags.sourceforge.io/\n🐛 Issues: https://sourceforge.net/projects/mcmc-jags/\nrjags uses the classic Gibbs Sampling approach and the BUGS model syntax (used in WinBUGS, OpenBUGS).\n\nMore manual than brms\nIdeal for users who want to write the full statistical model\nOften paired with the coda package for diagnostics\n\n\nHappy Bayesian coding! 💻✨",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  }
]