[
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html",
    "href": "coding/week_05/09_ggplot2_plus.html",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "",
    "text": "Description\nIn this class, we will explore how to create geographic maps using ggplot2, sf, maps, leaflet, and geojson. We’ll cover techniques for plotting data points on US and Canada maps, customizing map aesthetics, and working with spatial data.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#us-map-with-state-boundaries",
    "href": "coding/week_05/09_ggplot2_plus.html#us-map-with-state-boundaries",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "2.1 US Map with State Boundaries",
    "text": "2.1 US Map with State Boundaries\n\n# Load US map data\nus_map &lt;- map_data(\"state\")\n\n# Plot US map\nus_plot &lt;- \n  ggplot() +\n  geom_polygon(data = us_map, aes(x = long, y = lat, group = group),\n               fill = \"grey90\", color = \"grey35\") +\n  coord_fixed(1.3) +\n  labs(title = \"Map of the United States\") +\n  theme_base()\n\nus_plot",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#canada-map-with-provincial-boundaries",
    "href": "coding/week_05/09_ggplot2_plus.html#canada-map-with-provincial-boundaries",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "2.2 Canada Map with Provincial Boundaries",
    "text": "2.2 Canada Map with Provincial Boundaries\n\n# Load Canada map data\ncanada_map &lt;- map_data(\"world\", region = \"Canada\")\n\n# Plot Canada map\ncanada_plot &lt;- ggplot() +\n  geom_polygon(data = canada_map, aes(x = long, y = lat, group = group),\n               fill = \"steelblue\", color = \"black\") +\n  coord_fixed(1.3) +\n  labs(title = \"Map of Canada\")\n\ncanada_plot\n\n\n\n\n\n\n\ncanada_cut &lt;- canada_plot +\n  # Cut limits of map\n  scale_y_continuous(limits = c(41, 48))+\n  scale_x_continuous(limits = c(-87, -75))+\n  theme_minimal()\n\ncanada_cut",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-us-map",
    "href": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-us-map",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "3.1 Plotting Cities on US Map",
    "text": "3.1 Plotting Cities on US Map\n\n# Sample city data\ncities_us &lt;- data.frame(\n  city = c(\"New Jersey\", \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"),\n  lon = c(-74, -74.006, -118.2437, -87.6298, -95.3698, -112.074),\n  lat = c(40.9, 40.7128, 34.0522, 41.8781, 29.7604, 33.4484)\n)\n\n# Plot US map with cities\nus_plot +\n  geom_point(data = cities_us, aes(x = lon, y = lat), \n             color = \"red\", size = 3) +\n  # geom_label(data = cities_us, aes(x = lon, y = lat, label = city), \n  #                  size = 3, color = \"black\")\n  geom_label_repel(data = cities_us, aes(x = lon, y = lat, label = city), \n                    size = 3, color = \"black\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-canada-map",
    "href": "coding/week_05/09_ggplot2_plus.html#plotting-cities-on-canada-map",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "3.2 Plotting Cities on Canada Map",
    "text": "3.2 Plotting Cities on Canada Map\n\n# Sample city data for Canada\ncities_canada &lt;- data.frame(\n  city = c(\"Toronto\", \"Vancouver\", \"Montreal\", \"Calgary\", \"Ottawa\"),\n  lon = c(-79.3832, -123.1216, -73.5673, -114.0719, -75.6972),\n  lat = c(43.6511, 49.2827, 45.5017, 51.0447, 45.4215)\n)\n\n# Plot Canada map with cities\ncanada_plot +\n  geom_point(data = cities_canada, aes(x = lon, y = lat), \n             color = \"blue\", size = 3) +\n  geom_label_repel(data = cities_canada, aes(x = lon, y = lat, label = city), \n                   size = 3, color = \"black\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#loading-and-plotting-shapefiles",
    "href": "coding/week_05/09_ggplot2_plus.html#loading-and-plotting-shapefiles",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.1 Loading and Plotting Shapefiles",
    "text": "4.1 Loading and Plotting Shapefiles\n\n# Load shapefile (replace 'path_to_shapefile' with your actual path)\n# usa_shapefile &lt;- st_read(\"path_to_shapefile/usa_shapefile.shp\")\n\n# Example using built-in dataset from `sf`\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n# Plot shapefile\nggplot(nc) +\n  geom_sf(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Shapefile Example: North Carolina Counties\") +\n  # Adjust scales for limits\n  scale_y_continuous(limits = c(34.5, 36))+\n  scale_x_continuous(limits = c(-82, -78))+\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#read-shp-of-canada-and-us",
    "href": "coding/week_05/09_ggplot2_plus.html#read-shp-of-canada-and-us",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.2 Read shp of Canada and US",
    "text": "4.2 Read shp of Canada and US\n\n## Load shp files\nusa_shp &lt;- st_read(\"shp_map/us/usa.shp\") %&gt;%\n  # Remove non-contiguous and territories\n  filter(!(NAME %in% c(\"Alaska\", \"District of Columbia\", \"Hawaii\", \"Puerto Rico\")))\n\nReading layer `usa' from data source \n  `/Users/acorrend/Documents/GitHub/plnt6800/coding/week_05/shp_map/us/usa.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 17.88481 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\ncan_shp &lt;- st_read(\"shp_map/can/canada.shp\")\n\nReading layer `canada' from data source \n  `/Users/acorrend/Documents/GitHub/plnt6800/coding/week_05/shp_map/can/canada.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 13 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -141.0181 ymin: 41.67695 xmax: -52.5823 ymax: 89.99943\nGeodetic CRS:  NAD83",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#create-objects-for-maps",
    "href": "coding/week_05/09_ggplot2_plus.html#create-objects-for-maps",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.3 Create objects for maps",
    "text": "4.3 Create objects for maps\n\n# Create list of selected provinces\nselected_provinces &lt;- c(\"Ontario\", \"Manitoba\", \"Quebec\")\n\n# Define coordinates for Ontario cities\nontario_cities &lt;- data.frame(\n  city = c(\"Toronto\", \"Ottawa\", \"Hamilton\", \"London\", \"Kingston\"),\n  lon = c(-79.3832, -75.6972, -79.8711, -81.2497, -76.4880),\n  lat = c(43.6511, 45.4215, 43.2557, 42.9834, 44.2312)\n)",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#define-function-to-customize-plot",
    "href": "coding/week_05/09_ggplot2_plus.html#define-function-to-customize-plot",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.4 Define function to customize plot",
    "text": "4.4 Define function to customize plot\n\ngeo_plot &lt;- function(x, y, z, title = NULL){\n  ggplot()+\n    geom_sf(data=x, fill = \"white\", color = \"black\") + # Provinces map\n  geom_sf(data=y, fill = \"white\", color = \"black\")+ # US map\n  # Adjust scales for lat and lon\n  scale_y_continuous(limits = c(41.8, 46))+\n  scale_x_continuous(limits = c(-84, -75), breaks = seq(-84,-74, by=1)) +\n  # Add cities with points\n  geom_point(data = z, aes(x = lon, y = lat, fill = city), \n             color = \"grey25\", shape = 21, size = 3, alpha = 0.95) +\n  # Scalebar\n  annotation_scale(tick_height = 0.3)+\n  # Text Notes for names of cities\n  geom_text_repel(data = z, \n                  aes(x=lon, y=lat, label = city), size = 3)+\n  # Name of PROVINCE\n  annotate(\"text\", x = -78, y = 45, label = \"ONTARIO\", \n           size = 4, fontface = \"bold\")+\n  # Name of Lakes\n  ## Ontario\n  annotate(\"text\", x = -77.8, y = 43.7, label = \"Lake Ontario\", \n           size = 3, fontface = \"italic\")+\n  ## Huron \n  annotate(\"text\", x = -82.5, y = 44.5, label = \"Lake \\nHuron\", \n           size = 3, fontface = \"italic\")+\n  # Add labels\n  labs(title = title, \n       x = \"Longitude\", y = \"Latitude\")+\n  # Adjust theme\n  theme_base()+\n  # reduce axis text size\n  theme(\n    panel.background = element_rect(fill = \"#bde0fe\"),\n    title = element_text(size = rel(.7)),\n    axis.title =  element_text(size = rel(.9), face = \"bold\"),\n    axis.text = element_text(size = rel(.5))\n    )\n}",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plot",
    "href": "coding/week_05/09_ggplot2_plus.html#plot",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "4.5 Plot",
    "text": "4.5 Plot\n\n# Plot from function\ngeo_plot(x = can_shp, y = usa_shp, z = ontario_cities, \n         title = \"Ontario Map from SHP\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#loading-geojson-data",
    "href": "coding/week_05/09_ggplot2_plus.html#loading-geojson-data",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.1 Loading GeoJSON Data",
    "text": "5.1 Loading GeoJSON Data\n\n# Load Canada GeoJSON (replace with actual file path if available)\n# Read geojson for Canada\ncan_geojson &lt;- read_sf(\"geojson_maps/canada_provinces.geojson\")  %&gt;%  \n  # dplyr::filter(name == \"Ontario\")  %&gt;% \n  dplyr::filter(name %in% selected_provinces)  %&gt;% \n  dplyr::mutate(Province = name, \n                GEOID = cartodb_id)  %&gt;%  \n  dplyr::select(GEOID, Province, geometry)\n\n# Read geojson for US\nusa_geojson &lt;- read_sf(\"geojson_maps/us-states.json\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#plot-geojson",
    "href": "coding/week_05/09_ggplot2_plus.html#plot-geojson",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.2 Plot GEOJSON",
    "text": "5.2 Plot GEOJSON\n\n# Plot from function\ngeo_plot(x = can_geojson, y = usa_geojson, z = ontario_cities, \n         title = \"Ontario Map from GEOJSON\")",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#explanation",
    "href": "coding/week_05/09_ggplot2_plus.html#explanation",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "5.3 Explanation:",
    "text": "5.3 Explanation:\n\ngeojson_read(): Reads the GeoJSON file.\nst_as_sf(): Converts the data into an sf object for plotting.\ngeom_sf(): Plots the GeoJSON data.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#ontario-map-with-leaflet",
    "href": "coding/week_05/09_ggplot2_plus.html#ontario-map-with-leaflet",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "6.1 Ontario Map with leaflet",
    "text": "6.1 Ontario Map with leaflet\n\n# Create interactive map\nontario_cities %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n# Add PINS\naddMarkers(~lon, ~lat, popup = ~city) %&gt;%\naddCircleMarkers(~lon, ~lat, popup = ~city, radius = 5, color = \"gold\",\n                  fillOpacity = 0.7) %&gt;%\n# Configure initial view of the map\nsetView(lng = -80, lat = 44, zoom = 6)",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "coding/week_05/09_ggplot2_plus.html#explanation-1",
    "href": "coding/week_05/09_ggplot2_plus.html#explanation-1",
    "title": "Data Viz III: Geographic Mapping with ggplot2",
    "section": "6.2 Explanation:",
    "text": "6.2 Explanation:\n\naddTiles(): Adds the base map layer.\naddMarkers(): Plots city locations with popups displaying city names.\nsetView(): Centers the map on Toronto with a specified zoom level.",
    "crumbs": [
      "Lessons",
      "09-Data viz III"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLNT6800|01 - Special Topics in Plant Science",
    "section": "",
    "text": "Reproducible Ag Data Science with R is the first edition Ag data science course designed by Dr. Adrian Correndo offered at Department of Plant Agriculture (University of Guelph) for graduate students in crop and soil sciences to develop key skills in data science using R. This course emphasizes reproducibility in data analysis, ensuring that results can be consistently replicated. Students will learn essential data science concepts, and how to use functions, packages, and version control to effectively manage their data and collaborate with peers. Following tidy principles, the course promotes best coding practices for data wrangling, effective visualization, and clean deployment of statistical models common in agriculture. By the end of the course, students will be equipped to handle a variety of agricultural datasets and produce reliable, reproducible research outcomes.\n\n\nBy the end of this course, students will be able to:\n\nUnderstand and apply the principles of reproducible research in data science.\nUse version control tools like GitHub for managing code and collaborative projects.\nDevelop proficiency in R, including data wrangling, data visualization, and the use of relevant packages for agricultural datasets.\nApply statistical models to agricultural data and interpret the results.\nProduce professional reports using RMarkdown and Quarto, ensuring reproducibility and clarity.\n\n\n\n\nThis course will use a variety of technologies and resources. To successfully participate in and complete this course, students will need access to the following\n\n\nCourseLink. This platform will be used as the main Course-Home Page. If you need any assistance with the software tools or the CourseLink website, contact CourseLink Support. Email: courselink@uoguelph.ca Tel: 519-824-4120 ext. 56939 Toll-Free (CAN/USA): 1-866- 275-1478. Support Hours (Eastern Time): Monday thru Friday: 8:30 am–8:30 pm; Saturday: 10:00 am–4:00 pm; Sunday: 12:00 pm–6:00 pm\nZoom. This course will use Zoom for lectures when in-person class is not possible. Check your system requirements to ensure you will be able to participate (https://opened.uoguelph.ca/student-resources/system-and-software-requirements/). A Zoom link for the class will be provided before the first day of class. Please, check Home-Page and announcements on CourseLink, and emails from the instructor (acorrend@uoguelph.ca).\n\n\n\n\nR (latest stable version, available at CRAN).\nRStudio/Posit IDE (desktop or cloud-based version for writing and running R code).\nCourse-Specific Libraries and Packages: Students will be required to install R packages. Detailed instructions will be provided in class.\nVersion Control and Collaboration Tools: Git (for version control) and a free GitHub account for collaborative project work and sharing code.\n\n\n\n\nA laptop or desktop computer capable of running R and RStudio (Windows, MacOS, or Linux). Minimum specifications include:\n\nProcessor: At least a dual-core processor.\nRAM: 8 GB or more (16 GB recommended for handling larger datasets).\nStorage: 10 GB of free space for software installation, course files, and datasets.\n\n\n\n\nReliable high-speed internet for accessing online sessions, resources, downloading software, and using cloud-based platforms (e.g., Posit Cloud, GitHub).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#dpag-oac-uogwelcome",
    "href": "index.html#dpag-oac-uogwelcome",
    "title": "PLNT6800|01 - Special Topics in Plant Science",
    "section": "",
    "text": "Reproducible Ag Data Science with R is the first edition Ag data science course designed by Dr. Adrian Correndo offered at Department of Plant Agriculture (University of Guelph) for graduate students in crop and soil sciences to develop key skills in data science using R. This course emphasizes reproducibility in data analysis, ensuring that results can be consistently replicated. Students will learn essential data science concepts, and how to use functions, packages, and version control to effectively manage their data and collaborate with peers. Following tidy principles, the course promotes best coding practices for data wrangling, effective visualization, and clean deployment of statistical models common in agriculture. By the end of the course, students will be equipped to handle a variety of agricultural datasets and produce reliable, reproducible research outcomes.\n\n\nBy the end of this course, students will be able to:\n\nUnderstand and apply the principles of reproducible research in data science.\nUse version control tools like GitHub for managing code and collaborative projects.\nDevelop proficiency in R, including data wrangling, data visualization, and the use of relevant packages for agricultural datasets.\nApply statistical models to agricultural data and interpret the results.\nProduce professional reports using RMarkdown and Quarto, ensuring reproducibility and clarity.\n\n\n\n\nThis course will use a variety of technologies and resources. To successfully participate in and complete this course, students will need access to the following\n\n\nCourseLink. This platform will be used as the main Course-Home Page. If you need any assistance with the software tools or the CourseLink website, contact CourseLink Support. Email: courselink@uoguelph.ca Tel: 519-824-4120 ext. 56939 Toll-Free (CAN/USA): 1-866- 275-1478. Support Hours (Eastern Time): Monday thru Friday: 8:30 am–8:30 pm; Saturday: 10:00 am–4:00 pm; Sunday: 12:00 pm–6:00 pm\nZoom. This course will use Zoom for lectures when in-person class is not possible. Check your system requirements to ensure you will be able to participate (https://opened.uoguelph.ca/student-resources/system-and-software-requirements/). A Zoom link for the class will be provided before the first day of class. Please, check Home-Page and announcements on CourseLink, and emails from the instructor (acorrend@uoguelph.ca).\n\n\n\n\nR (latest stable version, available at CRAN).\nRStudio/Posit IDE (desktop or cloud-based version for writing and running R code).\nCourse-Specific Libraries and Packages: Students will be required to install R packages. Detailed instructions will be provided in class.\nVersion Control and Collaboration Tools: Git (for version control) and a free GitHub account for collaborative project work and sharing code.\n\n\n\n\nA laptop or desktop computer capable of running R and RStudio (Windows, MacOS, or Linux). Minimum specifications include:\n\nProcessor: At least a dual-core processor.\nRAM: 8 GB or more (16 GB recommended for handling larger datasets).\nStorage: 10 GB of free space for software installation, course files, and datasets.\n\n\n\n\nReliable high-speed internet for accessing online sessions, resources, downloading software, and using cloud-based platforms (e.g., Posit Cloud, GitHub).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html",
    "href": "coding/week_01/essentials_01.html",
    "title": "Essentials of R coding I",
    "section": "",
    "text": "This page provides an overview of the essential types of elements in R, including examples and explanations for each. Use this as a quick reference to understand the basics of data types and operations.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#numbers",
    "href": "coding/week_01/essentials_01.html#numbers",
    "title": "Essentials of R coding I",
    "section": "01. Numbers",
    "text": "01. Numbers\n\n20\n\n[1] 20",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#math-operations",
    "href": "coding/week_01/essentials_01.html#math-operations",
    "title": "Essentials of R coding I",
    "section": "02. Math Operations",
    "text": "02. Math Operations\n\n20+1 # addition\n\n[1] 21\n\n20-4 # subtraction\n\n[1] 16\n\n20*5 # multiplication\n\n[1] 100\n\n20/5 # division\n\n[1] 4\n\n2^2 # exponentials\n\n[1] 4\n\nsqrt(9) # square root\n\n[1] 3\n\n# Greater exponents for roots\n# notation is: x^(1/n)\n\n# Cubic root of 27\n27^(1/3)  # Result: 3\n\n[1] 3\n\n# 4th root of 16\n16^(1/4)  # Result: 2\n\n[1] 2\n\n# 5th root of 32\n32^(1/5)  # Result: 2\n\n[1] 2",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#text-or-characters-also-called-strings",
    "href": "coding/week_01/essentials_01.html#text-or-characters-also-called-strings",
    "title": "Essentials of R coding I",
    "section": "03. Text or characters (also called strings)",
    "text": "03. Text or characters (also called strings)\n\n\"coding is fun\"\n\n[1] \"coding is fun\"\n\n\nBut these elements are not stored as objects yet:",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#define-objects",
    "href": "coding/week_01/essentials_01.html#define-objects",
    "title": "Essentials of R coding I",
    "section": "04. Define objects",
    "text": "04. Define objects\n\na &lt;- 20\n10 -&gt; b\n# We can also use equal:\nc = 15\n# But using \"&lt;-\", and leave = only for operations (so you can notice the difference) is considered a better coding practice.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#print-objects",
    "href": "coding/week_01/essentials_01.html#print-objects",
    "title": "Essentials of R coding I",
    "section": "05. Print objects",
    "text": "05. Print objects\n\na\n\n[1] 20\n\nprint(a)\n\n[1] 20\n\nb\n\n[1] 10\n\nc\n\n[1] 15",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#vectors",
    "href": "coding/week_01/essentials_01.html#vectors",
    "title": "Essentials of R coding I",
    "section": "06. Vectors",
    "text": "06. Vectors\nA vector is one of the most basic data structures. It is a sequence of elements of the same type, such as numbers, characters, or logical values. Vectors are used to store and manipulate collections of data efficiently. \n\na. Creating a vector\nVectors can be created using the c() function (combine function):\n\n# Numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4.5)\nnumeric_vector\n\n[1] 1.0 2.0 3.0 4.5\n\n# Character vector\ncharacter_vector &lt;- c(\"corn\", \"wheat\", \"soybean\")\ncharacter_vector\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n# Logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE)\nlogical_vector\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\nb. Accessing Elements\nYou can access elements of a vector using square brackets []:\n\n# Access the first element\nnumeric_vector[1]\n\n[1] 1\n\n# Access multiple elements\nnumeric_vector[c(1, 3)]\n\n[1] 1 3\n\n\n\n\nc. Vectorized Operations\nIn R, vector-operations are applied to each element automatically:\n\n# Adding a scalar to a vector\nnumeric_vector + 2\n\n[1] 3.0 4.0 5.0 6.5\n\n# Element-wise addition\nnumeric_vector + c(10, 20, 30, 40)\n\n[1] 11.0 22.0 33.0 44.5\n\n\n\n\nd. Common Functions with Vectors\n\n‘length()’: Get the number of elements in a vector.\n‘typeof()’ or ‘class()’: Determine the type of elements in a vector.\n‘seq()’: Generate a sequence of numbers.\n‘rep()’: Repeat elements to create a vector.",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#lists",
    "href": "coding/week_01/essentials_01.html#lists",
    "title": "Essentials of R coding I",
    "section": "07. Lists",
    "text": "07. Lists\nIn R, a list is a versatile data structure that can contain elements of different types, including vectors, matrices, data frames, and even other lists. Unlike vectors, which are homogeneous, lists are heterogeneous, meaning their elements can be of different data types and lengths. \nKey Characteristics of Lists: \n\nHeterogeneous: Lists can store elements of varying types (numeric, character, logical, etc.) and structures (vectors, data frames, functions, etc.). \nIndexed: Elements in a list are accessed using double square brackets [[ ]] or named elements using $.\n\nWhy Use Lists? \n\nFlexibility: Lists can store complex and nested data structures. \nData Wrangling: Useful for handling results from models, nested data, or any mixed-type collections. \nFunctions: Functions in R often return their output as lists (e.g., lm()). \n\n\na. Creating a list\nLists are created using the list() function:\n\n# Create a list with different types of elements\nmy_list &lt;- list(\n  \"numeric_v\" = numeric_vector,\n  \"character_v\" = character_vector,\n  \"single_number\" = 42,\n  \"logical_value\" = TRUE\n)\n\n\n\nb. Accessing Elements in a List\nYou can access elements in a list by their position or name:\nBy Position:\n\n# Access the first element\nmy_list[[1]]\n\n[1] 1.0 2.0 3.0 4.5\n\n# Access the second element\nmy_list[[2]]\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n\nBy name:\n\n# Access by name\nmy_list$numeric_v\n\n[1] 1.0 2.0 3.0 4.5\n\nmy_list$character_v\n\n[1] \"corn\"    \"wheat\"   \"soybean\"\n\n\nSubelements:\n\n# Access the first value in the numeric vector\nmy_list$numeric_vector[1]\n\nNULL\n\n\n\n\nc. Some functions for lists\n\n# Number of elements in the list\nlength(my_list)\n\n[1] 4\n\n# Names of the elements\nnames(my_list)\n\n[1] \"numeric_v\"     \"character_v\"   \"single_number\" \"logical_value\"\n\n# Structure of the list\nstr(my_list)\n\nList of 4\n $ numeric_v    : num [1:4] 1 2 3 4.5\n $ character_v  : chr [1:3] \"corn\" \"wheat\" \"soybean\"\n $ single_number: num 42\n $ logical_value: logi TRUE",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#data-frame",
    "href": "coding/week_01/essentials_01.html#data-frame",
    "title": "Essentials of R coding I",
    "section": "08. Data frame",
    "text": "08. Data frame\nIn R, a data frame is a two-dimensional data structure used for storing tabular data. It is one of the most commonly used data structures in R for data analysis and manipulation. \nKey Characteristics of a Data Frame \n\nTabular Structure: Data is organized in rows and columns. \nHeterogeneous Columns: Each column can contain different data types (e.g., numeric, character, logical), but all elements in a column must be of the same type. \nRow and Column Names: Rows and columns can have names for easier identification. \n\nWhy Use a Data Frame? \n\nData Analysis: It is ideal for representing structured data like spreadsheets or databases. \nFlexible Operations: Columns can be easily added, removed, or modified. \nIntegration with R Functions: Many R functions for statistical modeling and analysis expect data frames as input. \n\n\na. Creating a Data Frame\nA data frame can be created using the data.frame() function:\n\n# Create a data frame\nmy_data &lt;- data.frame(\n  Crop = c(\"Corn\", \"Wheat\", \"Soybean\"), # Character column\n  Yield = c(180, 90, 50), # Numeric column\n  Legume = c(FALSE, FALSE, TRUE) # Logical column\n)\n\nprint(my_data)\n\n     Crop Yield Legume\n1    Corn   180  FALSE\n2   Wheat    90  FALSE\n3 Soybean    50   TRUE\n\n\n\n\nb. Accessing data in a data frame\nAccessing columns:\n\n# Access a column by name\nmy_data$Crop\n\n[1] \"Corn\"    \"Wheat\"   \"Soybean\"\n\n# Access a column by index\nmy_data[, 2]\n\n[1] 180  90  50\n\n\nAccessing rows:\n\n# Access the first row\nmy_data[1, ]\n\n  Crop Yield Legume\n1 Corn   180  FALSE\n\n# Access specific rows\nmy_data[c(1, 3), ]\n\n     Crop Yield Legume\n1    Corn   180  FALSE\n3 Soybean    50   TRUE\n\n\nAccessing specific elements\n\n# Access the element in the 2nd row, 3rd column\nmy_data[2, 3]\n\n[1] FALSE\n\n# Access specific cells by column name\nmy_data[2, \"Crop\"]\n\n[1] \"Wheat\"\n\n\n\n\nc. Adding a new column\n\nmy_data$Season &lt;- c(\"Summer\", \"Winter\", \"Summer\")\n\n\n\nd. Modify a column\n\nmy_data$Yield &lt;- my_data$Yield + 5\n\n\n\ne. Adding a new row\nIn base R, we can use rbind() to add rows:\n\nnew_row &lt;- data.frame(Crop = \"Barley\", Yield = 80, Legume = FALSE, Season = \"Winter\")\nmy_data &lt;- rbind(my_data, new_row)\n\n\n\nf. Filtering (rows)\nIn base R, we can use subset() to filter rows:\n\nsubset(my_data, Yield &gt; 150)\n\n  Crop Yield Legume Season\n1 Corn   185  FALSE Summer\n\n\nWe can also use logical conditions:\n\nmy_data[my_data$Legume == TRUE, ]\n\n     Crop Yield Legume Season\n3 Soybean    55   TRUE Summer\n\n\n\n\ng. Selecting (columns)\nIn base R, there is no function to select columns. We need to use brackets [] and vectors c():\n\nmy_data[c(\"Crop\", \"Yield\")]\n\n     Crop Yield\n1    Corn   185\n2   Wheat    95\n3 Soybean    55\n4  Barley    80\n\n\n\n\nh. Some functions for data frames\n\nnrow(my_data)        # Number of rows\n\n[1] 4\n\nncol(my_data)        # Number of columns\n\n[1] 4\n\ncolnames(my_data)    # Column names\n\n[1] \"Crop\"   \"Yield\"  \"Legume\" \"Season\"\n\nsummary(my_data)     # Summary statistics\n\n     Crop               Yield          Legume           Season         \n Length:4           Min.   : 55.00   Mode :logical   Length:4          \n Class :character   1st Qu.: 73.75   FALSE:3         Class :character  \n Mode  :character   Median : 87.50   TRUE :1         Mode  :character  \n                    Mean   :103.75                                     \n                    3rd Qu.:117.50                                     \n                    Max.   :185.00",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#matrix",
    "href": "coding/week_01/essentials_01.html#matrix",
    "title": "Essentials of R coding I",
    "section": "09. Matrix",
    "text": "09. Matrix\nIn R, a matrix is a two-dimensional, rectangular data structure that stores elements of the same type. It is similar to a data frame in structure but less flexible, as all elements in a matrix must be of a single data type (e.g., numeric, character, or logical). \nKey Characteristics of a Matrix \n\nHomogeneous: All elements in a matrix must be of the same type. \n2D Structure: A matrix has rows and columns, forming a table-like structure. \nDimensions: Defined by the number of rows and columns. \n\nWhy Use a Matrix? \n\nMathematical Operations: Ideal for linear algebra and mathematical modeling. \nEfficient Storage: Matrices use less memory compared to more complex structures like data frames. \nSimpler Operations: Homogeneous data ensures consistent behavior across elements. \n\n\na. Creating a Matrix\nYou can create a matrix using the matrix() function:\n\n# Create a numeric matrix\nmy_matrix &lt;- matrix(\n  data = 1:9,     # Data values\n  nrow = 3,       # Number of rows\n  ncol = 3,       # Number of columns\n)\n\nprint(my_matrix)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\nb. Accessing elements in a matrix\nAccessing rows:\n\n# Access the first row\nmy_matrix[1, ]\n\n[1] 1 4 7\n\n\nAccessing columns:\n\n# Access the second column\nmy_matrix[, 2]\n\n[1] 4 5 6\n\n\nAccessing specific elements:\n\n# Access the element in the 2nd row, 3rd column\nmy_matrix[2, 3]\n\n[1] 8\n\n\n\n\nc. Adding a new column\n\nnew_col &lt;- c(10, 11, 12) # Create the column\nmy_matrix &lt;- cbind(my_matrix, new_col) # Paste it to the existing\n\n\n\nd. Adding a new row\n\nnew_row &lt;- c(13, 14, 15, 16)\nmy_matrix &lt;- rbind(my_matrix, new_row)",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_01/essentials_01.html#functions",
    "href": "coding/week_01/essentials_01.html#functions",
    "title": "Essentials of R coding I",
    "section": "10. Functions",
    "text": "10. Functions\n\na. Create a function\nWe need to use the syntax function(x) { x as object of a task }. ‘x’ is considered an “argument”, and the function itself is inside the {}. For example:\n\nmy_function &lt;- function(x) { x + 1 }\n\n\n\nb. Check the function\n\nmy_function(9)\n\n[1] 10\n\n\n\n\nc. Write a function with 3 arguments\n\nmy_xyz_function &lt;- function(x, y, z) { x + y - z }\n\n\n\nd. Order of arguments\nNote: R is order sensitive (if you don’t explicitly specify the argument)\n\nmy_xyz_function(12, 3, 4)\n\n[1] 11\n\nmy_xyz_function(12, 4, 3)\n\n[1] 13\n\n\n\n\ne. Specifying arguments with names\nIf you specify the argument name as = to, the order doesn’t matter:\n\nmy_xyz_function(z = 4, x = 12, y = 3)\n\n[1] 11\n\n\n\n\nf. A more complex function\n\nfx &lt;- function(x, y, remove_na = NULL) {\n        # First operation is a sum, removing NAs\n        first &lt;- sum(c(x, y), na.rm = remove_na)\n        # Add a text message\n        text &lt;- \"This function is so cool\"\n        # Store result\n        result &lt;- first + x\n        # Print output\n        print(list(\"Message\" = text,\n                   \"1st\" = first,\n                   \"end\" = result))\n                   }\n\nRun the function with alternative arguments:\n\nfx(x = a, y = b, remove_na = FALSE)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 50\n\nfx(x = a, y = b, remove_na = TRUE)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 50\n\n\nStore the output in an object:\n\nfoo &lt;- fx(x=b, y=a)\n\n$Message\n[1] \"This function is so cool\"\n\n$`1st`\n[1] 30\n\n$end\n[1] 40",
    "crumbs": [
      "Lessons",
      "01-Essentials of R"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html",
    "href": "coding/week_06/11_weather.html",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "",
    "text": "Description\nThis lesson provides a step-by-step guide on retrieving and processing daily-weather data using R. It covers downloading data from DAYMET and processing it to generate weather summaries useful for agricultural research.\nThis tutorial focuses on how to:\nRequired packages\nlibrary(pacman)\np_load(dplyr, tidyr, stringr) # Data wrangling\np_load(purrr) # Iteration\np_load(lubridate) # Date operations\np_load(kableExtra) # Table formatting to better display\np_load(daymetr) # Weather data from Daymet\np_load(skimr) # Summarizing weather data\np_load(vegan) # Shannon Diversity Index\np_load(writexl)",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#how-to-cite-the-daymetr-package",
    "href": "coding/week_06/11_weather.html#how-to-cite-the-daymetr-package",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.1 How to cite the daymetr package",
    "text": "2.1 How to cite the daymetr package\n\nHufkens K., Basler J. D., Milliman T. Melaas E., Richardson A.D. 2018 An integrated phenology modelling framework in R: Phenology modelling with phenor. Methods in Ecology & Evolution, 9: 1-10. https://doi.org/10.1111/2041-210X.12970",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#evapotranspiration",
    "href": "coding/week_06/11_weather.html#evapotranspiration",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.2 Evapotranspiration",
    "text": "2.2 Evapotranspiration\nDaymet does not provide data on reference evapotranspiration (\\(\\text{ET}_0\\)). However, it is possible to estimate \\(\\text{ET}_0\\) using the Hargreaves and Samani approach, which only requires temperature information (Hargreaves and Samani, 1985; Raziei and Pereira, 2013). Nonetheless, the \\(\\text{ET}_{0-HS}\\) equation is reported to give unreliable estimates for daily \\(ET0\\) and therefore it should be used for 10-day periods at the shortest (Cobaner et al., 2017). \n\n2.2.1 Constants\n\n# Constants for ET0 (Cobaner et al., 2017)\n# Solar constant:\nGsc &lt;- 0.0820 # (MJ m-2 min-1)\n# Radiation adjustment coefficient (Samani, 2004)\nkRs &lt;- 0.17",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#function-to-retrieve",
    "href": "coding/week_06/11_weather.html#function-to-retrieve",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.3 Function to retrieve",
    "text": "2.3 Function to retrieve\n\n# Name of functions using dots (.) instead of underscore (_)\n# We keep underscore for other objects\nweather.daymet &lt;- function(input, dpp = 0){ \n  # Downloads the daily weather data from the DAYMET database and process it\n  # Args:\n  #  input = input file containing the locations and the start & end dates for the time series\n  #  dpp = days prior to the Start\n  # Returns:\n  #  a tibble of DAYMET weather variables for the requested time period\n  # STEP 1. Make use of metadata (locations and dates)\n  input %&gt;%\n    dplyr::mutate(\n      Weather = purrr::pmap(list(ID = ID,\n                                 # Rename vars to avoid conflicts\n                                 lat = latitude,\n                                 lon = longitude,\n                                 sta = Start - dpp,\n                                 end = End),\n                                        \n   # STEP 2. Retrieving daymet data:\n              function(ID, lat, lon, sta, end) {\n                daymetr::download_daymet(site = ID,\n                                         lat = lat, \n                                         lon = lon,\n                                       # Extracting year from date:\n                                       start = as.numeric(substr(sta, 1, 4)),\n                                       end = as.numeric(substr(end, 1, 4)),\n                                       internal = TRUE, \n                                       simplify = TRUE)})) %&gt;% \n    \n    # STEP 3. Organizing dataframe (Re-arranging rows and columns)\n    dplyr::mutate(Weather = Weather %&gt;% \n                # i. Adjusting dates format with lubridate and map()\n                purrr::map(~ \n                  dplyr::mutate(., \n                   Date = as.Date(as.numeric(yday) - 1, # Day of the year\n                   origin = paste0(year, '-01-01')),\n                   Year = year(Date),\n                   Month = month(Date),\n                   Day = mday(Date))) %&gt;%\n                # ii. Select columns of interest\n                purrr::map(~ \n                  dplyr::select(., yday, Year, Month, Day,\n                                Date, measurement, value)) %&gt;%\n                # iii. Re-arrange columns wider\n                purrr::map(~ \n                  tidyr::pivot_wider(.,\n                      names_from = measurement, values_from = value)) %&gt;%\n                # iv. Renaming variables with rename_with()\n                    purrr::map(~ rename_with(., ~c(\n                      \"DOY\",   # Date as Day of the year\n                      \"Year\",  # Year\n                      \"Month\", # Month \n                      \"Day\",   # Day of the month\n                      \"Date\",  # Date as normal format\n                      \"DL\",    # Day length (sec)\n                      \"PP\",    # Precipitation (mm)\n                      \"Rad\",   # Radiation (W/m2)\n                      \"SWE\",   # Snow water (kg/m2)\n                      \"Tmax\",  # Max. temp. (degC)\n                      \"Tmin\",  # Min. temp. (degC)\n                      \"VP\")))) %&gt;%   # Vap Pres (Pa)\n    # STEP 4. Processing data given start and ending dates:\n    dplyr::mutate(Weather = purrr::pmap(list(sta = Start - dpp, \n                                             end = End, \n                                             data = Weather), # Requested period\n                                        function(sta, end, data) {\n                                          dplyr::filter(data, Date &gt;= sta & Date &lt;= end) \n                                          })) %&gt;%\n    # STEP 5. Unnest\n    tidyr::unnest(cols = c(Weather)) %&gt;% \n    \n    # STEP 6. Converting units and adding extra variables:\n    dplyr::mutate(Rad = Rad*0.000001*DL, # Radiation (W/m2 to MJ/m2)\n                  Tmean = (Tmax+Tmin)/2, # Mean temperature (degC),\n                  VP = VP / 1000, # VP (Pa to kPa),\n                  # Creating variables for ET0 estimation:\n                  lat_rad = latitude*0.0174533,\n                  dr = 1 + 0.033*cos((2*pi/365)*DOY),\n                  Sd = 0.409*sin((2*pi/365)*DOY - 1.39),\n                  ws = acos(-tan(lat_rad)*tan(Sd)),\n                  Ra = (24*60)/(pi) * Gsc * dr * (ws*sin(lat_rad)*sin(Sd) + cos(lat_rad)*sin(ws)),\n                  ET0_HS = 0.0135 * kRs * (Ra / 2.45) * (sqrt(Tmax-Tmin)) * (Tmean + 17.8),\n                  # Extreme PP events\n                  EPE_i = case_when((PP &gt; 25) ~ 1, TRUE ~ 0),\n                  # Extreme Temp events\n                  ETE_i = case_when((Tmax &gt;= 30) ~ 1, TRUE ~ 0),\n                  # Day length (hours)\n                  DL = (DL/60)/60 \n                  ) %&gt;% \n    dplyr::select(-lat_rad, -dr, -Sd, -ws, -Ra)\n  \n}",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-retrieving-function",
    "href": "coding/week_06/11_weather.html#run-retrieving-function",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "2.4 Run retrieving function",
    "text": "2.4 Run retrieving function\n\n# Specify input = dataframe containing sites-data \n# Specify Days prior planting. Default is dpp = 0. Here we use dpp = 30.\n\ndf_weather_daymet &lt;- weather.daymet(input = df_input, dpp = 30)\n\n# Overview of the variables (useful checking for missing values):\nskimr::skim(df_weather_daymet)\n\n\nData summary\n\n\nName\ndf_weather_daymet\n\n\nNumber of rows\n911\n\n\nNumber of columns\n25\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n5\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart\n0\n1\n2002-04-25\n2010-05-20\n2005-05-01\n3\n\n\nEnd\n0\n1\n2002-09-30\n2010-10-10\n2006-09-30\n3\n\n\nFlo\n0\n1\n2002-07-20\n2010-07-15\n2006-07-21\n3\n\n\nSeFi\n0\n1\n2002-08-08\n2010-08-15\n2006-08-10\n3\n\n\nDate\n0\n1\n2002-03-26\n2010-10-10\n2005-12-23\n911\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlatitude\n0\n1\n43.20\n1.03\n42.45\n42.45\n42.45\n43.65\n45.08\n▇▁▃▁▂\n\n\nlongitude\n0\n1\n-80.33\n2.49\n-81.88\n-81.88\n-81.88\n-80.40\n-75.35\n▇▃▁▁▂\n\n\nDOY\n0\n1\n184.55\n78.43\n1.00\n128.00\n185.00\n242.00\n365.00\n▂▆▇▇▂\n\n\nYear\n0\n1\n2005.63\n2.56\n2002.00\n2005.00\n2005.00\n2006.00\n2010.00\n▆▇▇▁▅\n\n\nMonth\n0\n1\n6.58\n2.57\n1.00\n5.00\n7.00\n8.00\n12.00\n▃▇▇▇▃\n\n\nDay\n0\n1\n15.83\n8.88\n1.00\n8.00\n16.00\n24.00\n31.00\n▇▇▆▇▆\n\n\nDL\n0\n1\n13.17\n1.88\n8.89\n12.06\n13.68\n14.78\n15.44\n▂▂▃▅▇\n\n\nPP\n0\n1\n2.53\n5.31\n0.00\n0.00\n0.00\n2.74\n45.09\n▇▁▁▁▁\n\n\nRad\n0\n1\n16.23\n6.78\n1.40\n11.31\n16.61\n21.63\n29.86\n▃▆▇▇▃\n\n\nSWE\n0\n1\n0.98\n4.86\n0.00\n0.00\n0.00\n0.00\n41.65\n▇▁▁▁▁\n\n\nTmax\n0\n1\n18.74\n9.40\n-7.32\n12.90\n21.21\n26.16\n34.99\n▁▃▅▇▅\n\n\nTmin\n0\n1\n8.95\n8.13\n-15.01\n2.83\n10.31\n15.21\n24.83\n▁▃▅▇▃\n\n\nVP\n0\n1\n1.26\n0.61\n0.19\n0.75\n1.21\n1.70\n3.14\n▇▇▇▃▁\n\n\nTmean\n0\n1\n13.84\n8.60\n-11.16\n7.73\n16.12\n20.52\n28.40\n▁▃▅▇▆\n\n\nET0_HS\n0\n1\n3.33\n1.66\n0.26\n2.03\n3.48\n4.69\n7.13\n▆▆▇▇▂\n\n\nEPE_i\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nETE_i\n0\n1\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\n\n\n# Exporting data as a .csv file\n# write.csv(df.weather.daymet, row.names = FALSE, na = '', file = paste0(path, 'Output_daymet.csv'))",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#define-time-intervals",
    "href": "coding/week_06/11_weather.html#define-time-intervals",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.1 Define time intervals",
    "text": "3.1 Define time intervals\nIn this section we create time intervals during the cropping season using pre-specified dates as columns at the initial data table with site information. \nThe user can apply: i) a unique seasonal interval (season), ii) even intervals (even), or iii) customized intervals (custom). \n\n3.1.1 Full-season interval\n\n# Defining season-intervals\nseason_interval &lt;- \n  df_input %&gt;%\n  dplyr::mutate(Interval = \"Season\") %&gt;%\n  dplyr::rename(Start.in = Start, End.in = End) %&gt;%\n  dplyr::select(ID, Site, Interval, Start.in, End.in)\n\n# Creating a table to visualize results\nkable(season_interval) %&gt;% \n  kable_styling(latex_options = c(\"striped\"),\n                position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nSeason\n2002-04-25\n2002-09-30\n\n\n2\nRidgetown\nSeason\n2005-05-01\n2006-09-30\n\n\n3\nWinchester\nSeason\n2010-05-20\n2010-10-10\n\n\n\n\n\n\n\n\n\n3.1.2 Even-intervals\n\n# Number of intervals:\nn &lt;- 4 \n# Days prior planting:\ndpp &lt;- 30 \n\n# Defining even-intervals:\neven_intervals &lt;- \n  df_input %&gt;% \n  # Create new data:\n  dplyr::mutate(Intervals = \n          purrr::map2(.x = Start, .y = End,\n                      .f = ~ data.frame(\n                    Interval = c(\"Prev\", # Prior to start date \n                                 LETTERS[0:n]), # Each interval from start date\n                      # Start\n                      Start.in = c(.x - dpp, seq.Date(.x, .y + 1, length.out = n + 1)[1:n]),                              # End\n                      End.in = c(.x - 1, seq.Date(.x, .y + 1, length.out = n + 1)[2:(n + 1)]))) ) %&gt;% \n  # Selecting columns:\n  dplyr::select(ID, Site, Intervals) %&gt;% \n  tidyr::unnest(cols = c(Intervals))\n\n# Creating a table to visualize results\nkable(even_intervals) %&gt;% \n  kable_styling(latex_options = c(\"striped\"), \n                position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nPrev\n2002-03-26\n2002-04-24\n\n\n1\nElora\nA\n2002-04-25\n2002-06-03\n\n\n1\nElora\nB\n2002-06-03\n2002-07-13\n\n\n1\nElora\nC\n2002-07-13\n2002-08-22\n\n\n1\nElora\nD\n2002-08-22\n2002-10-01\n\n\n2\nRidgetown\nPrev\n2005-04-01\n2005-04-30\n\n\n2\nRidgetown\nA\n2005-05-01\n2005-09-07\n\n\n2\nRidgetown\nB\n2005-09-07\n2006-01-15\n\n\n2\nRidgetown\nC\n2006-01-15\n2006-05-24\n\n\n2\nRidgetown\nD\n2006-05-24\n2006-10-01\n\n\n3\nWinchester\nPrev\n2010-04-20\n2010-05-19\n\n\n3\nWinchester\nA\n2010-05-20\n2010-06-25\n\n\n3\nWinchester\nB\n2010-06-25\n2010-07-31\n\n\n3\nWinchester\nC\n2010-07-31\n2010-09-05\n\n\n3\nWinchester\nD\n2010-09-05\n2010-10-11\n\n\n\n\n\n\n\n\n\n3.1.3 Custom-intervals\n\n# Count the number of interval columns (assuming intervals start at column \"Start\")\ni &lt;- df_input %&gt;% dplyr::select(Start:last_col()) %&gt;% ncol()\n\n# Defining custom-intervals\ncustom_intervals &lt;- \n  df_input %&gt;% \n  dplyr::mutate(Intervals = # Create\n                  purrr::pmap(# List of object to iterate over\n                              .l = list(x = Start - dpp,\n                                        y = Start,\n                                        z = Flo,\n                                        m = SeFi,\n                                        k = End),\n                              # The function to run\n                              .f = function(x, y, z, m, k) {\n                      data.frame(# New data\n                        Interval = c(LETTERS[1:i]),\n                        Name = c(\"Prev\", \"Plant-Flo\", \"Flo-SeFi\", \"SeFi-End\"),\n                        Start.in = c(x, y, z, m),\n                        End.in = c(y-1, z-1, m-1, k))}\n                    )) %&gt;% \n  # Selecting columns:\n  dplyr::select(ID,, Site, Intervals) %&gt;% \n  tidyr::unnest(cols = c(Intervals))\n\n# Creating a table to visualize results:\nkable(custom_intervals) %&gt;% \n  kable_styling(latex_options = c(\"striped\"), position = \"center\", font_size = 10)\n\n\n\n\nID\nSite\nInterval\nName\nStart.in\nEnd.in\n\n\n\n\n1\nElora\nA\nPrev\n2002-03-26\n2002-04-24\n\n\n1\nElora\nB\nPlant-Flo\n2002-04-25\n2002-07-19\n\n\n1\nElora\nC\nFlo-SeFi\n2002-07-20\n2002-08-07\n\n\n1\nElora\nD\nSeFi-End\n2002-08-08\n2002-09-30\n\n\n2\nRidgetown\nA\nPrev\n2005-04-01\n2005-04-30\n\n\n2\nRidgetown\nB\nPlant-Flo\n2005-05-01\n2006-07-20\n\n\n2\nRidgetown\nC\nFlo-SeFi\n2006-07-21\n2006-08-09\n\n\n2\nRidgetown\nD\nSeFi-End\n2006-08-10\n2006-09-30\n\n\n3\nWinchester\nA\nPrev\n2010-04-20\n2010-05-19\n\n\n3\nWinchester\nB\nPlant-Flo\n2010-05-20\n2010-07-14\n\n\n3\nWinchester\nC\nFlo-SeFi\n2010-07-15\n2010-08-14\n\n\n3\nWinchester\nD\nSeFi-End\n2010-08-15\n2010-10-10",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#summary-function",
    "href": "coding/week_06/11_weather.html#summary-function",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.2 Summary function",
    "text": "3.2 Summary function\nFor each of the period or interval of interest a variety of variables can be created. Here, we present a set of variables that can capture environmental variations that might be missing by analyzing standard weather data (precipitations, temperature, radiation). These variables represent an example that was used for studying influence of weather in corn yields by Correndo et al. (2021). \n\n# Defining the function to summarize DAYMET and/or NASA-POWER\nsummary.daymet &lt;- function(input, intervals) {\n  # Creates summaries of the DAYMET daily data over the requested period\n  # Args:\n  #  input = a weather data object such as df.weather.daymet with the daily weather data\n  #  intervals = a tibble with the start and end date for the summary period\n  # STEP 1. \n  intervals %&gt;%\n    # NOTE: mergeing on ID only as the key, so remove Site:\n    dplyr::select(-Site) %&gt;%\n    # Merging weather data:\n    dplyr::left_join(input %&gt;%\n                     # Nesting weather data back for each site-ID:\n                     dplyr::select_if(names(.) %in% \n                                        c(\"ID\", \"Crop\", \"Site\",\n                                          \"Date\",\"DL\", \"PP\", \n                                          \"Rad\", \"Tmax\", \"Tmin\",\n                                          \"Tmean\", \"VP\", \"ET0_HS\")) %&gt;%\n                     dplyr::group_by(ID, Crop, Site) %&gt;% \n                     tidyr::nest(Weather = -c(ID, Crop, Site)) %&gt;%\n                     dplyr::ungroup(), \n                   by = c(\"ID\")) %&gt;% \n    # STEP 2. Create Weather column filtering for desired period only.\n    dplyr::mutate(Weather = purrr::pmap(\n      .l = list(x = Start.in,\n                y = End.in, \n                data = Weather),\n      # Filter function\n      .f = function(x, y, data) {\n        dplyr::filter(data, Date &gt;= x & Date &lt; y)} ) ) %&gt;% \n    \n    # STEP 3. Calculation of variables (daily) that will be useful to summarize the intervals \n    dplyr::mutate(Weather = Weather %&gt;% \n      # User must adapt depending on the crop (these ay be corn-specific)\n        purrr::map(~ mutate(.,\n                            # Extreme Precip. event:\n                            EPEi = case_when(PP &gt; 25 ~1, TRUE ~ 0),\n                            # Extreme Temp. event:\n                            ETEi = case_when(Tmax &gt;= 30 ~1, TRUE ~ 0), \n                            # Tmax factor,  crop heat units (CHU):\n                            Ymax = case_when(Tmax &lt; 10 ~ 0,\n                                             Tmax &gt;= 30 ~ 0,\n                                    TRUE ~ 3.33*(Tmax-10) - 0.084*(Tmax-10)^2),\n                            # Tmin factor, Crop heat units (CHU):\n                            Ymin = case_when(Tmin &lt; 4.44 ~ 0, \n                                    TRUE ~ 1.8*(Tmin-4.44)), \n                            # Daily CHU:\n                            Yavg = (Ymax + Ymin)/2,\n                            # For WHEAT (diff. base temp and winter negatives)\n                            # # Tmin threshold Growing Degrees:\n                            # Gmin = case_when(Tmin &gt;= 0 ~ Tmin, TRUE ~ 0),\n                            # # Tmax threshold Growing Degrees:\n                            # Gmax = case_when(Tmax &gt; 30 ~ 30,\n                            #                  Tmin &lt; 0 ~ 0, \n                            #                  between(Tmax, 0, 30) ~ Tmax),\n                            # # Daily Growing Degree Units:\n                            # GDU = ((Gmin + Gmax)/2) - 0,\n                            # GDD_c = cumsum(GDU) \n                            # For CORN, SOYBEAN (Base temp = 10)\n                            # Tmin threshold Growing Degrees:\n                            Gmin = case_when(Tmin &gt;= 10 ~ Tmin, TRUE ~ 10),\n                            # Tmax threshold Growing Degrees:\n                            Gmax = case_when(Tmax &lt;= 30 ~ Tmax,\n                                             Tmax &gt; 30 ~ 30,\n                                             Tmin &lt;= 10 ~ 10,\n                                             TRUE ~ 30),\n                            # Daily Growing Degree Units:\n                            GDU = ((Gmin + Gmax)/2) - 10) ) ) %&gt;% \n    \n    # STEP 4. Summary for each variable over the period of interest:\n    dplyr::mutate(\n      # Duration of interval (days):\n      Dur = Weather %&gt;% purrr::map(~nrow(.)),\n      # Accumulated PP (mm):\n      PP = Weather %&gt;% purrr::map(~sum(.$PP)),\n      # Mean Temp (C):\n      Tmean = Weather %&gt;% purrr::map(~mean(.$Tmean)),\n      # Accumulated Rad (MJ/m2):\n      Rad = Weather %&gt;% purrr::map(~sum(.$Rad)),\n      # Accumulated VP (kPa):\n      VP = Weather %&gt;% purrr::map(~sum(.$VP)),\n      # Accumulated ET0 (mm):\n      ET0_HS = Weather %&gt;% purrr::map(~sum(.$ET0_HS)),\n      # Number of ETE (#):\n      ETE = Weather %&gt;% purrr::map(~sum(.$ETEi)),\n      # Number of EPE (#):\n      EPE = Weather %&gt;% purrr::map(~sum(.$EPEi)),\n      # Accumulated Crop Heat Units (CHU):\n      CHU = Weather %&gt;% purrr::map(~sum(.$Yavg)),\n      # Shannon Diversity Index for PP:\n      SDI = Weather %&gt;% purrr::map(~ vegan::diversity(.$PP, index = \"shannon\")/log(length(.$PP))),\n      # Accumulated Growing Degree Days (GDD):\n      GDD =  Weather %&gt;% purrr::map(~sum(.$GDU))) %&gt;% \n    \n    # Additional indices and final units:\n    dplyr::select(-Weather) %&gt;% \n    # DS: `cols` is now required when using unnest()\n    tidyr::unnest(cols = c(Dur, PP, Tmean, Rad, VP, ET0_HS, ETE, EPE, CHU, SDI, GDD)) %&gt;% \n    dplyr::mutate(\n      # Photo-thermal quotient (Q):\n      Q_chu = Rad/CHU,\n      Q_gdd = Rad/GDD,\n      # Abundant and Well Distributed Water:\n      AWDR = PP*SDI) \n  }",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-summaries",
    "href": "coding/week_06/11_weather.html#run-summaries",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "3.3 Run summaries",
    "text": "3.3 Run summaries\n\n3.3.1 Seasonal\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\nseason_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = season_interval)\n\n# Skim data\nskimr::skim(season_summary_daymet)\n\n\nData summary\n\n\nName\nseason_summary_daymet\n\n\nNumber of rows\n3\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n6\n6\n0\n1\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-04-25\n2010-05-20\n2005-05-01\n3\n\n\nEnd.in\n0\n1\n2002-09-30\n2010-10-10\n2006-09-30\n3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n272.67\n211.73\n143.00\n150.50\n158.00\n337.50\n517.00\n▇▁▁▁▃\n\n\nPP\n0\n1\n683.57\n395.63\n361.53\n462.75\n563.98\n844.58\n1125.19\n▇▇▁▁▇\n\n\nTmean\n0\n1\n16.04\n3.20\n12.55\n14.63\n16.70\n17.78\n18.85\n▇▁▁▇▇\n\n\nRad\n0\n1\n4359.96\n2895.00\n2394.44\n2697.73\n3001.02\n5342.72\n7684.42\n▇▁▁▁▃\n\n\nVP\n0\n1\n358.94\n237.60\n218.15\n221.77\n225.40\n429.33\n633.26\n▇▁▁▁▃\n\n\nET0_HS\n0\n1\n925.30\n517.24\n594.03\n627.29\n660.56\n1090.94\n1521.32\n▇▁▁▁▃\n\n\nETE\n0\n1\n17.67\n3.79\n15.00\n15.50\n16.00\n19.00\n22.00\n▇▁▁▁▃\n\n\nEPE\n0\n1\n2.67\n2.52\n0.00\n1.50\n3.00\n4.00\n5.00\n▇▁▇▁▇\n\n\nCHU\n0\n1\n4323.15\n2564.95\n2740.35\n2843.47\n2946.59\n5114.55\n7282.51\n▇▁▁▁▃\n\n\nSDI\n0\n1\n0.74\n0.03\n0.71\n0.74\n0.76\n0.76\n0.76\n▃▁▁▁▇\n\n\nGDD\n0\n1\n1677.80\n742.37\n1208.96\n1249.85\n1290.73\n1912.22\n2533.71\n▇▁▁▁▃\n\n\nQ_chu\n0\n1\n0.99\n0.15\n0.81\n0.93\n1.06\n1.08\n1.10\n▃▁▁▁▇\n\n\nQ_gdd\n0\n1\n2.46\n0.59\n1.86\n2.17\n2.48\n2.76\n3.03\n▇▁▇▁▇\n\n\nAWDR\n0\n1\n513.54\n307.39\n257.93\n343.01\n428.09\n641.35\n854.62\n▇▇▁▁▇\n\n\n\n\n# Creating a table to visualize results\nkbl(season_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nSeason\n2002-04-25\n2002-09-30\nCorn\nElora\n158\n361.53\n16.70405\n3001.020\n218.1533\n660.5558\n16\n0\n2740.348\n0.7134269\n1208.965\n1.0951238\n2.482305\n257.9252\n\n\n2\nSeason\n2005-05-01\n2006-09-30\nCorn\nRidgetown\n517\n1125.19\n12.55279\n7684.417\n633.2642\n1521.3225\n22\n5\n7282.508\n0.7595347\n2533.715\n1.0551883\n3.032866\n854.6209\n\n\n3\nSeason\n2010-05-20\n2010-10-10\nSoybean\nWinchester\n143\n563.98\n18.85315\n2394.440\n225.3956\n594.0311\n15\n3\n2946.587\n0.7590440\n1290.725\n0.8126148\n1.855113\n428.0856\n\n\n\n\n\n\n\n\n\n3.3.2 Even\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\neven_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = even_intervals)\n\n# Skim data\nskimr::skim(even_summary_daymet)\n\n\nData summary\n\n\nName\neven_summary_daymet\n\n\nNumber of rows\n15\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n1\n4\n0\n5\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-03-26\n2010-09-05\n2005-09-07\n15\n\n\nEnd.in\n0\n1\n2002-04-24\n2010-10-11\n2006-01-15\n15\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n60.53\n43.22\n29.00\n36.00\n39.00\n84.50\n130.00\n▇▁▁▁▃\n\n\nPP\n0\n1\n153.76\n92.73\n45.94\n92.45\n113.04\n208.32\n364.82\n▇▃▂▂▁\n\n\nTmean\n0\n1\n14.46\n6.40\n4.11\n8.35\n18.00\n19.57\n22.11\n▃▃▁▁▇\n\n\nRad\n0\n1\n981.82\n680.51\n402.74\n595.51\n707.50\n924.39\n2563.53\n▇▂▁▁▂\n\n\nVP\n0\n1\n76.45\n62.46\n21.08\n37.52\n62.84\n77.33\n218.10\n▇▆▁▁▂\n\n\nET0_HS\n0\n1\n201.85\n152.45\n63.49\n117.28\n174.13\n198.55\n576.34\n▇▇▁▁▂\n\n\nETE\n0\n1\n3.53\n4.34\n0.00\n0.00\n2.00\n6.00\n14.00\n▇▃▂▁▁\n\n\nEPE\n0\n1\n0.60\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nCHU\n0\n1\n908.91\n846.64\n145.26\n415.56\n782.24\n865.76\n2963.26\n▇▇▁▁▂\n\n\nSDI\n0\n1\n0.64\n0.07\n0.43\n0.61\n0.65\n0.69\n0.73\n▁▁▃▅▇\n\n\nGDD\n0\n1\n347.29\n409.21\n-112.18\n98.38\n319.40\n404.69\n1285.44\n▇▇▁▁▂\n\n\nQ_chu\n0\n1\n1.57\n1.11\n0.73\n0.82\n0.92\n2.28\n3.59\n▇▁▁▁▂\n\n\nQ_gdd\n0\n1\n7.47\n19.56\n-15.99\n1.79\n2.15\n7.00\n74.52\n▇▇▁▁▁\n\n\nAWDR\n0\n1\n102.83\n68.43\n26.06\n57.67\n77.62\n145.51\n263.99\n▇▃▂▂▁\n\n\n\n\n# Creating a table to visualize results\nkbl(even_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nPrev\n2002-03-26\n2002-04-24\nCorn\nElora\n29\n95.53\n5.660000\n505.2583\n21.08116\n63.48611\n0\n1\n154.3458\n0.6357661\n6.780\n3.2735469\n74.521875\n60.73474\n\n\n1\nA\n2002-04-25\n2002-06-03\nCorn\nElora\n40\n113.04\n8.802250\n800.8372\n33.52344\n128.07920\n0\n0\n292.5157\n0.6866269\n84.400\n2.7377578\n9.488593\n77.61630\n\n\n1\nB\n2002-06-03\n2002-07-13\nCorn\nElora\n40\n98.94\n19.117375\n856.4385\n62.83969\n202.20827\n5\n0\n823.2502\n0.6508401\n371.250\n1.0403138\n2.306905\n64.39412\n\n\n1\nC\n2002-07-13\n2002-08-22\nCorn\nElora\n40\n89.36\n20.994750\n752.2604\n69.43155\n194.90035\n7\n0\n893.7629\n0.5993348\n437.950\n0.8416778\n1.717685\n53.55656\n\n\n1\nD\n2002-08-22\n2002-10-01\nCorn\nElora\n39\n60.83\n17.997436\n604.4711\n53.82624\n138.43587\n4\n0\n754.0653\n0.4283610\n324.600\n0.8016164\n1.862203\n26.05720\n\n\n2\nPrev\n2005-04-01\n2005-04-30\nCorn\nRidgetown\n29\n105.04\n7.902759\n522.1453\n21.72264\n75.06876\n0\n0\n145.2592\n0.5923778\n41.800\n3.5945772\n12.491513\n62.22337\n\n\n2\nA\n2005-05-01\n2005-09-07\nCorn\nRidgetown\n130\n225.28\n19.555615\n2563.5280\n216.16182\n576.34481\n14\n0\n2793.4159\n0.6940917\n1285.435\n0.9177037\n1.994288\n156.36498\n\n\n2\nB\n2005-09-07\n2006-01-15\nCorn\nRidgetown\n129\n255.66\n6.956744\n992.3342\n115.05132\n174.13068\n0\n2\n1025.8802\n0.6752569\n112.950\n0.9673003\n8.785606\n172.63617\n\n\n2\nC\n2006-01-15\n2006-05-24\nCorn\nRidgetown\n130\n287.94\n4.109692\n1793.2989\n85.09908\n236.08072\n0\n1\n510.1460\n0.6804557\n-112.175\n3.5152659\n-15.986619\n195.93041\n\n\n2\nD\n2006-05-24\n2006-10-01\nCorn\nRidgetown\n129\n364.82\n19.592442\n2341.9490\n218.10287\n536.30800\n8\n2\n2963.2599\n0.7236240\n1249.530\n0.7903286\n1.874264\n263.99249\n\n\n3\nPrev\n2010-04-20\n2010-05-19\nSoybean\nWinchester\n29\n45.94\n11.114483\n586.5492\n23.85418\n106.48366\n0\n0\n320.9666\n0.5915947\n112.350\n1.8274459\n5.220732\n27.17786\n\n\n3\nA\n2010-05-20\n2010-06-25\nSoybean\nWinchester\n36\n119.18\n18.890417\n685.8864\n51.43909\n177.92863\n2\n1\n782.2378\n0.7315317\n319.395\n0.8768259\n2.147455\n87.18395\n\n\n3\nB\n2010-06-25\n2010-07-31\nSoybean\nWinchester\n36\n85.30\n22.106111\n707.4951\n69.55371\n184.06589\n9\n0\n817.1332\n0.6402241\n426.640\n0.8658260\n1.658295\n54.61112\n\n\n3\nC\n2010-07-31\n2010-09-05\nSoybean\nWinchester\n36\n168.14\n20.660556\n612.1467\n63.55369\n150.74414\n4\n1\n837.7504\n0.6259308\n382.745\n0.7307030\n1.599359\n105.24400\n\n\n3\nD\n2010-09-05\n2010-10-11\nSoybean\nWinchester\n36\n191.36\n13.492083\n402.7351\n41.51125\n83.48533\n0\n1\n519.7147\n0.7037089\n165.755\n0.7749157\n2.429701\n134.66174\n\n\n\n\n\n\n\n\n\n3.3.3 Custom\n\n# Run the summary\n# input = dataframe containing the data (from daymet).\n# intervals = type of intervals (season, custom or even)\n\ncustom_summary_daymet &lt;-\n  summary.daymet(input = df_weather_daymet,\n                           intervals = custom_intervals)\n\n# Skim data\nskimr::skim(custom_summary_daymet)\n\n\nData summary\n\n\nName\ncustom_summary_daymet\n\n\nNumber of rows\n12\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nInterval\n0\n1\n1\n1\n0\n4\n0\n\n\nName\n0\n1\n4\n9\n0\n4\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart.in\n0\n1\n2002-03-26\n2010-08-15\n2005-12-10\n12\n\n\nEnd.in\n0\n1\n2002-04-24\n2010-10-10\n2006-07-30\n12\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDur\n0\n1\n74.92\n118.16\n18.00\n29.00\n40.50\n55.25\n445.00\n▇▁▁▁▁\n\n\nPP\n0\n1\n191.43\n233.36\n45.94\n82.21\n100.29\n188.89\n895.24\n▇▁▁▁▁\n\n\nTmean\n0\n1\n15.79\n5.72\n5.66\n11.36\n16.97\n20.28\n23.58\n▅▅▅▇▇\n\n\nRad\n0\n1\n1214.16\n1723.57\n332.92\n517.92\n653.53\n913.03\n6546.07\n▇▁▁▁▁\n\n\nVP\n0\n1\n94.38\n133.33\n21.08\n30.15\n66.72\n81.44\n507.83\n▇▁▁▁▁\n\n\nET0_HS\n0\n1\n249.35\n330.83\n63.49\n89.12\n151.66\n222.19\n1261.95\n▇▁▁▁▁\n\n\nETE\n0\n1\n4.42\n5.12\n0.00\n0.00\n3.50\n6.25\n18.00\n▇▅▁▁▁\n\n\nEPE\n0\n1\n0.75\n1.48\n0.00\n0.00\n0.00\n1.00\n5.00\n▇▁▁▁▁\n\n\nCHU\n0\n1\n1119.50\n1476.59\n145.26\n389.46\n852.85\n1123.81\n5634.82\n▇▁▁▁▁\n\n\nSDI\n0\n1\n0.64\n0.08\n0.52\n0.58\n0.61\n0.72\n0.75\n▆▆▂▂▇\n\n\nGDD\n0\n1\n427.22\n482.32\n6.78\n178.38\n362.19\n483.34\n1848.98\n▇▇▁▁▁\n\n\nQ_chu\n0\n1\n1.39\n1.02\n0.68\n0.76\n0.86\n1.54\n3.59\n▇▂▁▁▂\n\n\nQ_gdd\n0\n1\n9.28\n20.78\n1.30\n1.79\n1.94\n3.96\n74.52\n▇▁▁▁▁\n\n\nAWDR\n0\n1\n133.24\n178.12\n27.18\n44.11\n61.48\n140.08\n668.48\n▇▁▁▁▁\n\n\n\n\n# Creating a table to visualize results\nkbl(custom_summary_daymet) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", \n                latex_options = c(\"scale_down\"))\n\n\n\n\nID\nInterval\nName\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\nA\nPrev\n2002-03-26\n2002-04-24\nCorn\nElora\n29\n95.53\n5.660000\n505.2583\n21.08116\n63.48611\n0\n1\n154.3458\n0.6357661\n6.780\n3.2735469\n74.521875\n60.73474\n\n\n1\nB\nPlant-Flo\n2002-04-25\n2002-07-19\nCorn\nElora\n85\n211.98\n14.478353\n1767.0401\n105.60612\n358.83259\n7\n0\n1218.2390\n0.7117824\n518.900\n1.4504872\n3.405358\n150.88364\n\n\n1\nC\nFlo-SeFi\n2002-07-20\n2002-08-07\nCorn\nElora\n18\n61.48\n21.249444\n333.0742\n32.24373\n89.48994\n3\n0\n412.2869\n0.5459508\n200.385\n0.8078700\n1.662171\n33.56506\n\n\n1\nD\nSeFi-End\n2002-08-08\n2002-09-30\nCorn\nElora\n53\n88.07\n18.649434\n861.4328\n77.13351\n203.26534\n6\n0\n1063.5927\n0.5490199\n471.480\n0.8099273\n1.827082\n48.35218\n\n\n2\nA\nPrev\n2005-04-01\n2005-04-30\nCorn\nRidgetown\n29\n105.04\n7.902759\n522.1453\n21.72264\n75.06876\n0\n0\n145.2592\n0.5923778\n41.800\n3.5945772\n12.491513\n62.22337\n\n\n2\nB\nPlant-Flo\n2005-05-01\n2006-07-20\nCorn\nRidgetown\n445\n895.24\n11.445236\n6546.0731\n507.83287\n1261.94680\n18\n5\n5634.8226\n0.7467051\n1848.980\n1.1617177\n3.540370\n668.48028\n\n\n2\nC\nFlo-SeFi\n2006-07-21\n2006-08-09\nCorn\nRidgetown\n19\n71.87\n23.582632\n332.9171\n42.49005\n87.99388\n4\n0\n492.0876\n0.5904150\n255.540\n0.6765403\n1.302798\n42.43313\n\n\n2\nD\nSeFi-End\n2006-08-10\n2006-09-30\nCorn\nRidgetown\n51\n158.08\n17.779314\n760.1808\n79.45679\n161.56523\n0\n0\n1104.2616\n0.6849177\n407.350\n0.6884064\n1.866161\n108.27178\n\n\n3\nA\nPrev\n2010-04-20\n2010-05-19\nSoybean\nWinchester\n29\n45.94\n11.114483\n586.5492\n23.85418\n106.48366\n0\n0\n320.9666\n0.5915947\n112.350\n1.8274459\n5.220732\n27.17786\n\n\n3\nB\nPlant-Flo\n2010-05-20\n2010-07-14\nSoybean\nWinchester\n55\n181.19\n19.959273\n1067.8207\n87.38554\n278.97376\n8\n1\n1182.4459\n0.7532597\n538.705\n0.9030610\n1.982199\n136.48312\n\n\n3\nC\nFlo-SeFi\n2010-07-15\n2010-08-14\nSoybean\nWinchester\n30\n85.66\n21.524167\n566.9201\n56.30866\n141.75997\n3\n0\n743.4511\n0.5215225\n346.630\n0.7625520\n1.635519\n44.67362\n\n\n3\nD\nSeFi-End\n2010-08-15\n2010-10-10\nSoybean\nWinchester\n56\n297.13\n16.158571\n720.5205\n77.41198\n163.36417\n4\n2\n962.2477\n0.7257673\n377.755\n0.7487890\n1.907375\n215.64723",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#locations-and-dates-data",
    "href": "coding/week_06/11_weather.html#locations-and-dates-data",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.1 Locations and Dates Data",
    "text": "4.1 Locations and Dates Data\n\n# For historical data (from Jan-01-2000 to Dec-31-2022)\ndf_historical &lt;- data.frame(ID = c('1', '2', '3'),\n                      # Dates as YYYY_MM_DD, using \"_\" to separate\n                      Start = c('2000-01-01', '2000-01-01', '2000-01-01'),\n                      End = c('2022-12-31', '2022-12-31', '2022-12-31')) %&gt;% \n  # Express start and end as dates using \"across\"\n  dplyr::mutate(across(Start:End, ~as.Date(., format = '%Y-%m-%d')))\n\n# Merge for historical weather data\ndf_historical &lt;- df_sites %&gt;% dplyr::left_join(df_historical, by = \"ID\")",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#retrieve-from-daymet",
    "href": "coding/week_06/11_weather.html#retrieve-from-daymet",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.2 Retrieve from DAYMET",
    "text": "4.2 Retrieve from DAYMET\n\n# Specify input = dataframe containing historical dates from sites \nhist_weather_daymet &lt;- weather.daymet(input = df_historical)\n\n# This is a large data frame (21900 obs), so good to have an overview\n# Skim data\nskimr::skim(hist_weather_daymet)\n\n\nData summary\n\n\nName\nhist_weather_daymet\n\n\nNumber of rows\n25185\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n3\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n1\n1\n0\n3\n0\n\n\nCrop\n0\n1\n4\n7\n0\n2\n0\n\n\nSite\n0\n1\n5\n10\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStart\n0\n1\n2000-01-01\n2000-01-01\n2000-01-01\n1\n\n\nEnd\n0\n1\n2022-12-31\n2022-12-31\n2022-12-31\n1\n\n\nDate\n0\n1\n2000-01-01\n2022-12-31\n2011-07-02\n8395\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlatitude\n0\n1\n43.73\n1.08\n42.45\n42.45\n43.65\n45.08\n45.08\n▇▁▇▁▇\n\n\nlongitude\n0\n1\n-79.21\n2.80\n-81.88\n-81.88\n-80.40\n-75.35\n-75.35\n▇▇▁▁▇\n\n\nDOY\n0\n1\n183.00\n105.37\n1.00\n92.00\n183.00\n274.00\n365.00\n▇▇▇▇▇\n\n\nYear\n0\n1\n2011.00\n6.63\n2000.00\n2005.00\n2011.00\n2017.00\n2022.00\n▇▆▇▆▇\n\n\nMonth\n0\n1\n6.52\n3.45\n1.00\n4.00\n7.00\n10.00\n12.00\n▇▅▅▅▇\n\n\nDay\n0\n1\n15.72\n8.79\n1.00\n8.00\n16.00\n23.00\n31.00\n▇▇▇▇▆\n\n\nDL\n0\n1\n12.00\n2.26\n8.56\n9.80\n12.00\n14.20\n15.44\n▇▅▅▅▇\n\n\nPP\n0\n1\n2.58\n5.27\n0.00\n0.00\n0.00\n2.92\n85.53\n▇▁▁▁▁\n\n\nRad\n0\n1\n13.40\n7.37\n0.74\n6.84\n12.59\n19.32\n32.59\n▇▇▇▆▂\n\n\nSWE\n0\n1\n16.21\n32.83\n0.00\n0.00\n0.00\n12.65\n211.59\n▇▁▁▁▁\n\n\nTmax\n0\n1\n12.75\n11.54\n-22.58\n2.94\n13.55\n23.18\n36.23\n▁▅▇▇▅\n\n\nTmin\n0\n1\n3.03\n10.49\n-31.93\n-3.83\n3.42\n11.72\n25.02\n▁▂▇▇▅\n\n\nVP\n0\n1\n0.93\n0.59\n0.04\n0.46\n0.77\n1.35\n3.17\n▇▅▃▁▁\n\n\nTmean\n0\n1\n7.89\n10.85\n-26.25\n-0.37\n8.46\n17.42\n29.88\n▁▃▇▇▅\n\n\nET0_HS\n0\n1\n2.42\n1.83\n-0.45\n0.72\n1.97\n4.00\n7.38\n▇▅▅▅▁\n\n\nEPE_i\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nETE_i\n0\n1\n0.03\n0.17\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#summary-functions",
    "href": "coding/week_06/11_weather.html#summary-functions",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.3 Summary functions",
    "text": "4.3 Summary functions\n\n4.3.1 By year\n\n# Defining function to summarize historical weather (years)\n\n# Revised function:\nhistorical.years &lt;- function(hist.data) {\n  # Creates an input tibble with the start and end date for each year a summary is desired\n  # Args:\n  #  hist.data = data frame containing the historical weather data to summarize (must be complete years)\n  # Returns:\n  #  a tibble of monthly summaries for each ID\n  #\n  # By year:\n  hist.data %&gt;% \n    dplyr::group_by(ID, Crop, Site) %&gt;%\n    tidyr::nest() %&gt;%\n    dplyr::mutate(the_Dates = purrr::map(data, function(.data) {.data %&gt;% dplyr::group_by(Year) %&gt;% \n        dplyr::summarise(Start.in = min(Date), End.in = max(Date), .groups = \"drop\")})) %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::select(ID, Site, the_Dates) %&gt;%\n    tidyr::unnest(cols = c(the_Dates))\n}\n\n\n\n4.3.2 By year-month\n\n# Defining function to summarize historical weather (years & months)\n# Revised function:\nhistorical.yearmonths &lt;- function(hist.data) {\n  # Creates an input tibble with the start and end date for each year & month a summary is desired (monthly summaries)\n  # Args:\n  #  hist.data = data frame containing the historical weather data to summarize (must be complete years)\n  # Returns:\n  #  a tibble of monthly summaries for each ID\n  #\n  # By month in year:\n  hist.data %&gt;% \n    dplyr::group_by(ID, Crop, Site) %&gt;%\n    tidyr::nest() %&gt;%\n    dplyr::mutate(the_Dates = purrr::map(data, function(.data) {.data %&gt;% \n        dplyr::group_by(Year, Month) %&gt;% \n        dplyr::summarise(Start.in = min(Date), End.in = max(Date), .groups = \"drop\")})) %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::select(ID, Site, the_Dates) %&gt;%\n    tidyr::unnest(cols = c(the_Dates))\n}",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_06/11_weather.html#run-historical-summaries",
    "href": "coding/week_06/11_weather.html#run-historical-summaries",
    "title": "Retrieving and Processing Weather Data with R",
    "section": "4.4 Run Historical Summaries",
    "text": "4.4 Run Historical Summaries\nSummary can be obtained by years or by years.months. User must specify this option at the “intervals” argument of the summary function. \n\n4.4.1 By year\n\n# Specify hist.data = dataframe containing the historical weather data to summarize\nyear_intervals &lt;- historical.years(hist.data = hist_weather_daymet)\n\n# input = dataframe containing the historical weather data.\n# intervals = type of historical intervals (years, years.months)\n\n# Summarizing historical weather\nyear_summary_daymet &lt;-\n  summary.daymet(input = hist_weather_daymet,\n                           intervals = year_intervals)\n\n# Creating a table to visualize data\nkbl(head(year_summary_daymet)) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", latex_options = c(\"scale_down\"))\n\n\n\n\nID\nYear\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\n2000\n2000-01-01\n2000-12-30\nCorn\nElora\n364\n1035.79\n6.784354\n4882.077\n311.9404\n834.4626\n1\n5\n3363.850\n0.7951588\n480.635\n1.451336\n10.157556\n823.6175\n\n\n1\n2001\n2001-01-01\n2001-12-31\nCorn\nElora\n364\n909.30\n7.786786\n4956.964\n321.2663\n874.0620\n11\n2\n3266.985\n0.7815342\n644.145\n1.517290\n7.695417\n710.6491\n\n\n1\n2002\n2002-01-01\n2002-12-31\nCorn\nElora\n364\n795.34\n7.495824\n4869.331\n327.4677\n869.5653\n16\n2\n3140.274\n0.7811108\n630.985\n1.550607\n7.717032\n621.2487\n\n\n1\n2003\n2003-01-01\n2003-12-31\nCorn\nElora\n364\n948.80\n6.212733\n4976.636\n300.1867\n841.3591\n4\n2\n3158.797\n0.7825177\n388.790\n1.575484\n12.800318\n742.4528\n\n\n1\n2004\n2004-01-01\n2004-12-30\nCorn\nElora\n364\n952.48\n6.537857\n4883.104\n308.5060\n827.3507\n0\n3\n3248.058\n0.8086498\n401.735\n1.503392\n12.155037\n770.2228\n\n\n1\n2005\n2005-01-01\n2005-12-31\nCorn\nElora\n364\n842.09\n7.185206\n5117.729\n300.0098\n921.0563\n24\n3\n3242.154\n0.7836771\n681.280\n1.578497\n7.511932\n659.9266\n\n\n\n\n\n\n\n\n\n4.4.2 By year-month\n\n# Specify hist.data = dataframe containing the historical weather data to summarize\nyearmonth_intervals &lt;- historical.yearmonths(hist.data = hist_weather_daymet)\n\n# input = dataframe containing the historical weather data.\n# intervals = type of historical intervals (years, years.months)\n\n# Summarizing historical weather\nyearmonth_summary_daymet &lt;-\n  summary.daymet(input = hist_weather_daymet,\n                           intervals = yearmonth_intervals)\n\n# Creating a table to visualize data\nkbl(head(yearmonth_summary_daymet)) %&gt;%\n  kable_styling(font_size = 7, position = \"center\", latex_options = c(\"scale_down\"))\n\n\n\n\nID\nYear\nMonth\nStart.in\nEnd.in\nCrop\nSite\nDur\nPP\nTmean\nRad\nVP\nET0_HS\nETE\nEPE\nCHU\nSDI\nGDD\nQ_chu\nQ_gdd\nAWDR\n\n\n\n\n1\n2000\n1\n2000-01-01\n2000-01-31\nCorn\nElora\n30\n44.22\n-7.187333\n191.5335\n8.62155\n12.22926\n0\n0\n0.9515712\n0.7172157\n-190.010\n201.2813416\n-1.008018\n31.71528\n\n\n1\n2000\n2\n2000-02-01\n2000-02-29\nCorn\nElora\n28\n57.08\n-4.463036\n269.1460\n9.91130\n19.01688\n0\n0\n6.6954018\n0.7480953\n-141.315\n40.1986311\n-1.904582\n42.70128\n\n\n1\n2000\n3\n2000-03-01\n2000-03-31\nCorn\nElora\n30\n44.97\n2.868500\n441.0486\n16.34147\n47.73356\n0\n0\n70.8484896\n0.6801023\n-33.830\n6.2252371\n-13.037205\n30.58420\n\n\n1\n2000\n4\n2000-04-01\n2000-04-30\nCorn\nElora\n29\n64.87\n5.135345\n552.8653\n18.09003\n65.36530\n0\n1\n100.1956452\n0.5036470\n-1.030\n5.5178573\n-536.762404\n32.67158\n\n\n1\n2000\n5\n2000-05-01\n2000-05-31\nCorn\nElora\n30\n147.43\n12.970667\n584.3621\n31.84376\n114.94758\n0\n2\n398.9491488\n0.6533741\n140.385\n1.4647534\n4.162568\n96.32695\n\n\n1\n2000\n6\n2000-06-01\n2000-06-30\nCorn\nElora\n29\n199.88\n17.281207\n560.0957\n41.55868\n133.48935\n1\n1\n576.7064388\n0.7668216\n217.885\n0.9711973\n2.570603\n153.27230",
    "crumbs": [
      "Lessons",
      "11-Weather data"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html",
    "href": "coding/week_08/models_01.html",
    "title": "Statistical Models with R: I - Essentials",
    "section": "",
    "text": "Statistical modeling is a process of developing and analyzing mathematical models to represent real-world phenomena. In agricultural research, statistical modeling plays a crucial role in understanding the relationships between environmental variables, management practices, and crop responses. By leveraging statistical models, researchers can make informed decisions about optimizing yield, improving resource efficiency, and enhancing sustainability in agriculture.\nIn this lesson, we focus on the essentials of statistical modeling using R, with examples relevant to ag-data science. We will explore the use of statistical models to analyze field experiments, evaluate treatment effects, and understand interactions between genotype, environment, and management practices. The examples will utilize datasets from the agridat package, particularly the lasrosas.corn dataset, and introduce key functions from stats, nlme, lme4, car, multcomp, and agricolae.\n\n\n\nData Collection and Exploratory Data Analysis (EDA)\n\nStatistical modeling starts with data collection and EDA. In agricultural experiments, this involves gathering data on yield, soil properties, weather conditions, and management practices. EDA helps identify patterns, trends, and relationships between variables while detecting outliers or anomalies.\n\nTypes of Statistical Models\n\nIn agricultural research, common models include:\n\nRegression Models: For predicting continuous outcomes like crop yield based on variables such as soil nutrients or precipitation.\nTime Series Models: To analyze temporal data like seasonal growth patterns or yield trends over years.\nMixed-Effects Models: Ideal for experimental designs with hierarchical structures, such as split-plot designs or repeated measures.\n\n\nModel Selection and Assumptions\n\nThe choice of model depends on the data type, research question, and assumptions about the data. For example:\n\nLinear Regression assumes a linear relationship between predictors and outcome, suitable for continuous variables like yield or biomass. We call it linear because we are basically comparing “lines”, where the lines represent the “means”.\nGeneralized Linear Models (GLM) are used for non-normal distributions, such as count data (e.g., pest counts) or binary outcomes (e.g., disease presence).\n\n\nModel Evaluation\n\nEvaluating model performance is crucial to ensure accurate predictions and inferences. In agricultural modeling, common metrics include:\n\nR-squared (R²): Measures the proportion of variation explained by the model. But it is NOT always recommended as a criterion the “select models”.\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE): Assess prediction accuracy.\nAIC and BIC: For model comparison and selection. These two are recommended when selecting a model.\n\n\nApplication in Agricultural Research\n\nStatistical models provide insights into complex agricultural systems, enabling researchers to:\n\nIdentify Key Drivers: Determine which factors most influence crop performance, such as genotype-environment interactions.\nPredict Future Trends: Forecast yield potential under different climate scenarios or management practices.\nOptimize Inputs: Inform decision-making for fertilizer application, irrigation scheduling, or pest management.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#introduction",
    "href": "coding/week_08/models_01.html#introduction",
    "title": "Statistical Models with R: I - Essentials",
    "section": "",
    "text": "Statistical modeling is a process of developing and analyzing mathematical models to represent real-world phenomena. In agricultural research, statistical modeling plays a crucial role in understanding the relationships between environmental variables, management practices, and crop responses. By leveraging statistical models, researchers can make informed decisions about optimizing yield, improving resource efficiency, and enhancing sustainability in agriculture.\nIn this lesson, we focus on the essentials of statistical modeling using R, with examples relevant to ag-data science. We will explore the use of statistical models to analyze field experiments, evaluate treatment effects, and understand interactions between genotype, environment, and management practices. The examples will utilize datasets from the agridat package, particularly the lasrosas.corn dataset, and introduce key functions from stats, nlme, lme4, car, multcomp, and agricolae.\n\n\n\nData Collection and Exploratory Data Analysis (EDA)\n\nStatistical modeling starts with data collection and EDA. In agricultural experiments, this involves gathering data on yield, soil properties, weather conditions, and management practices. EDA helps identify patterns, trends, and relationships between variables while detecting outliers or anomalies.\n\nTypes of Statistical Models\n\nIn agricultural research, common models include:\n\nRegression Models: For predicting continuous outcomes like crop yield based on variables such as soil nutrients or precipitation.\nTime Series Models: To analyze temporal data like seasonal growth patterns or yield trends over years.\nMixed-Effects Models: Ideal for experimental designs with hierarchical structures, such as split-plot designs or repeated measures.\n\n\nModel Selection and Assumptions\n\nThe choice of model depends on the data type, research question, and assumptions about the data. For example:\n\nLinear Regression assumes a linear relationship between predictors and outcome, suitable for continuous variables like yield or biomass. We call it linear because we are basically comparing “lines”, where the lines represent the “means”.\nGeneralized Linear Models (GLM) are used for non-normal distributions, such as count data (e.g., pest counts) or binary outcomes (e.g., disease presence).\n\n\nModel Evaluation\n\nEvaluating model performance is crucial to ensure accurate predictions and inferences. In agricultural modeling, common metrics include:\n\nR-squared (R²): Measures the proportion of variation explained by the model. But it is NOT always recommended as a criterion the “select models”.\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE): Assess prediction accuracy.\nAIC and BIC: For model comparison and selection. These two are recommended when selecting a model.\n\n\nApplication in Agricultural Research\n\nStatistical models provide insights into complex agricultural systems, enabling researchers to:\n\nIdentify Key Drivers: Determine which factors most influence crop performance, such as genotype-environment interactions.\nPredict Future Trends: Forecast yield potential under different climate scenarios or management practices.\nOptimize Inputs: Inform decision-making for fertilizer application, irrigation scheduling, or pest management.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#essential-r-packages",
    "href": "coding/week_08/models_01.html#essential-r-packages",
    "title": "Statistical Models with R: I - Essentials",
    "section": "2 Essential R Packages",
    "text": "2 Essential R Packages\n\nlibrary(pacman)\np_load(agridat, agricolae)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(nlme, lme4, car, multcomp)",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#data",
    "href": "coding/week_08/models_01.html#data",
    "title": "Statistical Models with R: I - Essentials",
    "section": "3 Data",
    "text": "3 Data\n\ndata_corn &lt;- agridat::lasrosas.corn\n# Check data structure and variables\nglimpse(data_corn)\n\nRows: 3,443\nColumns: 9\n$ year  &lt;int&gt; 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   &lt;dbl&gt; -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  &lt;dbl&gt; -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield &lt;dbl&gt; 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro &lt;dbl&gt; 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    &lt;dbl&gt; 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    &lt;fct&gt; N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#key-statistical-models",
    "href": "coding/week_08/models_01.html#key-statistical-models",
    "title": "Statistical Models with R: I - Essentials",
    "section": "4 Key Statistical Models",
    "text": "4 Key Statistical Models\n\n4.1 Linear Models (LM)\nLinear regression models are fundamental for analyzing relationships between variables. The term “regression” could be confusing because it means we are working with a “continous response variable”, but it could also mean we using a “continuous covariate” (or independent / or explanatory variable) (e.g. a “regressor”).\n\n4.1.1 Categorical covariate/s as independent variable/s\n\n# Complete Randomized\nlm_fit_01 &lt;- lm(yield ~ nf, data = data_corn)\n# See summary\nsummary(lm_fit_01)\n\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9729     0.8218  79.060  &lt; 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,   Adjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n\n# Alternative models\n# Blocks (as fixed)\nlm_fit_02 &lt;- lm(yield ~ nf + rep , data = data_corn)\n# Add year (as fixed)\nlm_fit_03 &lt;- lm(yield ~ nf + rep + year, data = data_corn)\n# Add topography (as fixed)\nlm_fit_04 &lt;- lm(yield ~ nf + rep + year + topo, data = data_corn)\n# Different order \nlm_fit_05 &lt;- lm(yield ~ nf + year + topo + rep, data = data_corn)\n\n\n\n4.1.2 Continuous covariate/s as independent variable/s\n\n# Nitrogen (independent variable) as continuous predictor\nlm_reg_01 &lt;- lm(yield ~ nitro, data = data_corn)\n# See summary\nsummary(lm_reg_01)\n\n\nCall:\nlm(formula = yield ~ nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n# Compare to N as a categorical variable\nsummary(lm_fit_01)\n\n\nCall:\nlm(formula = yield ~ nf, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.313 -15.344  -3.126  13.563  45.337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9729     0.8218  79.060  &lt; 2e-16 ***\nnfN1          3.6435     1.1602   3.140   0.0017 ** \nnfN2          4.6774     1.1632   4.021 5.92e-05 ***\nnfN3          5.3630     1.1612   4.618 4.01e-06 ***\nnfN4          7.5901     1.1627   6.528 7.65e-11 ***\nnfN5          7.8589     1.1612   6.768 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3437 degrees of freedom\nMultiple R-squared:  0.01771,   Adjusted R-squared:  0.01629 \nF-statistic:  12.4 on 5 and 3437 DF,  p-value: 6.075e-12\n\n\n\n\n\n4.2 Generalized Linear Models (GLM)\nGLMs extend linear models to handle non-normal response distributions. In agricultural research, they are useful for modeling yield data with non-constant variance or non-normal residuals.\n\n4.2.1 Example using GLM as an LM\nlm() is just a special case of glm where the distribution of error is assumed to be Gaussian (i.e. normal)\n\nglm_fit_01 &lt;- glm(yield ~ nf + rep, data = data_corn, family = gaussian)\n# See summary\nsummary(glm_fit_01)\n\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian, data = data_corn)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8527)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328839  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 2\n\n# Compare to lm\nsummary(lm_fit_02)\n\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n\n\n\n\n4.2.2 Example using the Gaussian family with log link:\nThese approaches are particularly useful when yield data exhibit heteroscedasticity or skewness.\n\nFor this first approach, the model assumes a multiplicative relationship between predictors and yield, modeling the expected value as an exponential function.\n\nThus, the underlying model is:\n\\[E(Y) = \\exp(X\\beta)\\] , where the expected value of yield, \\(E(Y)\\), is modeled as an exponential function of the predictors \\(\\exp(X\\beta)\\).\nIf you believe the relationship between predictors and the expected value of yield is multiplicative, use this approach.\n\n# Using log link without manually transforming yield\nglm_fit_02 &lt;- glm(yield ~ nf + rep, data = data_corn, \n                  family = gaussian(link = \"log\"))\nsummary(glm_fit_02)\n\n\nCall:\nglm(formula = yield ~ nf + rep, family = gaussian(link = \"log\"), \n    data = data_corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.170242   0.014369 290.223  &lt; 2e-16 ***\nnfN1         0.054586   0.017387   3.140  0.00171 ** \nnfN2         0.069457   0.017308   4.013 6.12e-05 ***\nnfN3         0.079305   0.017202   4.610 4.17e-06 ***\nnfN4         0.110495   0.016981   6.507 8.79e-11 ***\nnfN5         0.114107   0.016934   6.738 1.87e-11 ***\nrepR2       -0.004492   0.011824  -0.380  0.70404    \nrepR3        0.015601   0.011702   1.333  0.18254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 386.8745)\n\n    Null deviance: 1354097  on 3442  degrees of freedom\nResidual deviance: 1328857  on 3435  degrees of freedom\nAIC: 30294\n\nNumber of Fisher Scoring iterations: 4\n\n\nAlternatively, you may want to manually log-transform the response:\n\nFor this second approach, the log-transformed yield is modeled as a linear function of the predictors, stabilizing variance or normalizing residuals.\n\nIn the second case, the model is:\n\\[\\log(Y) = X\\beta + \\epsilon \\],\nwhich is a linear model \\(X\\beta + \\epsilon\\) on the log-transformed outcome, \\(\\log(Y)\\).\nIf you want to stabilize variance or normalize the residuals, use this second approach.\n\n# Manually log-transforming yield\nglm_fit_03 &lt;- glm(log(yield) ~ nf + rep, data = lasrosas.corn, \n                  family = gaussian(link = \"identity\"))\nsummary(glm_fit_03)\n\n\nCall:\nglm(formula = log(yield) ~ nf + rep, family = gaussian(link = \"identity\"), \n    data = lasrosas.corn)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.114234   0.013869 296.650  &lt; 2e-16 ***\nnfN1         0.070491   0.016959   4.157 3.31e-05 ***\nnfN2         0.086082   0.017003   5.063 4.35e-07 ***\nnfN3         0.096942   0.016974   5.711 1.22e-08 ***\nnfN4         0.130277   0.016996   7.665 2.31e-14 ***\nnfN5         0.129767   0.016974   7.645 2.69e-14 ***\nrepR2       -0.004546   0.012007  -0.379    0.705    \nrepR3        0.019599   0.012002   1.633    0.103    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.08268533)\n\n    Null deviance: 291.07  on 3442  degrees of freedom\nResidual deviance: 284.02  on 3435  degrees of freedom\nAIC: 1198.4\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\n\n4.3 Mixed-Effects Models\nMixed-effects models account for both fixed and random effects, often used in agricultural experiments.\nUsing nlme:\n\nlme_fit &lt;- lme(yield ~ nf, random = ~1 | rep, data = data_corn)\nsummary(lme_fit)\n\nLinear mixed-effects model fit by REML\n  Data: data_corn \n       AIC      BIC    logLik\n  30286.69 30335.83 -15135.34\n\nRandom effects:\n Formula: ~1 | rep\n        (Intercept) Residual\nStdDev:   0.4656023 19.66857\n\nFixed effects:  yield ~ nf \n               Value Std.Error   DF  t-value p-value\n(Intercept) 64.97387 0.8645219 3435 75.15584  0.0000\nnfN1         3.64192 1.1599967 3435  3.13960  0.0017\nnfN2         4.67371 1.1630343 3435  4.01855  0.0001\nnfN3         5.36182 1.1610011 3435  4.61827  0.0000\nnfN4         7.59070 1.1625194 3435  6.52953  0.0000\nnfN5         7.85772 1.1610011 3435  6.76805  0.0000\n Correlation: \n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.6547157 -0.7871554 -0.1563133  0.6960595  2.2882930 \n\nNumber of Observations: 3443\nNumber of Groups: 3 \n\n\nUsing lme4:\n\nlmer_fit &lt;- lmer(yield ~ nf + (1 | rep), data = data_corn)\nsummary(lmer_fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ nf + (1 | rep)\n   Data: data_corn\n\nREML criterion at convergence: 30270.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6547 -0.7872 -0.1563  0.6961  2.2883 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n rep      (Intercept)   0.2168  0.4656 \n Residual             386.8526 19.6686 \nNumber of obs: 3443, groups:  rep, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  64.9739     0.8645  75.156\nnfN1          3.6419     1.1600   3.140\nnfN2          4.6737     1.1630   4.019\nnfN3          5.3618     1.1610   4.618\nnfN4          7.5907     1.1625   6.530\nnfN5          7.8577     1.1610   6.768\n\nCorrelation of Fixed Effects:\n     (Intr) nfN1   nfN2   nfN3   nfN4  \nnfN1 -0.673                            \nnfN2 -0.671  0.500                     \nnfN3 -0.673  0.501  0.500              \nnfN4 -0.672  0.501  0.499  0.500       \nnfN5 -0.673  0.501  0.500  0.501  0.500\n\n\n\n\n4.4 Choosing Between nlme and lme4\n\nnlme: Suitable for models that are linear and nonlinear mixed-effects models. It provides robust tools for analyzing data with nested random effects and handling different types of correlation structures within the data. It can handle heterogeneous variance models.\nlme4: Best for fitting large linear mixed-effects models. It does not handle nonlinear mixed-effects models or autoregressive correlation structures but is highly efficient with large datasets and complex random effects structures. It cannot handle heterogeneous variance models.\n\n\n\n4.5 Analysis of Variance (ANOVA)\nAnalysis of Variance (ANOVA) is widely used in agricultural research to compare the means of multiple groups and to understand the influence of categorical factors on continuous outcomes, such as yield or biomass. In R, there are multiple ways to perform ANOVA:\n\nanova(): Sequential (Type I) ANOVA\naov(): Similar for balanced designs\ncar::Anova(): Flexible ANOVA with options for Type II and Type III Sum of Squares\n\n\n4.5.1 Using anova()\nanova() performs Type I Sum of Squares (sequential). It tests each term sequentially, considering the order of the terms in the model.\n\nlm_fit &lt;- lm(yield ~ rep + nf + year + topo, data = data_corn)\nanova(lm_fit)  # Type I Sum of Squares\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(&gt;F)    \nrep          2   1271     635    3.7264 0.02418 *  \nnf           5  23987    4797   28.1334 &lt; 2e-16 ***\nyear         1  97313   97313  570.6692 &lt; 2e-16 ***\ntopo         3 646456  215485 1263.6625 &lt; 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.2 Using aov()\naov() is similar to lm() but is designed for balanced experimental designs. It also uses Type I Sum of Squares.\n\naov_fit &lt;- aov(yield ~ nf + year + topo + rep, data = data_corn)\nsummary(aov_fit)  # Type I Sum of Squares\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nnf             5  23987    4797   28.13  &lt; 2e-16 ***\nyear           1  97321   97321  570.71  &lt; 2e-16 ***\ntopo           3 643667  214556 1258.21  &lt; 2e-16 ***\nrep            2   4053    2027   11.88 7.18e-06 ***\nResiduals   3431 585070     171                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.3 Using car::Anova()\nThe Anova() function from the car package allows for Type II and Type III Sum of Squares:\n\nType II: Assumes no interaction between factors and tests each main effect after the other main effects.\nType III: Tests each main effect and interaction after all other terms, typically used with dummy coding.\n\n\ncar::Anova(lm_fit, type = 2)  # Type II Sum of Squares\n\nAnova Table (Type II tests)\n\nResponse: yield\n          Sum Sq   Df  F value    Pr(&gt;F)    \nrep         4053    2   11.885 7.183e-06 ***\nnf         21727    5   25.483 &lt; 2.2e-16 ***\nyear      120660    1  707.584 &lt; 2.2e-16 ***\ntopo      646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals 585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit, type = 3)  # Type III Sum of Squares\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.5.4 Comparison of anova() vs. Anova()\n\n# For the anova(), the order of factors matter\nanova(lm_fit_01)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nnf           5   23987  4797.4  12.396 6.075e-12 ***\nResiduals 3437 1330110   387.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_02)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nnf           5   23987  4797.4 12.4011 6.01e-12 ***\nrep          2    1271   635.6  1.6429   0.1936    \nResiduals 3435 1328839   386.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_03)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nnf           5   23987    4797  13.3771 6.105e-13 ***\nrep          2    1271     636   1.7722    0.1701    \nyear         1   97313   97313 271.3489 &lt; 2.2e-16 ***\nResiduals 3434 1231526     359                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_04)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq   F value  Pr(&gt;F)    \nnf           5  23987    4797   28.1331 &lt; 2e-16 ***\nrep          2   1271     636    3.7272 0.02416 *  \nyear         1  97313   97313  570.6692 &lt; 2e-16 ***\ntopo         3 646456  215485 1263.6625 &lt; 2e-16 ***\nResiduals 3431 585070     171                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_fit_05)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nnf           5  23987    4797   28.133 &lt; 2.2e-16 ***\nyear         1  97321   97321  570.714 &lt; 2.2e-16 ***\ntopo         3 643667  214556 1258.209 &lt; 2.2e-16 ***\nrep          2   4053    2027   11.885 7.183e-06 ***\nResiduals 3431 585070     171                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# For the Anova(type=3), the order of factors doesn't matter\ncar::Anova(lm_fit_01, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 2418907    1 6250.447 &lt; 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_02, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 1800690    1 4654.7170 &lt; 2.2e-16 ***\nnf            23987    5   12.4012 6.009e-12 ***\nrep            1271    2    1.6429    0.1936    \nResiduals   1328839 3435                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_03, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept)   96132    1 268.0552 &lt; 2.2e-16 ***\nnf            23836    5  13.2930 7.436e-13 ***\nrep            1264    2   1.7616    0.1719    \nyear          97313    1 271.3489 &lt; 2.2e-16 ***\nResiduals   1231526 3434                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_04, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm_fit_05, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 119183    1  698.920 &lt; 2.2e-16 ***\nnf           21727    5   25.483 &lt; 2.2e-16 ***\nyear        120660    1  707.584 &lt; 2.2e-16 ***\ntopo        646456    3 1263.662 &lt; 2.2e-16 ***\nrep           4053    2   11.885 7.183e-06 ***\nResiduals   585070 3431                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn agricultural research, Type III Sum of Squares is particularly useful for unbalanced designs, such as field trials with missing data or unequal replications.\n\n\n\n4.6 Post-hoc Tests\nAfter detecting significant differences with ANOVA, post-hoc tests can be conducted to identify specific group differences.\nUsing multcomp for multiple comparisons:\n\n# Using glht() function\ncomp &lt;- glht(aov_fit, linfct = mcp(nf = \"Tukey\"))\nsummary(comp)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = yield ~ nf + year + topo + rep, data = data_corn)\n\nLinear Hypotheses:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nN1 - N0 == 0   3.8048     0.7702   4.940  &lt; 0.001 ***\nN2 - N0 == 0   4.8232     0.7722   6.246  &lt; 0.001 ***\nN3 - N0 == 0   5.0724     0.7709   6.580  &lt; 0.001 ***\nN4 - N0 == 0   7.4876     0.7718   9.701  &lt; 0.001 ***\nN5 - N0 == 0   7.3672     0.7709   9.556  &lt; 0.001 ***\nN2 - N1 == 0   1.0184     0.7708   1.321  0.77331    \nN3 - N1 == 0   1.2676     0.7696   1.647  0.56708    \nN4 - N1 == 0   3.6828     0.7705   4.779  &lt; 0.001 ***\nN5 - N1 == 0   3.5624     0.7697   4.628  &lt; 0.001 ***\nN3 - N2 == 0   0.2492     0.7716   0.323  0.99954    \nN4 - N2 == 0   2.6644     0.7726   3.449  0.00767 ** \nN5 - N2 == 0   2.5440     0.7717   3.297  0.01264 *  \nN4 - N3 == 0   2.4152     0.7712   3.132  0.02154 *  \nN5 - N3 == 0   2.2948     0.7702   2.980  0.03456 *  \nN5 - N4 == 0  -0.1204     0.7712  -0.156  0.99999    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\n4.7 Nonlinear Models\nNonlinear models are useful when the relationship between the predictor and response variables is not linear. In agricultural research, these models are commonly used to model yield response to inputs, such as nitrogen fertilizer.\nFor nonlinear relationships, we could use nls(). Let’s see an example using a power function:\n\nnls_fit &lt;- nls(yield ~ a * nitro^b, data = data_corn, start = list(a = 1, b = 1))\nsummary(nls_fit)\n\n\nFormula: yield ~ a * nitro^b\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 55.71689    4.33102  12.865  &lt; 2e-16 ***\nb  0.05641    0.01811   3.116  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.99 on 3441 degrees of freedom\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.822e-07\n\n# Alternative exponential\n# nls_mitscherlich &lt;- nls(yield ~ a * (1 - exp(-b * nitro)), data = data_corn, start = list(a = 55, b = 0.05))\n\nVisualizing the model’s predictions can help in understanding the fitted curve and the data’s behavior.\n\n# Creating a data frame with predictions\ndata_corn &lt;- data_corn %&gt;% \n  mutate(pred = predict(nls_fit))\n\n# Plotting observed vs. predicted yield\ndata_corn %&gt;% \nggplot(aes(x = nitro, y = yield)) +\n  geom_point(color = \"blue\", size = 2) +  # Observed data\n  geom_line(aes(y = pred), color = \"red\", size = 1) +  # Fitted curve\n  geom_smooth()+\n  labs(title = \"Yield Response to Nitrogen\",\n       x = \"Nitrogen (kg/ha)\",\n       y = \"Yield (qq/ha)\") +\n  theme_minimal()",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_08/models_01.html#conclusion",
    "href": "coding/week_08/models_01.html#conclusion",
    "title": "Statistical Models with R: I - Essentials",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lesson introduced essential statistical models in R for agricultural research, providing practical code examples. In the next session, we will delve deeper into model diagnostics and interpretation.",
    "crumbs": [
      "Lessons",
      "12-Models I"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html",
    "href": "coding/week_09/models_03.html",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "",
    "text": "Linear models are foundational in statistical analysis, particularly in agricultural data science. These models allow researchers to evaluate relationships between variables and assess treatment effects in experiments. This document covers the essentials of linear modeling in R using stats, car, broom, emmeans, multcomp, and cld for statistical inference and means comparisons.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#introduction",
    "href": "coding/week_09/models_03.html#introduction",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "",
    "text": "Linear models are foundational in statistical analysis, particularly in agricultural data science. These models allow researchers to evaluate relationships between variables and assess treatment effects in experiments. This document covers the essentials of linear modeling in R using stats, car, broom, emmeans, multcomp, and cld for statistical inference and means comparisons.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#what-is-a-linear-model",
    "href": "coding/week_09/models_03.html#what-is-a-linear-model",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "2 What is a Linear Model?",
    "text": "2 What is a Linear Model?\nA linear model is a mathematical equation describing the relationship between a response variable (dependent) and one or more explanatory variables (independent). The simplest form is:\n\\[ Y = \\beta_0 + \\beta_1 X + ... + \\beta_2 X ... + \\epsilon \\]\nwhere:\n\\[Y\\] is the dependent variable (response variable),  \\[X\\] is the independent variable (matrix of experimental design),  \\[ \\beta_0 \\] is the intercept,  \\[ \\beta_1 \\] is the effect of factor #1 on Y (i.e. slope (regression) or mean effect (anova)),  \\[ \\beta_2 \\] is the effect of factor #2 on Y (i.e. slope (regression) or mean effect (anova)),  \\[ \\epsilon \\] represents error (unexplained variation). \nNote: A polynomial term–a quadratic (squared or \\(2^{nd}\\) order) or cubic (\\(3^{rd}\\) order) term turns a linear regression model into a curve. But because it is X that is squared or cubed, not the Beta coefficient ($ $), it still qualifies as a linear model.\n\n2.1 Regression vs. ANOVA\n\nRegression Analysis: Used when the explanatory variable(s) are continuous (e.g., predicting yield from nitrogen levels).\nANOVA (Analysis of Variance): Used when at least one explanatory variable is categorical (e.g., comparing mean yields across different treatments).\nTechnically, ANOVA is a type of linear model, but its focus is on comparing means across groups (made from categorical predictors), while regression aims to quantify the relationship between continuous variables.\n\nWe will use the data_corn dataset from the agridat package.\n\n# Load required packages\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(agridat)\np_load(broom)\np_load(emmeans)\np_load(multcomp, multcompView)\np_load(car) # for assumption checks\np_load(performance) # for assumption checks\n\nLoad dataset:\n\ndata_corn &lt;- agridat::lasrosas.corn\n\n# Inspect dataset\nglimpse(data_corn)\n\nRows: 3,443\nColumns: 9\n$ year  &lt;int&gt; 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999…\n$ lat   &lt;dbl&gt; -33.05113, -33.05115, -33.05116, -33.05117, -33.05118, -33.05120…\n$ long  &lt;dbl&gt; -63.84886, -63.84879, -63.84872, -63.84865, -63.84858, -63.84851…\n$ yield &lt;dbl&gt; 72.14, 73.79, 77.25, 76.35, 75.55, 70.24, 76.17, 69.17, 69.77, 6…\n$ nitro &lt;dbl&gt; 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 131.5, 1…\n$ topo  &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W…\n$ bv    &lt;dbl&gt; 162.60, 170.49, 168.39, 176.68, 171.46, 170.56, 172.94, 171.86, …\n$ rep   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, …\n$ nf    &lt;fct&gt; N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, N5, …",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-fitting-linear-regression-anova",
    "href": "coding/week_09/models_03.html#model-fitting-linear-regression-anova",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "3 Model Fitting: Linear Regression & ANOVA",
    "text": "3 Model Fitting: Linear Regression & ANOVA\n\n3.1 Simple Linear Regression (Continuous Predictor)\n\nreg_fit &lt;- lm(yield ~ 1 + nitro, data = data_corn)\nsummary(reg_fit)\n\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n# Comparing analysis of variance options\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nnitro        1   23790 23790.3  61.537 5.754e-15 ***\nResiduals 3441 1330307   386.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(reg_fit, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n           Sum Sq   Df F value    Pr(&gt;F)    \nnitro       23790    1  61.537 5.754e-15 ***\nResiduals 1330307 3441                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(reg_fit, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 4525464    1 11705.665 &lt; 2.2e-16 ***\nnitro         23790    1    61.537 5.754e-15 ***\nResiduals   1330307 3441                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2 Linear Regression, no-intercept\n\nreg_noint &lt;- lm(yield ~ -1 + nitro, data = data_corn)\nsummary(reg_fit)\n\n\nCall:\nlm(formula = yield ~ 1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\nsummary(reg_noint)\n\n\nCall:\nlm(formula = yield ~ -1 + nitro, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.435  -7.134  20.741  45.962 108.840 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nnitro 0.772271   0.009088   84.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.25 on 3442 degrees of freedom\nMultiple R-squared:  0.6772,    Adjusted R-squared:  0.6771 \nF-statistic:  7222 on 1 and 3442 DF,  p-value: &lt; 2.2e-16\n\n# Comparing analysis of variance options\nanova(reg_noint)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nnitro        1 12286371 12286371  7221.9 &lt; 2.2e-16 ***\nResiduals 3442  5855771     1701                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(reg_noint, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(&gt;F)    \nnitro     12286371    1  7221.9 &lt; 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(reg_noint, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n            Sum Sq   Df F value    Pr(&gt;F)    \nnitro     12286371    1  7221.9 &lt; 2.2e-16 ***\nResiduals  5855771 3442                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3 ANOVA (Categorical Predictors)\n\nanova_crd &lt;- lm(yield ~ nf, data = data_corn) # assuming CRD\nanova_fit &lt;- lm(yield ~ nf + rep, data = data_corn) # assuming RCBD\nanova_03 &lt;- lm(yield ~ nf*rep, data = data_corn) # assuming RCBD\n\nAnova(anova_crd, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept) 2418907    1 6250.447 &lt; 2.2e-16 ***\nnf            23987    5   12.396 6.075e-12 ***\nResiduals   1330110 3437                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Anova(anova_fit, type = 3)\nAnova(anova_03, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept)  799899    1 2063.2074 &lt; 2.2e-16 ***\nnf             7035    5    3.6290  0.002818 ** \nrep              88    2    0.1139  0.892378    \nnf:rep          977   10    0.2520  0.990548    \nResiduals   1327862 3425                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.4 ANOVA, no-intercept\n\nanova_noint &lt;- lm(yield ~ -1 + nf + rep, data = data_corn)\nAnova(anova_noint, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n           Sum Sq   Df   F value Pr(&gt;F)    \nnf        5575937    6 2402.2655 &lt;2e-16 ***\nrep          1271    2    1.6429 0.1936    \nResiduals 1328839 3435                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(anova_fit)\n\n\nCall:\nlm(formula = yield ~ nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.7216     0.9486  68.225  &lt; 2e-16 ***\nnfN1          3.6395     1.1600   3.138  0.00172 ** \nnfN2          4.6679     1.1630   4.014 6.11e-05 ***\nnfN3          5.3600     1.1610   4.617 4.04e-06 ***\nnfN4          7.5916     1.1625   6.530 7.53e-11 ***\nnfN5          7.8559     1.1610   6.767 1.54e-11 ***\nrepR2        -0.3301     0.8213  -0.402  0.68775    \nrepR3         1.0915     0.8210   1.329  0.18377    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.01665 \nF-statistic: 9.327 on 7 and 3435 DF,  p-value: 1.708e-11\n\nsummary(anova_noint)\n\n\nCall:\nlm(formula = yield ~ -1 + nf + rep, data = data_corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.062 -15.476  -3.079  13.468  44.495 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nnfN0   64.7216     0.9486  68.225   &lt;2e-16 ***\nnfN1   68.3611     0.9464  72.235   &lt;2e-16 ***\nnfN2   69.3895     0.9495  73.082   &lt;2e-16 ***\nnfN3   70.0816     0.9478  73.940   &lt;2e-16 ***\nnfN4   72.3132     0.9491  76.195   &lt;2e-16 ***\nnfN5   72.5775     0.9478  76.573   &lt;2e-16 ***\nrepR2  -0.3301     0.8213  -0.402    0.688    \nrepR3   1.0915     0.8210   1.329    0.184    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.67 on 3435 degrees of freedom\nMultiple R-squared:  0.9268,    Adjusted R-squared:  0.9266 \nF-statistic:  5433 on 8 and 3435 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-assumptions",
    "href": "coding/week_09/models_03.html#model-assumptions",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "4 Model Assumptions",
    "text": "4 Model Assumptions\nLinear models assume:\n\nLinearity: Relationship between predictors and response is linear (continuous).\n\n\n# Residual diagnostics\npar(mfrow=c(2,2))\nplot(anova_fit)\n\n\n\n\n\n\n\n\n\nNormality of Residuals: Residuals should be normally distributed.\n\n\n# Normality test\nshapiro.test(resid(anova_fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(anova_fit)\nW = 0.94724, p-value &lt; 2.2e-16\n\n\n\nHomoscedasticity: Equal variance across all levels of predictors.\n\n\n# Homoscedasticity check\nleveneTest(anova_crd) #1-way anova, only nf and CRD\nleveneTest(anova_fit) #2-way anova including blocks\n\n\nIndependence: Observations are independent (i.e. the “error” of replications is independent).\n\nThere is no test for independence. You have to make sure you specify the error-structure correctly for potential autocorrelation (e.g. blocks, split-plots, repeated measures, etc.).\n\nPerformance package With the performance package, we could check all at once.\n\n\nperformance::check_model(anova_fit)",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#model-selection-aicbic-criteria",
    "href": "coding/week_09/models_03.html#model-selection-aicbic-criteria",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "5 Model Selection: AIC/BIC Criteria",
    "text": "5 Model Selection: AIC/BIC Criteria\n\n5.1 Candidate models\nThese are all fixed-effect models.\n\n# null model\nlm_00 &lt;- lm(yield ~ 1, data = data_corn)\n# Simplest model\nlm_01 &lt;- lm(yield ~ nf + rep, data = data_corn)\n# Add year\nlm_02 &lt;- lm(yield ~ nf + year + rep, data = data_corn)\n# Add topo\nlm_03 &lt;- lm(yield ~ nf + topo + rep, data = data_corn)\n# Add year and topo\nlm_04 &lt;- lm(yield ~ nf + year + topo + rep, data = data_corn)\n\n# Main effects and interactions\nlm_05 &lt;- lm(yield ~ nf*year*topo + rep, data = data_corn)\n#lm_05 &lt;- lm(yield ~ nf + year + topo + nf:year + nf:topo + year:topo + nf:year:topo + rep, data = data_corn)\n\n\n\n5.2 Selection criteria\n\n5.2.1 F-Test\n\nF-tests evaluate if added predictors significantly improve model fit via sum of squares and degrees of freedom.\n\n\nanova(lm_01, lm_02, lm_03, lm_04, lm_05)\n\nAnalysis of Variance Table\n\nModel 1: yield ~ nf + rep\nModel 2: yield ~ nf + year + rep\nModel 3: yield ~ nf + topo + rep\nModel 4: yield ~ nf + year + topo + rep\nModel 5: yield ~ nf * year * topo + rep\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   3435 1328839                                    \n2   3434 1231526  1     97313  914.137 &lt; 2.2e-16 ***\n3   3432  705730  2    525796 2469.604 &lt; 2.2e-16 ***\n4   3431  585070  1    120660 1133.457 &lt; 2.2e-16 ***\n5   3393  361197 38    223873   55.342 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.2.2 AIC (Akaike Information Criterion)\n\nAIC balances goodness of fit with model complexity.\n\n\nAIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n\n      df      AIC\nlm_01  9 30294.35\nlm_02 10 30034.50\nlm_03 12 28121.52\nlm_04 13 27477.95\nlm_05 51 25893.36\n\n\n\n\n5.2.3 BIC (Bayesian Information criterion)\n\nBIC applies a stricter penalty for complexity, favoring simpler models.\n\n\nBIC(lm_01, lm_02, lm_03, lm_04, lm_05)\n\n      df      BIC\nlm_01  9 30349.64\nlm_02 10 30095.94\nlm_03 12 28195.25\nlm_04 13 27557.82\nlm_05 51 26206.71",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#significance-of-effects",
    "href": "coding/week_09/models_03.html#significance-of-effects",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "6 Significance of effects",
    "text": "6 Significance of effects\n\n# Compare Anova sum of squares\nAnova(lm_05, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \nnf            24028   20   11.2855 &lt; 2.2e-16 ***\nyear         120553    1 1132.4508 &lt; 2.2e-16 ***\ntopo         648289   18  338.3269 &lt; 2.2e-16 ***\nrep            3995    2   18.7623 7.877e-09 ***\nnf:year        1486    5    2.7919   0.01601 *  \nnf:topo        2126   15    1.3315   0.17376    \nyear:topo    218811    3  685.1532 &lt; 2.2e-16 ***\nnf:year:topo   1869   15    1.1704   0.28739    \nResiduals    361197 3393                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(lm_05, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df  F value    Pr(&gt;F)    \n(Intercept)   26405    1 248.0418 &lt; 2.2e-16 ***\nnf              322    5   0.6050    0.6961    \nyear          26539    1 249.2978 &lt; 2.2e-16 ***\ntopo          38269    3 119.8315 &lt; 2.2e-16 ***\nrep            3995    2  18.7623 7.877e-09 ***\nnf:year         321    5   0.6039    0.6970    \nnf:topo        1868   15   1.1701    0.2876    \nyear:topo     38337    3 120.0429 &lt; 2.2e-16 ***\nnf:year:topo   1869   15   1.1704    0.2874    \nResiduals    361197 3393                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#means-comparisons-with-emmeans-and-cld",
    "href": "coding/week_09/models_03.html#means-comparisons-with-emmeans-and-cld",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "7 Means Comparisons with emmeans and cld",
    "text": "7 Means Comparisons with emmeans and cld\n\n7.1 Interaction\n\n# Pairwise comparisons among treatment means\nemmeans(lm_05, pairwise ~ year:topo) %&gt;% \n  cld(., level = 0.05, decreasing = F)\n\n year topo emmean    SE   df lower.CL upper.CL .group  \n 2001 HT     44.6 0.497 3393     44.6     44.6  1      \n 1999 HT     53.4 0.549 3393     53.4     53.5   2     \n 1999 E      64.8 0.538 3393     64.7     64.8    3    \n 1999 W      66.0 0.438 3393     65.9     66.0    34   \n 2001 W      67.7 0.467 3393     67.7     67.8     4   \n 1999 LO     71.3 0.481 3393     71.3     71.3      5  \n 2001 E      92.7 0.543 3393     92.6     92.7       6 \n 2001 LO     99.9 0.501 3393     99.9     99.9        7\n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 8 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n7.2 Alternative\nUsing the same model, same sum of squares we could group comparisons differently. In this case, showing comparisons of topo means, grouped by “year”\n\nemmeans(lm_05, pairwise ~ topo, by = \"year\") %&gt;% \n  cld(., level = 0.05, decreasing = FALSE, Letters = letters) # add letters\n\nyear = 1999:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     53.4 0.549 3393     53.4     53.5  a    \n E      64.8 0.538 3393     64.7     64.8   b   \n W      66.0 0.438 3393     65.9     66.0   b   \n LO     71.3 0.481 3393     71.3     71.3    c  \n\nyear = 2001:\n topo emmean    SE   df lower.CL upper.CL .group\n HT     44.6 0.497 3393     44.6     44.6  a    \n W      67.7 0.467 3393     67.7     67.8   b   \n E      92.7 0.543 3393     92.6     92.7    c  \n LO     99.9 0.501 3393     99.9     99.9     d \n\nResults are averaged over the levels of: nf, rep \nConfidence level used: 0.05 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n# By default in R\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#interpreting-coefficients",
    "href": "coding/week_09/models_03.html#interpreting-coefficients",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "8 Interpreting Coefficients",
    "text": "8 Interpreting Coefficients\n\n8.1 Regression\n\nIntercept (\\(\\beta_0\\)): Baseline value of the dependent variable. Is the value of Y, when X = 0.\nSlope (\\(\\beta_1\\)): Change in response variable (Y) per unit increase in predictor (X).\np-value: Significance of predictor effect.\n\nWe can extract regression coefficient estimates with the ‘coef()’ function, or with the ‘tidy()’ function of the “broom” package.\n\n# Extracting coefficients with coef()\ncoef(reg_fit)\n\n(Intercept)       nitro \n65.84321305  0.06171718 \n\n# Tidy summary of coefficients\nreg_coefs &lt;- broom::tidy(reg_fit)\nreg_coefs\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  65.8      0.609      108.   0       \n2 nitro         0.0617   0.00787      7.84 5.75e-15\n\n\n\n\n8.2 ANOVA\n\nIntercept: the reference or benchmark level (baseline mean) for categorical predictors.\nFactor Levels: estimates of mean differences “with respect to” the intercept (if any, if not, from zero).\np-value: whether a factor significantly differs from baseline.\n\nWe can also extract ANOVA coefficient estimates with the ‘coef()’ function, or with the ‘tidy()’ function of the “broom” package.\n\n# Extracting coefficients with coef()\ncoef(lm_05)\n\n     (Intercept)             nfN1             nfN2             nfN3 \n   -2.930444e+04     7.134866e+01     4.085152e+02     3.143485e+03 \n            nfN4             nfN5             year           topoHT \n    2.469591e+03     2.724513e+03     1.468933e+01     3.840595e+04 \n          topoLO            topoW            repR2            repR3 \n   -8.387929e+02     2.518130e+04     4.372000e-01     2.476942e+00 \n       nfN1:year        nfN2:year        nfN3:year        nfN4:year \n   -3.417600e-02    -2.023608e-01    -1.569826e+00    -1.232124e+00 \n       nfN5:year      nfN1:topoHT      nfN2:topoHT      nfN3:topoHT \n   -1.359608e+00    -3.857242e+03    -2.925203e+03    -4.044360e+03 \n     nfN4:topoHT      nfN5:topoHT      nfN1:topoLO      nfN2:topoLO \n   -1.041123e+03     1.878725e+03     3.636709e+03     1.417504e+03 \n     nfN3:topoLO      nfN4:topoLO      nfN5:topoLO       nfN1:topoW \n   -2.679130e+03    -1.098798e+02    -1.411964e+03    -5.869613e+01 \n      nfN2:topoW       nfN3:topoW       nfN4:topoW       nfN5:topoW \n    3.338172e+03    -2.204795e+03     3.275604e+03     1.455853e+03 \n     year:topoHT      year:topoLO       year:topoW nfN1:year:topoHT \n   -1.921966e+01     4.227740e-01    -1.259712e+01     1.930477e+00 \nnfN2:year:topoHT nfN3:year:topoHT nfN4:year:topoHT nfN5:year:topoHT \n    1.464313e+00     2.024633e+00     5.232640e-01    -9.370874e-01 \nnfN1:year:topoLO nfN2:year:topoLO nfN3:year:topoLO nfN4:year:topoLO \n   -1.818932e+00    -7.089521e-01     1.339684e+00     5.517450e-02 \nnfN5:year:topoLO  nfN1:year:topoW  nfN2:year:topoW  nfN3:year:topoW \n    7.067462e-01     2.969667e-02    -1.668718e+00     1.102522e+00 \n nfN4:year:topoW  nfN5:year:topoW \n   -1.636535e+00    -7.268434e-01 \n\n# Tidy summary of coefficients\nanova_coefs &lt;- broom::tidy(lm_05)\nanova_coefs\n\n# A tibble: 50 × 5\n   term        estimate std.error statistic  p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept) -29304.   1861.     -15.7    5.43e-54\n 2 nfN1            71.3  2621.       0.0272 9.78e- 1\n 3 nfN2           409.   2621.       0.156  8.76e- 1\n 4 nfN3          3143.   2653.       1.18   2.36e- 1\n 5 nfN4          2470.   2637.       0.937  3.49e- 1\n 6 nfN5          2725.   2671.       1.02   3.08e- 1\n 7 year            14.7     0.930   15.8    3.01e-54\n 8 topoHT       38406.   2590.      14.8    2.95e-48\n 9 topoLO        -839.   2527.      -0.332  7.40e- 1\n10 topoW        25181.   2438.      10.3    1.23e-24\n# ℹ 40 more rows\n\nperformance::r2(lm_00)\n\n# R2 for Linear Regression\n       R2: 0.000\n  adj. R2: 0.000\n\nperformance::r2(lm_01)\n\n# R2 for Linear Regression\n       R2: 0.019\n  adj. R2: 0.017\n\nperformance::r2(lm_02)\n\n# R2 for Linear Regression\n       R2: 0.091\n  adj. R2: 0.088\n\nperformance::r2(lm_03)\n\n# R2 for Linear Regression\n       R2: 0.479\n  adj. R2: 0.477\n\nperformance::r2(lm_04)\n\n# R2 for Linear Regression\n       R2: 0.568\n  adj. R2: 0.567\n\nperformance::r2(lm_05)\n\n# R2 for Linear Regression\n       R2: 0.733\n  adj. R2: 0.729",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_09/models_03.html#conclusion",
    "href": "coding/week_09/models_03.html#conclusion",
    "title": "Models III: Linear Models in Ag-Data Science",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nLinear models are essential for agricultural research, helping to quantify relationships and test hypotheses. This quick guide covered essentials for regression (continuous predictors), ANOVA (categorical predictors), assumption checks, model selection, and means comparisons using emmeans, and coefficients’ extraction with broom.",
    "crumbs": [
      "Lessons",
      "14-Models III"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html",
    "href": "coding/week_13/bayes_02.html",
    "title": "Bayesian Statistics in R",
    "section": "",
    "text": "📌 Today’s Topics\n\n\n\nWe’ll learn how to compute posterior distributions, step-by-step:\n\n🎯 Acceptance/Rejection Sampling (AR Sampling)\n🔁 Markov Chain Monte Carlo (MCMC) — more efficient than AR!\n\nAnd introduce powerful R packages for Bayesian modeling:\n\n📦 brms — beginner-friendly interface to Stan\n🔬 rstan — write your own Stan models\n🧪 rjags — Gibbs sampling with BUGS syntax",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#packages-well-use-today",
    "href": "coding/week_13/bayes_02.html#packages-well-use-today",
    "title": "Bayesian Statistics in R",
    "section": "1 📦 Packages we’ll use today",
    "text": "1 📦 Packages we’ll use today\n\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#computing-posterior-distributions",
    "href": "coding/week_13/bayes_02.html#computing-posterior-distributions",
    "title": "Bayesian Statistics in R",
    "section": "2 🎲 Computing Posterior Distributions",
    "text": "2 🎲 Computing Posterior Distributions\n\n2.1 1️⃣ Acceptance/Rejection Sampling — Basics\nHere’s how AR sampling works:\n\nPropose values for parameters\nSimulate data based on those values\nMeasure how well it fits the observed data\nAccept if close enough (✔️), reject otherwise (❌)\n\n\n2.1.1 🌽 Simulating Corn Yield vs. Plant Density\nWe simulate yield using a parabolic function:\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2 \\]\nThen compare simulated data to the real observed values. If the “fit” is good enough, we keep those parameter values.\n👀 We’ll visualize which parameter sets are accepted and which aren’t!\n\n\n\n\n\n\n\n\n\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2\\]\n\nGenerate proposal parameter values using the prior ditributions:\n\n\\[\\beta_0 \\sim uniform(4, 6)\\]\n\\[\\beta_1 \\sim uniform(1, 3)\\]\n\\[\\beta_2 \\sim uniform(0.1, 2)\\]\n\\[\\sigma \\sim Gamma(2, 2)\\]\n\nset.seed(6767)\nb0_try &lt;- runif(1, 4, 6)  # Parameter model for intercept (Uniform)\nb1_try &lt;- runif(1, 1, 3)  # Parameter model for slope (Uniform)\nb2_try &lt;- rgamma(1, 0.1, 2) # Parameter model for quadratic term (Gamma)\n# Mathematical equation for process model\nmu_try &lt;- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try &lt;- rgamma(1, 2, 2)\n\n\nGenerate data with those parameters\n\n\n\nset.seed(567)\ny_try &lt;- rnorm(n, mu_try, sigma_try)  # Process model\n\n\nCompare the simulated data with the observed data = “difference”\n\n\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] &lt;- sum(abs(y - y_try))\n\n\n“Accept” (gold) that combination of parameters if the difference &lt; predifined acceptable error. “Reject” (red) if the difference &gt; predifined acceptable error.\n\n\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Corn Yield (Mg/ha)\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] &lt; error, \"gold\", \"tomato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, what if whe change the priors:\n\n\n\n\n\n\n\n\n\nNow, do many tries\n\nfor (k in 1:K_tries) {\n    \n    b0_try &lt;- runif(1, 2, 10)  # Parameter model for intercept as uniform\n    b1_try &lt;- rnorm(1, 2.2, .5)  # Parameter model for slope as normal or gaussian\n    b2_try &lt;- rgamma(1, .25, 2) # Parameter model for quad term as gamma\n    # Mathematical equation for process model\n    mu_try &lt;- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try &lt;- rgamma(1, 2, 2)\n\n    y_try &lt;- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] &lt;- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] &lt;- y_try\n    \n    posterior_samp_parameters[k, ] &lt;- c(b0_try, b1_try, b2_try, sigma_try)\n}\n\nAcceptance rate\n\nlength(which(diff &lt; error))/K_tries\n\n[1] 0.169531\n\n\nPriors versus posteriors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 📊 Plot predictions\n\n# Prepare data\nfiltered_yhat &lt;- y_hat[which(diff &lt; error), 50]\ndf_yhat &lt;- data.frame(pred = filtered_yhat)\n\n# Plot\nggplot(df_yhat, aes(x = pred)) +\n  # Posterior\n  geom_histogram(aes(y = ..density..), fill = \"grey\", color = \"black\", bins = 30) +\n  geom_vline(xintercept = y[50], color = \"gold\", linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    x = expression(hat(y)[50]),\n    y = \"Density\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s get started",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#markov-chain-monte-carlo-mcmc",
    "href": "coding/week_13/bayes_02.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian Statistics in R",
    "section": "3 🔁 Markov Chain Monte Carlo (MCMC)",
    "text": "3 🔁 Markov Chain Monte Carlo (MCMC)\n\nMCMC methods changed Bayesian stats forever! 🧠🔥\n\nThey let us generate samples from complex distributions\nThey form a chain, where each sample depends on the previous\nUsed in packages like brms, rstan, and rjags\n\n📚 More info: - MCMC Handbook - MCMCpack - mcmc - Paper: Foundations of MCMC",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#brms-bayesian-modeling-made-easy",
    "href": "coding/week_13/bayes_02.html#brms-bayesian-modeling-made-easy",
    "title": "Bayesian Statistics in R",
    "section": "4 brms: Bayesian Modeling Made Easy",
    "text": "4 brms: Bayesian Modeling Made Easy\n\n🔗 Docs: https://paul-buerkner.github.io/brms/\n🐛 Issues: https://github.com/paul-buerkner/brms/issues\nbrms makes it easy to run complex Bayesian models — without writing Stan code manually. It’s inspired by lme4, so syntax feels familiar.\nIt supports a wide range of models: - Linear, GLM, survival, zero-inflated, ordinal, count, and more\n✨ We’ll use brms as our go-to interface in this session!\n📚 More info: - JSS Article on brms\n\n\n4.1 Fit brms\nLet’s fit the example using the brms package.\n\n\n4.2 brms pars\n\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n4.3 Model\n\n# 01. Run model\nbayes_model &lt;- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\n#bayes_model &lt;- readRDS(file = \"bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n\n\n\n\n\n\n\n# Visualize model results\nbayes_model\n\n\n4.3.1 Compare vs traditional linear model (lm)\n\ndata_frame_q &lt;- data_frame %&gt;% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n      5.415        2.038       -0.114  \n\n\n\n\n\n4.4 Using posterior distributions\n\n4.4.1 Prepare summary\n\n# Create predictions\nm1 &lt;- data_frame %&gt;% \n  ungroup() %&gt;% \n  dplyr::select(x) %&gt;% \n  group_by(x) %&gt;% filter(x == max(x)) %&gt;% \n  ungroup() %&gt;% unique() %&gt;% rename(max = x) %&gt;% \n  # Generate a sequence of x values\n  mutate(data = max %&gt;% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %&gt;% \n  unnest() %&gt;% dplyr::select(-max) %&gt;%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %&gt;% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %&gt;% ungroup()\n\n# Summarize\nm1_quantiles &lt;- m1 %&gt;% \n  group_by(x) %&gt;% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n\n\n4.4.2 Plot posterior\n\n# Plot\nm1_plot &lt;- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk1\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"gold3\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T, \n              linetype = \"dashed\")+\n  scale_color_manual(values=c(\"purple4\", \"tomato\"))+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_classic()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rstan-full-control-with-stan",
    "href": "coding/week_13/bayes_02.html#rstan-full-control-with-stan",
    "title": "Bayesian Statistics in R",
    "section": "5 rstan: Full Control with Stan",
    "text": "5 rstan: Full Control with Stan\n\n🔗 Docs: https://mc-stan.org/rstan/\n🐛 Issues: https://github.com/stan-dev/rstan/issues\nStan is a powerful, high-performance platform for Bayesian modeling, using: - Hamiltonian Monte Carlo (HMC) - No-U-Turn Sampler (NUTS)\nUnlike brms, Stan requires writing the full model — offering full flexibility and speed.\n✨ brms can even show the Stan code it generates under the hood!\nStan also supports Python, Julia, MATLAB, and more.",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_13/bayes_02.html#rjags-just-another-gibbs-sampler",
    "href": "coding/week_13/bayes_02.html#rjags-just-another-gibbs-sampler",
    "title": "Bayesian Statistics in R",
    "section": "6 rjags: Just Another Gibbs Sampler",
    "text": "6 rjags: Just Another Gibbs Sampler\n\n🔗 Docs: https://mcmc-jags.sourceforge.io/\n🐛 Issues: https://sourceforge.net/projects/mcmc-jags/\nrjags uses the classic Gibbs Sampling approach and the BUGS model syntax (used in WinBUGS, OpenBUGS).\n\nMore manual than brms\nIdeal for users who want to write the full statistical model\nOften paired with the coda package for diagnostics\n\n\nHappy Bayesian coding! 💻✨",
    "crumbs": [
      "Lessons",
      "21-Bayesian in R"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html",
    "href": "coding/week_09/models_04.html",
    "title": "Models IV: Mixed Models I",
    "section": "",
    "text": "After planning the experimental design, identifying dependent variable, independent variable(s), conducting the experiment, and collecting the data…the expected path would be as follows:\n\nflowchart LR\n  A[Dig the data] --&gt; B{Model Selection}\n  B --&gt; C[Significant\\nEffects?]\n  subgraph Inference\n  C --&gt;|Yes| D[Comparisons]\n  end\n\n\n\n\nflowchart LR\n  A[Dig the data] --&gt; B{Model Selection}\n  B --&gt; C[Significant\\nEffects?]\n  subgraph Inference\n  C --&gt;|Yes| D[Comparisons]\n  end",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#corn-data",
    "href": "coding/week_09/models_04.html#corn-data",
    "title": "Models IV: Mixed Models I",
    "section": "4.1 Corn Data",
    "text": "4.1 Corn Data\n\ndata_corn &lt;- agridat::lasrosas.corn\n\nFirst, the most important step is ALWAYS to write down the model.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#block-as-fixed",
    "href": "coding/week_09/models_04.html#block-as-fixed",
    "title": "Models IV: Mixed Models I",
    "section": "4.2 Block as Fixed",
    "text": "4.2 Block as Fixed\nIn a traditional approach blocks are defined as fixed, affecting the mean of the expected value. Yet there is no consensus about treating blocks as fixed or as random. For more information, read Dixon (2016).\nLet’s define the model. For simplification (and avoid writing interaction terms), here we are going to consider that \\(\\tau_i\\) is the “treatment”.\n\\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\]\n\\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] where \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\n\n# SIMPLEST MODEL\nblock_fixed &lt;- lm(\n  # Response variable\n  yield ~ \n    # Fixed\n    nf + rep,\n  # Data\n  data = data_corn)\n\ncar::Anova(block_fixed, type=3)\n\nAnova Table (Type III tests)\n\nResponse: yield\n             Sum Sq   Df   F value    Pr(&gt;F)    \n(Intercept) 1800690    1 4654.7170 &lt; 2.2e-16 ***\nnf            23987    5   12.4012 6.009e-12 ***\nrep            1271    2    1.6429    0.1936    \nResiduals   1328839 3435                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#block-as-random",
    "href": "coding/week_09/models_04.html#block-as-random",
    "title": "Models IV: Mixed Models I",
    "section": "4.3 Block as Random",
    "text": "4.3 Block as Random\nAn alternative approach is considering a MIXED MODEL, where blocks are considered “random”. Basically, we add a term to the model that it is expected to show a “null” overall effect over the mean of the variable of interest but introduces “noise”. By convention, a random effect is expected to have an expected value equal to zero but a positive variance as follows: \\[ y_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij} \\] \\[ \\beta_j \\sim N(0, \\sigma^2_{b} )\\] \\[ \\epsilon_{ij} \\sim N(0, \\sigma^2_{e} )\\] Similar than before, \\(\\mu\\) represents the overall mean (if intercept is used), \\(\\tau_i\\) is the effect of treatment-j over \\(\\mu\\), \\(\\beta_j\\) is the “random” effect of block-j over \\(\\mu\\), and \\(\\epsilon_{ij}\\) is the random effect of each experimental unit.\nSo what’s the difference? Simply specifying this component: \\[ \\beta_j \\sim N(0, \\sigma^2_b) \\], which serves to model the variance.\nHow do we write that?\n\n4.3.1 nlme\n\n# RANDOM BLOCK\nblock_random_nlme &lt;- \n  nlme::lme(\n    # Fixed\n    yield ~ nf,\n    # Random\n    random = ~1|rep,\n    # Data\n    data = data_corn)\n\n# ANOVA\ncar::Anova(block_random_nlme, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n               Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5648.401  1  &lt; 2.2e-16 ***\nnf            62.005  5  4.677e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.3.2 lme4\n\n# RANDOM BLOCK\nblock_random_lme4 &lt;- \n  lme4::lmer(# Response variable\n                   yield ~ \n                   # Fixed (Removing intercept? Why?)\n                   nf +\n                   # Random\n                   (1|rep), \n                   # Data\n                   data=data_corn)\n\n# ANOVA\ncar::Anova(block_random_lme4, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n               Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5648.406  1  &lt; 2.2e-16 ***\nnf            62.005  5  4.677e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#create-a-split-plot-design",
    "href": "coding/week_09/models_04.html#create-a-split-plot-design",
    "title": "Models IV: Mixed Models I",
    "section": "5.1 Create a split-plot design",
    "text": "5.1 Create a split-plot design\nThe agricolae package (Mendiburu 2021) brings a set of very useful functions to generate different experimental designs, among them, the split-plot. Let’s see an example:\n\n# Example with agricolae\n\n# Define plots\nmainplot &lt;- c(0,50,80,110,140)\nsubplot &lt;- c(\"Var_1\", \"Var_2\", \"Var_3\")\n\n# Produce\nsp_design &lt;- agricolae::design.split(\n                        trt1 = mainplot, trt2 = subplot, \n                        design = \"rcbd\", r = 3, \n                        randomization = TRUE, seed = 4561)\n\n#View(sp_design)\n\n#View(sp_design$book)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#split-plot-as-a-mixed-model",
    "href": "coding/week_09/models_04.html#split-plot-as-a-mixed-model",
    "title": "Models IV: Mixed Models I",
    "section": "5.2 Split-Plot as a Mixed Model",
    "text": "5.2 Split-Plot as a Mixed Model\nWhen moving to a Split-Plot design, an additional source of error is introduced: Main plot error (^2_m), which accounts for variation among the whole plots within each block.\n\\[y_{ijk} = \\mu + \\alpha_i + \\tau_k + (\\alpha\\tau){ik} + \\beta_j + \\gamma{ij} + \\epsilon_{ijk}\\]\nwhere: \\(\\alpha_i\\) = Fixed effect of the main-plot factor (e.g., tillage), \\(\\tau_k\\) = Fixed effect of the subplot factor (e.g., nitrogen rate), \\((\\alpha\\tau)_{ik}\\) = Fixed interaction effect between main and subplot factors, \\(\\beta_j \\sim N(0, \\sigma^2_b)\\) = Random effect of block, \\(\\gamma_{ij} \\sim N(0, \\sigma^2_m)\\) = Random effect of main plot within block (main-plot error), \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_e)\\) = Random error of experimental unit.\nThus, three sources of variability are modeled: 1. Block-to-block variation → \\(\\sigma^2_b\\) 2. Main-plot variation (whole plot error) → \\(\\sigma^2_m\\) 3. Residual variation (subplot error) → \\(\\sigma^2_e\\)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#data",
    "href": "coding/week_09/models_04.html#data",
    "title": "Models IV: Mixed Models I",
    "section": "5.3 Data",
    "text": "5.3 Data\nThe following is just a fake data set where we have a split-splot arrangement within an RCBD (BLOCK), where at the main plot corresponds to nitrogen rate (NRATE, with 5 levels), and the subplot to wheat variety (VARIETY, with 3 levels).\n\n# Read csv\n#split_plot_data_0 &lt;- read.csv(file = \"../data/01_split_plot_data.csv\", header = TRUE)\n\n# File online? Try this...(remove \"#\")\nurl_split &lt;- \"https://raw.githubusercontent.com/adriancorrendo/tidymixedmodelsweb/master/data/01_split_plot_data.csv\"\n\nsplit_plot_data_0 &lt;- read.csv(url_split)\n\n\n# Data hierarchy\nsplit_plot_data &lt;- \n  split_plot_data_0 %&gt;% \n  mutate(NRATE = factor(NRATE)) %&gt;% \n  # Identify Main Plot\n  #mutate(main = factor(BLOCK:NRATE)) %&gt;% \n  dplyr::select(BLOCK, NRATE, VARIETY, YIELD)\n\n#View(split_plot_data)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#explore-the-data",
    "href": "coding/week_09/models_04.html#explore-the-data",
    "title": "Models IV: Mixed Models I",
    "section": "5.4 Explore the data",
    "text": "5.4 Explore the data\nNow, let’s use several functions to explore the data.\n\n5.4.1 glimpse()\nFor example, the glimpse() function from the dplyr package (Wickham et al. 2022) allows to take a quick look to the columns in our data frame (it’s like a transposed version of print())\n\n# Glimpse from dplyr\ndplyr::glimpse(split_plot_data)\n\nRows: 45\nColumns: 4\n$ BLOCK   &lt;chr&gt; \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B1\", \"B…\n$ NRATE   &lt;fct&gt; 0, 0, 0, 50, 50, 50, 80, 80, 80, 110, 110, 110, 140, 140, 140,…\n$ VARIETY &lt;chr&gt; \"Var_1\", \"Var_2\", \"Var_3\", \"Var_1\", \"Var_2\", \"Var_3\", \"Var_1\",…\n$ YIELD   &lt;int&gt; 1938, 3946, 4628, 2038, 4346, 5949, 3837, 4287, 6765, 3466, 49…\n\n\n\n\n5.4.2 skim()\nThen, the skim() function from the skimr package (Waring et al. 2022) allows to take a deeper look to all the variables (columns), creating a quick summary that reports the presence of missing values, etc., etc.\n\n# Skim from skimr\nskimr::skim(split_plot_data)\n\n\nData summary\n\n\nName\nsplit_plot_data\n\n\nNumber of rows\n45\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBLOCK\n0\n1\n2\n2\n0\n3\n0\n\n\nVARIETY\n0\n1\n5\n5\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nNRATE\n0\n1\nFALSE\n5\n0: 9, 50: 9, 80: 9, 110: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYIELD\n0\n1\n4444.02\n1347.72\n1938\n3389\n4458\n5440\n6950\n▅▆▇▆▅\n\n\n\n\n\n\n\n5.4.3 ggplot()\nOf course, we shouldn’t miss to use ggplot2 for a better look\n\n# Boxplot\nsplit_plot_data %&gt;% \n  # Plot\nggplot() + \n  # Boxplots\n  geom_boxplot(aes(x = NRATE, y = YIELD, fill = VARIETY))+\n  geom_jitter(aes(x = NRATE, y = YIELD, fill = VARIETY))+\n  # Plot by site\n  facet_wrap(~VARIETY)+\n  scale_y_continuous(limits = c(0,10000), breaks = seq(0,10000, by=2000))+\n  # Change theme\n  theme_bw()",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#models",
    "href": "coding/week_09/models_04.html#models",
    "title": "Models IV: Mixed Models I",
    "section": "5.5 Models",
    "text": "5.5 Models\nFor the analysis of split-plot designs we basically need to specify an error term that otherwise the model will not see: the MAIN PLOT ERROR TERM (see Venables and Ripley (2002), pg. 283). By default, the random term that the computer will identify is the one happening at the lowest level in the hierarchy (replication). However, we need to specify that the main plot serves as a kind of block to the design.\n\n5.5.1 nlme::lme\n\n# Model without split component\nno_split &lt;- nlme::lme(# Response variable\n                 YIELD ~\n                   # Fixed\n                   0 + NRATE*VARIETY,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|BLOCK, \n                   # Data\n                   data = split_plot_data,\n                   # Method\n                   method = \"REML\")\n\n# Model with split component\nsplit_nlme &lt;- nlme::lme(# Response variable\n                 YIELD ~\n                   # Fixed (Removing intercept? Why?)\n                   0 + NRATE*VARIETY,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|BLOCK/NRATE, \n                   # Data\n                   data = split_plot_data,\n                   # Method\n                   method = \"REML\")\n\n# Type 3 (when interaction is present)\ncar::Anova(split_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: YIELD\n                Chisq Df Pr(&gt;Chisq)    \nNRATE         559.646  5  &lt; 2.2e-16 ***\nVARIETY        22.128  2  1.567e-05 ***\nNRATE:VARIETY  18.117  8    0.02036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Let's see the difference between models in terms of DFs\nsummary(no_split)\n\nLinear mixed-effects model fit by REML\n  Data: split_plot_data \n       AIC     BIC    logLik\n  511.0067 534.827 -238.5033\n\nRandom effects:\n Formula: ~1 | BLOCK\n        (Intercept) Residual\nStdDev:  0.02585478  521.401\n\nFixed effects:  YIELD ~ 0 + NRATE * VARIETY \n                         Value Std.Error DF   t-value p-value\nNRATE0                2536.000  301.0310 28  8.424381  0.0000\nNRATE50               2786.667  301.0310 28  9.257074  0.0000\nNRATE80               3857.667  301.0310 28 12.814847  0.0000\nNRATE110              3467.333  301.0310 28 11.518192  0.0000\nNRATE140              3100.667  301.0310 28 10.300156  0.0000\nVARIETYVar_2           649.667  425.7222 28  1.526034  0.1382\nVARIETYVar_3          1965.333  425.7222 28  4.616469  0.0001\nNRATE50:VARIETYVar_2   603.000  602.0621 28  1.001558  0.3251\nNRATE80:VARIETYVar_2   104.000  602.0621 28  0.172740  0.8641\nNRATE110:VARIETYVar_2  830.667  602.0621 28  1.379703  0.1786\nNRATE140:VARIETYVar_2 1561.000  602.0621 28  2.592756  0.0150\nNRATE50:VARIETYVar_3  1152.333  602.0621 28  1.913978  0.0659\nNRATE80:VARIETYVar_3   763.667  602.0621 28  1.268419  0.2151\nNRATE110:VARIETYVar_3 1033.000  602.0621 28  1.715770  0.0973\nNRATE140:VARIETYVar_3  292.667  602.0621 28  0.486107  0.6307\n Correlation: \n                      NRATE0 NRATE50 NRATE80 NRATE110 NRATE140 VARIETYV_2\nNRATE50                0.000                                             \nNRATE80                0.000  0.000                                      \nNRATE110               0.000  0.000   0.000                              \nNRATE140               0.000  0.000   0.000   0.000                      \nVARIETYVar_2          -0.707  0.000   0.000   0.000    0.000             \nVARIETYVar_3          -0.707  0.000   0.000   0.000    0.000    0.500    \nNRATE50:VARIETYVar_2   0.500 -0.500   0.000   0.000    0.000   -0.707    \nNRATE80:VARIETYVar_2   0.500  0.000  -0.500   0.000    0.000   -0.707    \nNRATE110:VARIETYVar_2  0.500  0.000   0.000  -0.500    0.000   -0.707    \nNRATE140:VARIETYVar_2  0.500  0.000   0.000   0.000   -0.500   -0.707    \nNRATE50:VARIETYVar_3   0.500 -0.500   0.000   0.000    0.000   -0.354    \nNRATE80:VARIETYVar_3   0.500  0.000  -0.500   0.000    0.000   -0.354    \nNRATE110:VARIETYVar_3  0.500  0.000   0.000  -0.500    0.000   -0.354    \nNRATE140:VARIETYVar_3  0.500  0.000   0.000   0.000   -0.500   -0.354    \n                      VARIETYV_3 NRATE50:VARIETYV_2 NRATE80:VARIETYV_2\nNRATE50                                                               \nNRATE80                                                               \nNRATE110                                                              \nNRATE140                                                              \nVARIETYVar_2                                                          \nVARIETYVar_3                                                          \nNRATE50:VARIETYVar_2  -0.354                                          \nNRATE80:VARIETYVar_2  -0.354      0.500                               \nNRATE110:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE140:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE50:VARIETYVar_3  -0.707      0.500              0.250            \nNRATE80:VARIETYVar_3  -0.707      0.250              0.500            \nNRATE110:VARIETYVar_3 -0.707      0.250              0.250            \nNRATE140:VARIETYVar_3 -0.707      0.250              0.250            \n                      NRATE110:VARIETYV_2 NRATE140:VARIETYV_2\nNRATE50                                                      \nNRATE80                                                      \nNRATE110                                                     \nNRATE140                                                     \nVARIETYVar_2                                                 \nVARIETYVar_3                                                 \nNRATE50:VARIETYVar_2                                         \nNRATE80:VARIETYVar_2                                         \nNRATE110:VARIETYVar_2                                        \nNRATE140:VARIETYVar_2  0.500                                 \nNRATE50:VARIETYVar_3   0.250               0.250             \nNRATE80:VARIETYVar_3   0.250               0.250             \nNRATE110:VARIETYVar_3  0.500               0.250             \nNRATE140:VARIETYVar_3  0.250               0.500             \n                      NRATE50:VARIETYV_3 NRATE80:VARIETYV_3 NRATE110:VARIETYV_3\nNRATE50                                                                        \nNRATE80                                                                        \nNRATE110                                                                       \nNRATE140                                                                       \nVARIETYVar_2                                                                   \nVARIETYVar_3                                                                   \nNRATE50:VARIETYVar_2                                                           \nNRATE80:VARIETYVar_2                                                           \nNRATE110:VARIETYVar_2                                                          \nNRATE140:VARIETYVar_2                                                          \nNRATE50:VARIETYVar_3                                                           \nNRATE80:VARIETYVar_3   0.500                                                   \nNRATE110:VARIETYVar_3  0.500              0.500                                \nNRATE140:VARIETYVar_3  0.500              0.500              0.500             \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.9300562 -0.6188455  0.0428333  0.5702584  1.6941534 \n\nNumber of Observations: 45\nNumber of Groups: 3 \n\nsummary(split_nlme)\n\nLinear mixed-effects model fit by REML\n  Data: split_plot_data \n       AIC      BIC    logLik\n  513.0067 538.2282 -238.5033\n\nRandom effects:\n Formula: ~1 | BLOCK\n        (Intercept)\nStdDev:  0.03201185\n\n Formula: ~1 | NRATE %in% BLOCK\n        (Intercept) Residual\nStdDev: 0.006112608  521.401\n\nFixed effects:  YIELD ~ 0 + NRATE * VARIETY \n                         Value Std.Error DF   t-value p-value\nNRATE0                2536.000  301.0310  8  8.424381  0.0000\nNRATE50               2786.667  301.0310  8  9.257074  0.0000\nNRATE80               3857.667  301.0310  8 12.814847  0.0000\nNRATE110              3467.333  301.0310  8 11.518192  0.0000\nNRATE140              3100.667  301.0310  8 10.300156  0.0000\nVARIETYVar_2           649.667  425.7222 21  1.526034  0.1419\nVARIETYVar_3          1965.333  425.7222 21  4.616469  0.0001\nNRATE50:VARIETYVar_2   603.000  602.0621 21  1.001558  0.3280\nNRATE80:VARIETYVar_2   104.000  602.0621 21  0.172740  0.8645\nNRATE110:VARIETYVar_2  830.667  602.0621 21  1.379703  0.1822\nNRATE140:VARIETYVar_2 1561.000  602.0621 21  2.592756  0.0170\nNRATE50:VARIETYVar_3  1152.333  602.0621 21  1.913978  0.0694\nNRATE80:VARIETYVar_3   763.667  602.0621 21  1.268419  0.2185\nNRATE110:VARIETYVar_3 1033.000  602.0621 21  1.715770  0.1009\nNRATE140:VARIETYVar_3  292.667  602.0621 21  0.486107  0.6319\n Correlation: \n                      NRATE0 NRATE50 NRATE80 NRATE110 NRATE140 VARIETYV_2\nNRATE50                0.000                                             \nNRATE80                0.000  0.000                                      \nNRATE110               0.000  0.000   0.000                              \nNRATE140               0.000  0.000   0.000   0.000                      \nVARIETYVar_2          -0.707  0.000   0.000   0.000    0.000             \nVARIETYVar_3          -0.707  0.000   0.000   0.000    0.000    0.500    \nNRATE50:VARIETYVar_2   0.500 -0.500   0.000   0.000    0.000   -0.707    \nNRATE80:VARIETYVar_2   0.500  0.000  -0.500   0.000    0.000   -0.707    \nNRATE110:VARIETYVar_2  0.500  0.000   0.000  -0.500    0.000   -0.707    \nNRATE140:VARIETYVar_2  0.500  0.000   0.000   0.000   -0.500   -0.707    \nNRATE50:VARIETYVar_3   0.500 -0.500   0.000   0.000    0.000   -0.354    \nNRATE80:VARIETYVar_3   0.500  0.000  -0.500   0.000    0.000   -0.354    \nNRATE110:VARIETYVar_3  0.500  0.000   0.000  -0.500    0.000   -0.354    \nNRATE140:VARIETYVar_3  0.500  0.000   0.000   0.000   -0.500   -0.354    \n                      VARIETYV_3 NRATE50:VARIETYV_2 NRATE80:VARIETYV_2\nNRATE50                                                               \nNRATE80                                                               \nNRATE110                                                              \nNRATE140                                                              \nVARIETYVar_2                                                          \nVARIETYVar_3                                                          \nNRATE50:VARIETYVar_2  -0.354                                          \nNRATE80:VARIETYVar_2  -0.354      0.500                               \nNRATE110:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE140:VARIETYVar_2 -0.354      0.500              0.500            \nNRATE50:VARIETYVar_3  -0.707      0.500              0.250            \nNRATE80:VARIETYVar_3  -0.707      0.250              0.500            \nNRATE110:VARIETYVar_3 -0.707      0.250              0.250            \nNRATE140:VARIETYVar_3 -0.707      0.250              0.250            \n                      NRATE110:VARIETYV_2 NRATE140:VARIETYV_2\nNRATE50                                                      \nNRATE80                                                      \nNRATE110                                                     \nNRATE140                                                     \nVARIETYVar_2                                                 \nVARIETYVar_3                                                 \nNRATE50:VARIETYVar_2                                         \nNRATE80:VARIETYVar_2                                         \nNRATE110:VARIETYVar_2                                        \nNRATE140:VARIETYVar_2  0.500                                 \nNRATE50:VARIETYVar_3   0.250               0.250             \nNRATE80:VARIETYVar_3   0.250               0.250             \nNRATE110:VARIETYVar_3  0.500               0.250             \nNRATE140:VARIETYVar_3  0.250               0.500             \n                      NRATE50:VARIETYV_3 NRATE80:VARIETYV_3 NRATE110:VARIETYV_3\nNRATE50                                                                        \nNRATE80                                                                        \nNRATE110                                                                       \nNRATE140                                                                       \nVARIETYVar_2                                                                   \nVARIETYVar_3                                                                   \nNRATE50:VARIETYVar_2                                                           \nNRATE80:VARIETYVar_2                                                           \nNRATE110:VARIETYVar_2                                                          \nNRATE140:VARIETYVar_2                                                          \nNRATE50:VARIETYVar_3                                                           \nNRATE80:VARIETYVar_3   0.500                                                   \nNRATE110:VARIETYVar_3  0.500              0.500                                \nNRATE140:VARIETYVar_3  0.500              0.500              0.500             \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.9300562 -0.6188455  0.0428333  0.5702584  1.6941534 \n\nNumber of Observations: 45\nNumber of Groups: \n           BLOCK NRATE %in% BLOCK \n               3               15 \n\n\n\n\n5.5.2 lmer code (lme4)\n\nsplit_lme4 &lt;- lme4::lmer(# Response variable\n                   YIELD ~ \n                   # Fixed (Removing intercept? Why?)\n                   0+NRATE*VARIETY +\n                   # Random\n                   (1|BLOCK/NRATE), \n                   # Data\n                   data=split_plot_data,\n                   contrasts = list(NRATE = \"contr.sum\", VARIETY = \"contr.sum\"\n                   )\n)\n\n# Alternative (being more explicit with the syntax)\n# split_lme4 &lt;- lme4::lmer(YIELD ~ NRATE * VARIETY + (1|BLOCK) + (1|BLOCK:NRATE), data=split_plot_data)\n\n# Type 3\ncar::Anova(split_lme4, type = 3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: YIELD\n                 Chisq Df Pr(&gt;Chisq)    \nNRATE         3326.392  5    &lt; 2e-16 ***\nVARIETY        188.511  2    &lt; 2e-16 ***\nNRATE:VARIETY   18.117  8    0.02036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Adjusting options for constrasts\noptions(contrasts = c(\"contr.sum\", \"contr.poly\")) # Ensure correct contrasts for Type 3\n\ncar::Anova(split_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: YIELD\n               Chisq Df Pr(&gt;Chisq)    \nNRATE         559.65  5  &lt; 2.2e-16 ***\nVARIETY               0               \nNRATE:VARIETY         0               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#check-assumptions",
    "href": "coding/week_09/models_04.html#check-assumptions",
    "title": "Models IV: Mixed Models I",
    "section": "5.6 Check assumptions",
    "text": "5.6 Check assumptions\n\n# Single tests\nperformance::check_normality(split_lme4)\n\nOK: residuals appear as normally distributed (p = 0.910).\n\nperformance::check_homogeneity(split_lme4)\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.396).\n\nperformance::check_autocorrelation(split_lme4)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.410).\n\nperformance::check_outliers(split_lme4)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\nperformance::check_collinearity(split_lme4)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n  Increased SE  VIF      VIF  CI SE_factor Tolerance\n         NRATE 1.00 [1.00, 1.00]      1.00      1.00\n       VARIETY 1.00 [1.00, 1.00]      1.00      1.00\n NRATE:VARIETY 1.00 [1.00, 1.00]      1.00      1.00\n\n# Check Plots\nperformance::check_model(split_lme4)",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#comparisons",
    "href": "coding/week_09/models_04.html#comparisons",
    "title": "Models IV: Mixed Models I",
    "section": "5.7 Comparisons",
    "text": "5.7 Comparisons\n\n# Estimate the comparisons (pairwise)\nsplit_lm4_comparisons &lt;-  \n  split_lme4 %&gt;% \n  # ~ specifies the level of comparison (marginal or interaction)\n  # Since interaction was significant we specify ~ Interaction (Factor1*Factor2)\n  emmeans(., ~ NRATE*VARIETY)\n\n# Add letters\nsplit_lm4_comparisons %&gt;% \n  # Compact Letters Display (cld)\n  cld(., \n      # Specify grouped comparisons by...\n      #by = \"NRATE\", \n      # Order\n      decreasing = TRUE, details=FALSE, reversed=TRUE, \n      # Specs\n      alpha=0.05,  adjust = \"tukey\", Letters=LETTERS)\n\n NRATE VARIETY emmean  SE df lower.CL upper.CL .group  \n 80    Var_3     6587 301 30     5630     7544  A      \n 110   Var_3     6466 301 30     5509     7423  AB     \n 50    Var_3     5904 301 30     4947     6861  ABC    \n 140   Var_3     5359 301 30     4402     6316  ABCD   \n 140   Var_2     5311 301 30     4354     6268  ABCD   \n 110   Var_2     4948 301 30     3991     5905   BCDE  \n 80    Var_2     4611 301 30     3654     5568    CDEF \n 0     Var_3     4501 301 30     3544     5458    CDEF \n 50    Var_2     4039 301 30     3082     4996     DEFG\n 80    Var_1     3858 301 30     2901     4815     DEFG\n 110   Var_1     3467 301 30     2510     4424      EFG\n 0     Var_2     3186 301 30     2229     4143       FG\n 140   Var_1     3101 301 30     2144     4058       FG\n 50    Var_1     2787 301 30     1830     3744        G\n 0     Var_1     2536 301 30     1579     3493        G\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 15 estimates \nP value adjustment: tukey method for comparing a family of 15 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#data-1",
    "href": "coding/week_09/models_04.html#data-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.1 Data",
    "text": "6.1 Data\n\n6.1.1 Details\nAn interesting split-split plot experiment in which the sub-plot treatments have a 2*5 factorial structure.\nAn experiment was conducted in 1932 on the experimental field of the Dominion Rust Research Laboratory. The study was designed to determine the effect on the incidence of root rot, of variety of wheat, kinds of dust for seed treatment, method of application of the dust, and efficacy of soil inoculation with the root-rot organism.\nThe field had 4 blocks.\nEach block has 2 whole plots for the genotypes.\nEach whole-plot had 10 sub-plots for the 5 different kinds of dust and 2 methods of application.\nEach sub-plot had 2 sub-sub-plots, one for inoculated soil and the other one for uninoculated soil.\nC. H. Goulden, (1939). Methods of statistical analysis, 1st ed. Page 18. https://archive.org/stream/methodsofstatist031744mbp\n\nsplitsplit_data &lt;- agridat::goulden.splitsplit\n\nglimpse(splitsplit_data)\n\nRows: 160\nColumns: 9\n$ row   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2…\n$ col   &lt;int&gt; 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 3, 4, 5, 6, 7,…\n$ yield &lt;int&gt; 64, 69, 67, 66, 71, 64, 64, 75, 70, 66, 67, 68, 65, 68, 71, 62, …\n$ inoc  &lt;fct&gt; Uninoc, Uninoc, Uninoc, Inoc, Inoc, Uninoc, Uninoc, Inoc, Inoc, …\n$ trt   &lt;int&gt; 5, 7, 1, 1, 9, 9, 10, 10, 2, 2, 6, 5, 6, 4, 4, 3, 3, 8, 8, 7, 6,…\n$ gen   &lt;fct&gt; Marquis, Marquis, Marquis, Marquis, Marquis, Marquis, Marquis, M…\n$ dry   &lt;fct&gt; Dry, Dry, Dry, Dry, Dry, Dry, Wet, Wet, Wet, Wet, Wet, Dry, Wet,…\n$ dust  &lt;fct&gt; DuBay, Check, Ceresan, Ceresan, CaCO3, CaCO3, CaCO3, CaCO3, Cere…\n$ block &lt;fct&gt; B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, B1, …\n\n# Make sure all factors are not numbers or integers.\nsplitsplit_data &lt;- splitsplit_data %&gt;% \n  mutate(trt = as.factor(trt))",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#models-1",
    "href": "coding/week_09/models_04.html#models-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.2 Models",
    "text": "6.2 Models\n\n6.2.1 nlme\n\n# Model with split component\nsplitsplit_nlme &lt;- nlme::lme(# Response variable\n                 yield ~\n                   # Fixed (Removing intercept? Why?)\n                   0 + gen*trt*inoc,\n                   # Random error of MAINPLOT (NRATE nested in BLOCK)\n                   random = ~1|block/gen/trt, \n                   # Data\n                   data = splitsplit_data,\n                   # Method\n                   method = \"REML\")\n\n# Type 3 (when interaction is present)\ncar::Anova(splitsplit_nlme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n                 Chisq Df Pr(&gt;Chisq)    \ngen          2202.1234  2  &lt; 2.2e-16 ***\ntrt            58.6809  9  2.405e-09 ***\ninoc           62.7247  1  2.377e-15 ***\ngen:trt        19.6914  9  0.0199156 *  \ngen:inoc        0.0199  1  0.8878172    \ntrt:inoc       30.8330  9  0.0003162 ***\ngen:trt:inoc    8.5127  9  0.4834183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.2.2 lme4\n\nsplitsplit_lme4 &lt;- lme4::lmer(# Response variable\n                   yield ~ \n                   # Fixed (Removing intercept? Why?)\n                   0+gen*trt*inoc +\n                   # Random\n                   (1|block/gen/trt), \n                   # Data\n                   data=splitsplit_data)\n\n# Type 3 (when interaction is present)\ncar::Anova(splitsplit_lme4, type = 3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: yield\n                 Chisq Df Pr(&gt;Chisq)    \ngen          2202.1225  2  &lt; 2.2e-16 ***\ntrt            58.6809  9  2.405e-09 ***\ninoc           62.7247  1  2.377e-15 ***\ngen:trt        19.6914  9  0.0199156 *  \ngen:inoc        0.0199  1  0.8878172    \ntrt:inoc       30.8330  9  0.0003162 ***\ngen:trt:inoc    8.5127  9  0.4834183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_09/models_04.html#comparisons-1",
    "href": "coding/week_09/models_04.html#comparisons-1",
    "title": "Models IV: Mixed Models I",
    "section": "6.3 Comparisons",
    "text": "6.3 Comparisons\n\n# Estimate the comparisons (pairwise)\nsplitsplit_lm4_comparisons &lt;-  \n  splitsplit_lme4 %&gt;% \n  # ~ specifies the level of comparison (marginal or interaction)\n  # Since interaction was significant we specify ~ Interaction (Factor1*Factor2)\n  emmeans(., ~ trt:inoc)\n\n# Add letters\nsplitsplit_lm4_comparisons %&gt;% \n  # Compact Letters Display (cld)\n  cld(., \n      # Specify grouped comparisons by...\n      by = \"trt\", \n      # Order\n      decreasing = TRUE, details=FALSE, reversed=TRUE, \n      # Specs\n      alpha=0.05,  adjust = \"tukey\", Letters=LETTERS)\n\ntrt = 1:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.4 2.03 11.7     58.6     74.1  A    \n Uninoc   64.1 2.03 11.7     56.4     71.9  A    \n\ntrt = 2:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     67.2 2.03 11.7     59.5     75.0  A    \n Uninoc   62.6 2.03 11.7     54.9     70.4   B   \n\ntrt = 3:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.9 2.03 11.7     59.1     74.6  A    \n Uninoc   65.5 2.03 11.7     57.7     73.3  A    \n\ntrt = 4:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.9 2.03 11.7     59.1     74.6  A    \n Uninoc   63.8 2.03 11.7     56.0     71.5  A    \n\ntrt = 5:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     66.6 2.03 11.7     58.9     74.4  A    \n Uninoc   66.1 2.03 11.7     58.4     73.9  A    \n\ntrt = 6:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     65.1 2.03 11.7     57.4     72.9  A    \n Uninoc   61.1 2.03 11.7     53.4     68.9   B   \n\ntrt = 7:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     77.5 2.03 11.7     69.7     85.3  A    \n Uninoc   67.4 2.03 11.7     59.6     75.1   B   \n\ntrt = 8:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     75.1 2.03 11.7     67.4     82.9  A    \n Uninoc   64.6 2.03 11.7     56.9     72.4   B   \n\ntrt = 9:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     71.5 2.03 11.7     63.7     79.3  A    \n Uninoc   67.8 2.03 11.7     60.0     75.5  A    \n\ntrt = 10:\n inoc   emmean   SE   df lower.CL upper.CL .group\n Inoc     72.5 2.03 11.7     64.7     80.3  A    \n Uninoc   63.6 2.03 11.7     55.9     71.4   B   \n\nResults are averaged over the levels of: gen \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 20 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same.",
    "crumbs": [
      "Lessons",
      "15-Models IV"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html",
    "href": "coding/week_08/models_02.html",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "",
    "text": "Statistical models in agriculture serve two primary purposes: explanatory and predictive modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.\nThis article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.\nRequired packages:\n\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(emmeans, multcomp, multcompView)\np_load(randomForest, caret, metrica)",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#introduction",
    "href": "coding/week_08/models_02.html#introduction",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "",
    "text": "Statistical models in agriculture serve two primary purposes: explanatory and predictive modeling. While explanatory models aim to understand the relationships between variables and identify response patterns, predictive models focus on forecasting future outcomes based on past data. Both approaches are essential for data-driven decision-making in precision agriculture, crop management, and environmental studies.\nThis article provides an overview of explanatory and predictive models, highlighting their key differences and applications using R.\nRequired packages:\n\nlibrary(pacman)\np_load(dplyr, tidyr)\np_load(ggplot2)\np_load(emmeans, multcomp, multcompView)\np_load(randomForest, caret, metrica)",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#explanatory-models",
    "href": "coding/week_08/models_02.html#explanatory-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "2 Explanatory Models",
    "text": "2 Explanatory Models\nExplanatory models are designed to understand how different factors influence a response variable. These models help answer questions such as: What are the main drivers of yield variation? How do nitrogen application and rainfall affect crop performance?\n\n2.1 Example: Linear Regression for Explanation\n\n# Simulated agricultural data\ndata_ag &lt;- data.frame(\n  nitrogen = c(50, 100, 150, 200, 250, 300),\n  rainfall = c(800, 850, 900, 950, 1000, 1050),\n  yield = c(2.5, 3.1, 3.8, 4.2, 4.5, 4.6)\n)\n\n# Fit a linear model\nlm_fit &lt;- lm(yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = yield ~ as.factor(nitrogen) + rainfall, data = data_ag)\n\nResiduals:\nALL 6 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 2.5        NaN     NaN      NaN\nas.factor(nitrogen)100      0.6        NaN     NaN      NaN\nas.factor(nitrogen)150      1.3        NaN     NaN      NaN\nas.factor(nitrogen)200      1.7        NaN     NaN      NaN\nas.factor(nitrogen)250      2.0        NaN     NaN      NaN\nas.factor(nitrogen)300      2.1        NaN     NaN      NaN\nrainfall                     NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 5 and 0 DF,  p-value: NA\n\n\n\n\n2.2 Interpretation\n\nThe coefficients indicate the effect of each predictor on yield.\nThe p-values help determine statistical significance.\nThe R-squared value explains how much variance is accounted for by the model.\n\n\n\n2.3 Means Comparisons\n\n# Perform multiple comparisons\nemmeans_fit &lt;- emmeans(lm_fit, ~ nitrogen)\ncomp &lt;- cld(emmeans_fit, Letters = letters)\ncomp\n\n nitrogen emmean  SE df lower.CL upper.CL .group\n      100    3.1 NaN  0      NaN      NaN       \n      150    3.8 NaN  0      NaN      NaN       \n      200    4.2 NaN  0      NaN      NaN       \n      250    4.5 NaN  0      NaN      NaN       \n       50 nonEst  NA NA       NA       NA       \n      300 nonEst  NA NA       NA       NA       \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 2 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n2.4 Visualization\nWhat are we missing here?\n\n# Create ggplot of estimated means\ncomp_plot &lt;- ggplot(comp, aes(x = as.factor(nitrogen), y = emmean, fill = nitrogen)) +\n  geom_col(color = \"black\") +\n  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.2) +\n  geom_text(aes(label = .group), vjust = -0.5, size = 5) +\n  labs(title = \"Means Comparison for Nitrogen Levels\",\n       x = \"Nitrogen Levels (kg/ha)\",\n       y = \"Estimated Yield (t/ha)\") +\n  theme_minimal()\n\ncomp_plot",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#predictive-models",
    "href": "coding/week_08/models_02.html#predictive-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "3 Predictive Models",
    "text": "3 Predictive Models\nPredictive models aim to forecast future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments. Machine learning models dominate here. A key challenge in predictive modeling is ensuring that the model generalizes well to unseen data, which is why we use techniques like cross-validation. An advantage is that we don’t need repetitions of the data to use these models, but we need to have a good size so the algorithms can “learn” (machine learning).\n\n3.1 Cross-Validation and Generalization Performance\nCross-validation is a resampling technique used to evaluate a model’s ability to generalize to new data. It helps avoid overfitting, where a model performs well on training data but poorly on unseen data. One common method is k-fold cross-validation, where the dataset is split into k subsets, and the model is trained and tested multiple times.\n\nTraining Error: The error the model makes on the data it was trained on.\nGeneralization Performance: The model’s ability to make accurate predictions on unseen data.\nValidation Set Approach: One practical method in agriculture is to leave out data from the latest year as a test set, ensuring the model is evaluated on future-like conditions.\n\n\n\n3.2 Updated Agricultural Dataset with Multiple Years\nTo better illustrate predictive modeling, we expand our dataset to include multiple years, allowing us to simulate a real-world scenario where we leave the latest year out for validation.\n\n# Simulated multi-year agricultural data\ndata_ag &lt;- data.frame(\n  year = rep(2011:2020, each = 50),  # 20 observations per year\n  nitrogen = runif(500, 90, 300),\n  rainfall = runif(500, 700, 1050),\n  psnt = runif(500, 5, 60), # pre-sidedress N test (ppm)\n  yield = 2 + 0.01 * runif(500, 90, 300) + 0.005 * runif(500, 700, 1050) + rnorm(500, 0, 0.2) - 0.002 * runif(500, 5, 60) \n)\n\n# Splitting into training (excluding latest year) and test set (latest year only)\ntrain_data &lt;- data_ag %&gt;% filter(year &lt; 2020)\ntest_data &lt;- data_ag %&gt;% filter(year == 2020)\n\n\n\n3.3 Example: Random Forest with Cross-Validation\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Train model with cross-validation using \"caret\" package\ntrain_control &lt;- caret::trainControl(method = \"cv\", number = 5)\nrf_fit &lt;- caret::train(yield ~ nitrogen + rainfall + psnt, data = train_data, method = \"rf\", trControl = train_control)\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\n# Model Performance\nprint(rf_fit)\n\nRandom Forest \n\n450 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 359, 360, 360, 361, 360 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared     MAE      \n  2     0.9101655  0.005663882  0.7370689\n  3     0.9151354  0.005187781  0.7380953\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n# Predict on the test set\npredictions &lt;- predict(rf_fit, test_data)\n\n# Evaluate Generalization Performance\nsqrt(mean((predictions - test_data$yield)^2))  # Root Mean Squared Error on test data\n\n[1] 0.7080847\n\n# Using the metrica package\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n\n$RMSE\n[1] 0.7080847\n\n# Plot predicted vs observed scatter\nmetrica::scatter_plot(pred = predictions, obs = test_data$yield)\n\n\n\n\n\n\n\n# With tidyverse syntax will be...\ntest_preds &lt;- test_data %&gt;% mutate(predictions = predict(rf_fit, test_data))\n\nmetrica::scatter_plot(data = test_preds, \n                      pred = predictions, obs = yield)\n\n\n\n\n\n\n\n# Root mean square error\nmetrica::RMSE(pred = predictions, obs = test_data$yield)\n\n$RMSE\n[1] 0.7080847\n\n# Relative mean square error (as a proportion)\nmetrica::RRMSE(data = test_preds, pred = predictions, obs = yield)\n\n$RRMSE\n[1] 0.08590629\n\n# Estimate more prediction error metrics\nmetrica::metrics_summary(data = test_preds, pred = predictions, obs = yield,\n                         type = \"regression\")\n\n   Metric         Score\n1      B0  5.0350052760\n2      B1  0.3876722744\n3       r  0.2274194841\n4      R2  0.0517196217\n5      Xa  0.6738755631\n6     CCC  0.1532524329\n7     MAE  0.5788009582\n8    RMAE  0.0702213256\n9    MAPE  7.1616881311\n10  SMAPE  7.0596644078\n11    RAE  1.0032320662\n12    RSE  0.9742467992\n13    MBE  0.0121206657\n14    PBE  0.1470504149\n15    PAB  0.0293010081\n16    PPB 38.4856531036\n17    MSE  0.5013838960\n18   RMSE  0.7080846672\n19  RRMSE  0.0859062918\n20    RSR  1.3758902632\n21 iqRMSE  0.8338883760\n22    MLA  0.1931077775\n23    MLP  0.3082761185\n24   RMLA  0.1931077775\n25   RMLP  0.3082761185\n26     SB  0.0001469105\n27   SDSD  0.1929608669\n28    LCS  0.3082761185\n29    PLA 38.5149541118\n30    PLP 61.4850458882\n31     Ue 61.4850458882\n32     Uc 38.4856531036\n33     Ub  0.0293010081\n34    NSE  0.0257532008\n35     E1 -0.0032320662\n36   Erel -0.0689801883\n37    KGE  0.0145411153\n38      d  0.3918632952\n39     d1  0.2676482520\n40    d1r  0.4983839669\n41    RAC  0.5765736892\n42     AC -2.9375050057\n43 lambda  0.1532524329\n44  dcorr  0.3276562374\n45    MIC  0.3437581054\n\n\n\n\n3.4 Interpretation\n\nCross-validation ensures the model is not just memorizing the training data but generalizing well (e.g. predicting well on unseen observations).\nTraining vs. Test/Validation Performance: Comparing error metrics between the training set and the unseen test set gives an estimate of real-world predictive ability. If the difference training between training error and testing error is too much, it’s very likely our model is “over-fitted” (e.g. reading really well the training data but too much).\nLeaving the latest year out allows us to test predictions on future-like data, a common technique in agricultural forecasting.\n\nUsing these techniques ensures that predictive models in agriculture provide reliable and actionable insights rather than overfitted results that fail in practice.\nPredictive models aim to forecast future values based on historical data. They are widely used in precision agriculture for yield prediction, disease detection, and climate impact assessments.\n\n\n3.5 Example: Final Random Forest for Forecasting\nNow we have our final model, we train one more time with all the available data, then predict new observations\n\n# Fit a random forest model\nset.seed(123)\n\nrf_fit &lt;- randomForest(yield ~ nitrogen + rainfall + psnt, data = data_ag, ntree = 500)\nprint(rf_fit)\n\n\nCall:\n randomForest(formula = yield ~ nitrogen + rainfall + psnt, data = data_ag,      ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.7684589\n                    % Var explained: -6.18\n\n# Predict yield for new nitrogen and rainfall levels\nnew_data &lt;- data.frame(field = c(\"Elora\", \"Waterloo\", \"Ridgetown\", \"Winchester\"),\n                       nitrogen = c(125, 175, 225, 220), \n                       rainfall = c(870, 920, 980, 1000),\n                       psnt = c(35, 30, 45, 60))\n\n# Adding predictions\nnew_preds &lt;- new_data %&gt;% mutate(predictions = predict(rf_fit, new_data))\n\n\n\n3.6 Interpretation\n\nThis model is non-parametric and learns patterns from data.\nIt is robust against outliers and complex interactions.\nPerformance is evaluated using Mean Squared Error (MSE) or R-squared.",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#key-differences-between-explanatory-and-predictive-models",
    "href": "coding/week_08/models_02.html#key-differences-between-explanatory-and-predictive-models",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "4 Key Differences Between Explanatory and Predictive Models",
    "text": "4 Key Differences Between Explanatory and Predictive Models\n\n\n\n\n\n\n\n\nFeature\nExplanatory Models\nPredictive Models\n\n\n\n\nPurpose\nUnderstanding relationships\nMaking accurate forecasts\n\n\nExample\nLinear regression, ANOVA\nMachine learning (random forests, neural networks)\n\n\nAssumptions\nRequires assumptions about data distribution\nOften non-parametric, flexible\n\n\nOutput\nCoefficients, p-values\nPredictions, accuracy metrics",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_08/models_02.html#conclusion",
    "href": "coding/week_08/models_02.html#conclusion",
    "title": "Explanatory vs. Predictive Models in Agriculture with R",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nUnderstanding the distinction between explanatory and predictive models is essential for agricultural research. While explanatory models help us understand why certain patterns exist, predictive models allow us to make data-driven decisions for future planning. A combination of both approaches can maximize insights and improve decision-making in precision agriculture.\nThis article brings simple examples in R using linear regression for explanatory analysis and random forests for prediction. Depending on the research question, both modeling strategies play a crucial role in agricultural data science.",
    "crumbs": [
      "Lessons",
      "13-Models II"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html",
    "href": "coding/week_06/10_iteration.html",
    "title": "Iteration in R: The Power of purrr",
    "section": "",
    "text": "Description\nThis lesson explores iteration in R, focusing on the power of the purrr package for functional programming. We’ll compare traditional for loops with purrr’s map() functions to illustrate more efficient and readable approaches to iteration.\nRequired packages for today\nlibrary(pacman) # to install and load packages faster\np_load(dplyr, tidyr) # data wrangling\np_load(purrr) # iteration mapping\np_load(ggplot2) # plots\np_load(agridat) # data\np_load(nlme, broom.mixed, car, performance) # mixed models work\np_load(emmeans, multcomp, multcompView) # multiple comparisons",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#understanding-the-difference-between-map-and-map_dbl",
    "href": "coding/week_06/10_iteration.html#understanding-the-difference-between-map-and-map_dbl",
    "title": "Iteration in R: The Power of purrr",
    "section": "4.1 Understanding the Difference Between map() and map_dbl()",
    "text": "4.1 Understanding the Difference Between map() and map_dbl()\n\nmap_dbl() guarantees that the output is a numeric vector.\n\nSimilarly:\n\nmap_chr() returns a character vector.\nmap_lgl() produces a logical vector.\nmap_int() yields an integer vector.\n\n\nmap(), the most general form, returns a list by default.\n\nThese functions are part of an iterative approach where a function is mapped over elements of a list or vector.",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#practical-application",
    "href": "coding/week_06/10_iteration.html#practical-application",
    "title": "Iteration in R: The Power of purrr",
    "section": "5.1 Practical Application",
    "text": "5.1 Practical Application\nA common workflow involves combining group_by() and nest() to create nested data frames for iteration. You can then use mutate() along with map() to apply a function to each nested data frame:\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\n\ndata %&gt;%\n  group_by(group_variable) %&gt;%\n  nest() %&gt;%\n  mutate(results = map(data, your_function))\nThis approach is very powerful for applying custom functions to subsets of data efficiently.\nLet’s see that in practice…",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#load-the-data",
    "href": "coding/week_06/10_iteration.html#load-the-data",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.1 Load the data",
    "text": "6.1 Load the data\n\ndata_corn_00 &lt;- agridat::lasrosas.corn\nhead(data_corn_00)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#prepare-the-data",
    "href": "coding/week_06/10_iteration.html#prepare-the-data",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.2 Prepare the data",
    "text": "6.2 Prepare the data\n\ndata_corn_01 &lt;- data_corn_00 %&gt;% \n  # Select only necessary variables\n  dplyr::select(year, topo, rep, nf, yield) %&gt;% \n  # Group by\n  group_by(year, topo) %&gt;% \n  # Create nested data frames\n  nest(my_data = c(rep, nf, yield))",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#create-functions",
    "href": "coding/week_06/10_iteration.html#create-functions",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.3 Create functions",
    "text": "6.3 Create functions\n\n6.3.1 Rep (block) as fixed\n\n# SIMPLEST MODEL\nfit_block_fixed &lt;- function(x){\n  lm(# Response variable\n     yield ~ \n       # Fixed (treatment)\n       nf + \n       # Block as fixed too\n       rep,\n     # Data\n     data = x)\n}\n\n\n\n6.3.2 Rep (block) as random\n\n# RANDOM BLOCK (mixed model)\nfit_block_random &lt;- function(x){\n  nlme::lme(# Response variable\n    yield ~\n    # Fixed\n    nf,\n    # Random\n    random = ~1|rep,\n    # Data\n    data = x)\n  }",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#fit-models-with-mapping",
    "href": "coding/week_06/10_iteration.html#fit-models-with-mapping",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.4 Fit models with mapping",
    "text": "6.4 Fit models with mapping\n\nmodels &lt;- data_corn_01 %&gt;% \n  # BLOCK as FIXED \n  mutate(model_1 = map(my_data, fit_block_fixed)) %&gt;% \n  # BLOCK as RANDOM\n  mutate(model_2 = map(my_data, fit_block_random)) %&gt;% \n    \n  # Data wrangling\n  pivot_longer(cols = c(model_1:model_2), # show alternative 'contains' model\n               names_to = \"model_id\",\n               values_to = \"model\") %&gt;% \n  # Map over model column\n  mutate(results = map(model, broom.mixed::augment )) %&gt;% \n  # Performance\n  mutate(performance = map(model, broom.mixed::glance )) %&gt;% \n  # Extract AIC\n  mutate(AIC = map(performance, ~.x$AIC)) %&gt;% \n  ungroup()",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#model-selection",
    "href": "coding/week_06/10_iteration.html#model-selection",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.5 Model selection",
    "text": "6.5 Model selection\nCompare models performance\n\n# Visual model selection\nbest_models &lt;- \n  models %&gt;% \n  group_by(year, topo) %&gt;% \n  # Use case_when to identify the best model\n  mutate(best_model = \n           case_when(AIC == min(as.numeric(AIC)) ~ \"Yes\",\n                     TRUE ~ \"No\")) %&gt;% \n  ungroup()\n\n# Plot\nbest_models %&gt;% \n  ggplot()+\n  geom_point(aes(x = model_id, y = as.numeric(AIC), \n                 color = best_model, shape = best_model), \n             size = 3)+\n  facet_wrap(year~topo)+\n  theme_bw()\n\n\n\n\n\n\n\n# Final models\nselected_models &lt;- best_models %&gt;% dplyr::filter(best_model == \"Yes\")",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#run-anova",
    "href": "coding/week_06/10_iteration.html#run-anova",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.6 Run ANOVA",
    "text": "6.6 Run ANOVA\nEstimate the effects of factor under study\n\nmodels_effects &lt;- \n  selected_models %&gt;%\n  # Type 3 Sum of Squares (Partial SS, when interactions are present)\n  mutate(ANOVA = map(model, ~Anova(., type = 3)) )\n\n# Extract ANOVAS\nmodels_effects$ANOVA[[1]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 5729.92  1  &lt; 2.2e-16 ***\nnf           164.03  5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[2]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n              Chisq Df Pr(&gt;Chisq)    \n(Intercept) 6089.64  1  &lt; 2.2e-16 ***\nnf           318.03  5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[3]]\n\nAnova Table (Type III tests)\n\nResponse: yield\n            Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept) 158985   1 7299.604 &lt; 2.2e-16 ***\nnf            1975   5   18.136 4.699e-16 ***\nrep            691   2   15.858 2.509e-07 ***\nResiduals     7841 360                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodels_effects$ANOVA[[8]]\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: yield\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 18282.200  1  &lt; 2.2e-16 ***\nnf             72.431  5  3.194e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  },
  {
    "objectID": "coding/week_06/10_iteration.html#means-comparison",
    "href": "coding/week_06/10_iteration.html#means-comparison",
    "title": "Iteration in R: The Power of purrr",
    "section": "6.7 Means comparison",
    "text": "6.7 Means comparison\n\n# MULTCOMPARISON\n# emmeans and cld multcomp\n# We need to specify ourselves the most important interaction to perform the comparisons\nmult_comp &lt;- \n  models_effects %&gt;% \n  # Comparisons estimates (emmeans)\n  mutate(mc_estimates = map(model, ~emmeans(., ~ nf))) %&gt;% \n  # Assign letters and p-value adjustment (multcomp)\n  mutate(mc_letters = \n           map(mc_estimates, \n               ~as.data.frame( \n                 # By specifies a strata or level to assign the letters\n                 cld(., decreasing = TRUE, details=FALSE,\n                     reversed=TRUE, alpha=0.05,  adjust = \"tukey\", Letters=LETTERS))))\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons",
    "crumbs": [
      "Lessons",
      "10-Iteration"
    ]
  }
]